%===========================================
\documentclass[12pt]{report}
%===========================================
\usepackage[utf8]{inputenc}
%\usepackage[margin=2.5in]{geometry}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{graphicx,graphics}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{color}
\usepackage{float} 
\usepackage{subfig}
\usepackage[figuresright]{rotating}
\usepackage{enumerate}
\usepackage{anysize} 
\usepackage{url}
\usepackage{imakeidx}
\usepackage[left=0.5in, right=0.5in, top=1in, bottom=1in]{geometry}

%===========================================
\title{Notas sobre Regresión Logística \\
\textit{Breve introducción con aplicaciones}}
\author{Carlos E Martinez-Rodriguez}
\date{}
%===========================================
\newtheorem{Criterio}{Criterio}%[section]
\newtheorem{Sup}{Supuesto}%[section]
\newtheorem{Note}{Nota}%[section]
\newtheorem{Ejem}{Ejemplo}%[section]
\newtheorem{Prop}{Proposici\'on}%[section]
\newtheorem{Def}{Definici\'on}
\newtheorem{Teo}{Teorema}
\newtheorem{Algthm}{Algoritmo}
\newtheorem{Sol}{Soluci\'on}

%===========================================
\begin{document}
%===========================================
\maketitle
\tableofcontents
%__________________________
\chapter{Principios}
%__________________________

\section{Introducci\'on}

La Estad\'istica es una ciencia formal que estudia la recolecci\'on, an\'alisis e interpretaci\'on de datos de una muestra representativa, ya sea para ayudar en la toma de decisiones o para explicar condiciones regulares o irregulares de alg\'un fen\'omeno o estudio aplicado, de ocurrencia en forma aleatoria o condicional. Sin embargo, la estad\'istica es m\'as que eso, es decir, es transversal a una amplia variedad de disciplinas, desde la f\'isica hasta las ciencias sociales, desde las ciencias de la salud hasta el control de calidad. Se usa para la toma de decisiones en \'areas de negocios o instituciones gubernamentales. Ahora bien, las t\'ecnicas estad\'isticas se aplican de manera amplia en mercadotecnia, contabilidad, control de calidad y en otras actividades; estudios de consumidores; an\'alisis de resultados en deportes; administradores de instituciones; en la educaci\'on; organismos pol\'iticos; m\'edicos; y por otras personas que intervienen en la toma de decisiones.

\begin{Def}
    La Estad\'istica es la ciencia cuyo objetivo es reunir una informaci\'on cuantitativa concerniente a individuos, grupos, series de hechos, etc. y deducir de ello gracias al an\'alisis de estos datos unos significados precisos o unas previsiones para el futuro.
\end{Def}

La estad\'istica, en general, es la ciencia que trata de la recopilaci\'on, organizaci\'on presentaci\'on, an\'alisis e interpretaci\'on de datos num\'ericos con el fin de realizar una toma de decisi\'on m\'as efectiva. Los m\'etodos estad\'isticos tradicionalmente se utilizan para prop\'ositos descriptivos, para organizar y resumir datos num\'ericos. La estad\'istica descriptiva, por ejemplo trata de la tabulaci\'on de datos, su presentaci\'on en forma gr\'afica o ilustrativa y el c\'alculo de medidas descriptivas.


\section{Historia de la Estad\'istica}

Es dif\'icil conocer los or\'igenes de la Estad\'istica. Desde los comienzos de la civilizaci\'on han existido formas sencillas de estad\'istica, pues ya se utilizaban representaciones gr\'aficas y otros s\'imbolos en pieles, rocas, palos de madera y paredes de cuevas para contar el n\'umero de personas, animales o ciertas cosas. Su origen empieza posiblemente en la isla de Cerde\~na, donde existen monumentos prehist\'oricos pertenecientes a los Nuragas, las primeros habitantes de la isla; estos monumentos constan de bloques de basalto superpuestos sin mortero y en cuyas paredes de encontraban grabados toscos signos que han sido interpretados con mucha verosimilidad como muescas que serv\'ian para llevar la cuenta del ganado y la caza. Los babilonios usaban ya peque\~nas tablillas de arcilla para recopilar datos en tablas sobre la producci\'on agr\'icola y los g\'eneros vendidos o cambiados mediante trueque. Otros vestigios pueden ser hallados en el antiguo Egipto, cuyos faraones lograron recopilar, hacia el a\~no 3050 antes de Cristo, prolijos datos relativos a la poblaci\'on y la riqueza del pa\'is. De acuerdo al historiador griego Her\'odoto, dicho registro de riqueza y poblaci\'on se hizo con el objetivo de preparar la construcci\'on de las pir\'amides. En el mismo Egipto, Rams\'es II hizo un censo de las tierras con el objeto de verificar un nuevo reparto. En el antiguo Israel la Biblia da referencias, en el libro de los N\'umeros, de los datos estad\'isticos obtenidos en dos recuentos de la poblaci\'on hebrea. El rey David por otra parte, orden\'o a Joab, general del ej\'ercito hacer un censo de Israel con la finalidad de conocer el n\'umero de la poblaci\'on. Tambi\'en los chinos efectuaron censos hace m\'as de cuarenta siglos. Los griegos efectuaron censos peri\'odicamente con fines tributarios, sociales (divisi\'on de tierras) y militares (c\'alculo de recursos y hombres disponibles). La investigaci\'on hist\'orica revela que se realizaron 69 censos para calcular los impuestos, determinar los derechos de voto y ponderar la potencia guerrera. \medskip

Fueron los romanos, maestros de la organizaci\'on pol\'itica, quienes mejor supieron emplear los recursos de la estad\'istica. Cada cinco a\~nos realizaban un censo de la poblaci\'on y sus funcionarios p\'ublicos ten\'ian la obligaci\'on de anotar nacimientos, defunciones y matrimonios, sin olvidar los recuentos peri\'odicos del ganado y de las riquezas contenidas en las tierras conquistadas. Para el nacimiento de Cristo suced\'ia uno de estos empadronamientos de la poblaci\'on bajo la autoridad del imperio. Durante los mil a\~nos siguientes a la ca\'ida del imperio Romano se realizaron muy pocas operaciones Estad\'isticas, con la notable excepci\'on de las relaciones de tierras pertenecientes a la Iglesia, compiladas por Pipino el Breve en el 758 y por Carlomagno en el 762 DC. Durante el siglo IX se realizaron en Francia algunos censos parciales de siervos. En Inglaterra, Guillermo el Conquistador recopil\'o el Domesday Book o libro del Gran Catastro para el a\~no 1086, un documento de la propiedad, extensi\'on y valor de las tierras de Inglaterra. Esa obra fue el primer compendio estad\'istico de Inglaterra.  Aunque Carlomagno, en Francia; y Guillermo el Conquistador, en Inglaterra, trataron de revivir la t\'ecnica romana, los m\'etodos estad\'isticos permanecieron casi olvidados durante la Edad Media.


Durante los siglos XV, XVI, y XVII, hombres como Leonardo de Vinci, Nicol\'as Cop\'ernico, Galileo, Neper, William Harvey, Sir Francis Bacon y Ren\'e Descartes, hicieron grandes operaciones al m\'etodo cient\'ifico, de tal forma que cuando se crearon los Estados Nacionales y surgi\'o como fuerza el comercio internacional exist\'ia ya un m\'etodo capaz de aplicarse a los datos econ\'omicos. Para el a\~no 1532 empezaron a registrarse en Inglaterra las defunciones debido al temor que Enrique VII ten\'ia por la peste.  M\'as o menos por la misma \'epoca, en Francia la ley exigi\'o a los cl\'erigos registrar los bautismos, fallecimientos y matrimonios. Durante un brote de peste que apareci\'o a fines de la d\'ecada de 1500, el gobierno ingl\'es comenz\'o a publicar estad\'istica semanales de los decesos. Esa costumbre continu\'o muchos a\~nos, y en 1632 estos Bills of Mortality (Cuentas de Mortalidad) conten\'ian los nacimientos y fallecimientos por sexo. En 1662, el capit\'an John Graunt us\'o documentos que abarcaban treinta a\~nos y efectu\'o predicciones sobre el n\'umero de personas que morir\'ian de varias enfermedades y sobre las proporciones de nacimientos de varones y mujeres que cabr\'ia esperar. El trabajo de Graunt, condensado en su obra \textit{Natural and Political Observations...Made upon the Bills of Mortality}, fue un esfuerzo innovador en el an\'alisis estad\'istico. Por el a\~no 1540 el alem\'an Sebasti\'an Muster realiz\'o una compilaci\'on estad\'istica de los recursos nacionales, comprensiva de datos sobre organizaci\'on pol\'itica, instrucciones sociales, comercio y poder\'io militar. 

Los eruditos del siglo XVII demostraron especial inter\'es por la Estad\'istica Demogr\'afica como resultado de la especulaci\'on sobre si la poblaci\'on aumentaba, decrec\'ia o permanec\'ia est\'atica. En los tiempos modernos tales m\'etodos fueron resucitados por algunos reyes que necesitaban conocer las riquezas monetarias y el potencial humano de sus respectivos pa\'ises. El primer empleo de los datos estad\'isticos para fines ajenos a la pol\'itica tuvo lugar en 1691 y estuvo a cargo de Gaspar Neumann, un profesor alem\'an que viv\'ia en Breslau. Este investigador se propuso destruir la antigua creencia popular de que en los a\~nos terminados en siete mor\'ia m\'as gente que en los restantes, y para lograrlo hurg\'o pacientemente en los archivos parroquiales de la ciudad. Despu\'es de revisar miles de partidas de defunci\'on pudo demostrar que en tales a\~nos no fallec\'ian m\'as personas que en los dem\'as. Los procedimientos de Neumann fueron conocidos por el astr\'onomo ingl\'es Halley, descubridor del cometa que lleva su nombre, quien los aplic\'o al estudio de la vida humana. 

Durante el siglo XVII y principios del XVIII, matem\'aticos como Bernoulli, Francis Maseres, Lagrange y Laplace desarrollaron la teor\'ia de probabilidades. No obstante durante cierto tiempo, la teor\'ia de las probabilidades limit\'o su aplicaci\'on a los juegos de azar y hasta el siglo XVIII no comenz\'o a aplicarse a los grandes problemas cient\'ificos. Godofredo Achenwall, profesor de la Universidad de Gotinga, acu\~n\'o en 1760 la palabra estad\'istica, que extrajo del t\'ermino italiano statista (estadista). Cre\'ia, y con sobrada raz\'on, que los datos de la nueva ciencia ser\'ian el aliado m\'as eficaz del gobernante consciente. La ra\'iz remota de la palabra se halla, por otra parte, en el t\'ermino latino status, que significa estado o situaci\'on; Esta etimolog\'ia aumenta el valor intr\'inseco de la palabra, por cuanto la estad\'istica revela el sentido cuantitativo de las m\'as variadas situaciones. Jacques Qu\'etelect es quien aplica las Estad\'isticas a las ciencias sociales. Este interpret\'o la teor\'ia de la probabilidad para su uso en las ciencias sociales y resolver la aplicaci\'on del principio de promedios y de la variabilidad a los fen\'omenos sociales. Qu\'etelect fue el primero en realizar la aplicaci\'on pr\'actica de todo el m\'etodo Estad\'istico, entonces conocido, a las diversas ramas de la ciencia. Entretanto, en el per\'iodo del 1800 al 1820 se desarrollaron dos conceptos matem\'aticos fundamentales para la teor\'ia Estad\'istica; la teor\'ia de los errores de observaci\'on, aportada por Laplace y Gauss; y la teor\'ia de los m\'inimos cuadrados desarrollada por Laplace, Gauss y Legendre. A finales del siglo XIX, Sir Francis Gaston ide\'o el m\'etodo conocido por Correlaci\'on, que ten\'ia por objeto medir la influencia relativa de los factores sobre las variables. De aqu\'i parti\'o el desarrollo del coeficiente de correlaci\'on creado por Karl Pearson y otros cultivadores de la ciencia biom\'etrica como J. Pease Norton, R. H. Hooker y G. Udny Yule, que efectuaron amplios estudios sobre la medida de las relaciones.

La historia de la estad\'istica est\'a resumida en tres grandes etapas o fases.

\begin{itemize}
    \item \textbf{Fase 1: Los Censos:} Desde el momento en que se constituye una autoridad pol\'itica, la idea de inventariar de una forma m\'as o menos regular la poblaci\'on y las riquezas existentes en el territorio est\'a ligada a la conciencia de soberan\'ia y a los primeros esfuerzos administrativos.
    \item \textbf{Fase 2: De la Descripci\'on de los Conjuntos a la Aritm\'etica Pol\'itica:} Las ideas mercantilistas extra\~nan una intensificaci\'on de este tipo de investigaci\'on. Colbert multiplica las encuestas sobre art\'iculos manufacturados, el comercio y la poblaci\'on: los intendentes del Reino env\'ian a Par\'is sus memorias. Vauban, m\'as conocido por sus fortificaciones o su Dime Royale, que es la primera propuesta de un impuesto sobre los ingresos, se se\~nala como el verdadero precursor de los sondeos. M\'as tarde, Buf\'on se preocupa de esos problemas antes de dedicarse a la historia natural. La escuela inglesa proporciona un nuevo progreso al superar la fase puramente descriptiva.

Sus tres principales representantes son Graunt, Petty y Halley. El pen\'ultimo es autor de la famosa Aritm\'etica Pol\'itica. Chaptal, ministro del interior franc\'es, publica en 1801 el primer censo general de poblaci\'on, desarrolla los estudios industriales, de las producciones y los cambios, haci\'endose sistem\'aticos durantes las dos terceras partes del siglo XIX.

\item \textbf{Fase 3: Estad\'istica y C\'alculo de Probabilidades:} El c\'alculo de probabilidades se incorpora r\'apidamente como un instrumento de an\'alisis extremadamente poderoso para el estudio de los fen\'omenos econ\'omicos y sociales y en general para el estudio de fen\'omenos cuyas causas son demasiados complejas para conocerlos totalmente y hacer posible su an\'alisis.

\end{itemize}


La Estad\'istica para su mejor estudio se ha dividido en dos grandes ramas: \textbf{la Estad\'istica Descriptiva y la Estad\'istica Inferencial}.

\begin{itemize}
    \item \textbf{Descriptiva:} consiste sobre todo en la presentaci\'on de datos en forma de tablas y gr\'aficas. Esta comprende cualquier actividad relacionada con los datos y est\'a dise\~nada para resumir o describir los mismos sin factores pertinentes adicionales; esto es, sin intentar inferir nada que vaya m\'as all\'a de los datos, como tales.
    \item \textbf{Inferencial:} se deriva de muestras, de observaciones hechas s\'olo acerca de una parte de un conjunto numeroso de elementos y esto implica que su an\'alisis requiere de generalizaciones que van m\'as all\'a de los datos. Como consecuencia, la caracter\'istica m\'as importante del reciente crecimiento de la estad\'istica ha sido un cambio en el \'enfasis de los m\'etodos que describen a m\'etodos que sirven para hacer generalizaciones. La Estad\'istica Inferencial investiga o analiza una poblaci\'on partiendo de una muestra tomada.
\end{itemize}

\section*{Estad\'istica Inferencial}

Los m\'etodos b\'asicos de la estad\'istica inferencial son la estimaci\'on y el contraste de hip\'otesis, que juegan un papel fundamental en la investigaci\'on. Por tanto, algunos de los objetivos que se persiguen son:

\begin{itemize}
    \item Calcular los par\'ametros de la distribuci\'on de medias o proporciones muestrales de tama\~no $n$, extra\'idas de una poblaci\'on de media y varianza conocidas.
    \item Estimar la media o la proporci\'on de una poblaci\'on a partir de la media o proporci\'on muestral.
    \item Utilizar distintos tama\~nos muestrales para controlar la confianza y el error admitido.
    \item Contrastar los resultados obtenidos a partir de muestras.
    \item Visualizar gr\'aficamente, mediante las respectivas curvas normales, las estimaciones realizadas.
\end{itemize}

En definitiva, la idea es, a partir de una poblaci\'on se extrae una muestra por algunos de los m\'etodos existentes, con la que se generan datos num\'ericos que se van a utilizar para generar estad\'isticos con los que realizar estimaciones o contrastes poblacionales. Existen dos formas de estimar par\'ametros: la \textit{estimaci\'on puntual} y la \textit{estimaci\'on por intervalo de confianza}. En la primera se busca, con base en los datos muestrales, un \'unico valor estimado para el par\'ametro. Para la segunda, se determina un intervalo dentro del cual se encuentra el valor del par\'ametro, con una probabilidad determinada.

Si el objetivo del tratamiento estad\'istico inferencial, es efectuar generalizaciones acerca de la estructura, composici\'on o comportamiento de las poblaciones no observadas, a partir de una parte de la poblaci\'on, ser\'a necesario que la proporci\'on de poblaci\'on examinada sea representativa del total. Por ello, la selecci\'on de la muestra requiere unos requisitos que lo garanticen, debe ser representativa y aleatoria. 

Adem\'as, la cantidad de elementos que integran la muestra (el tama\~no de la muestra) depende de m\'ultiples factores, como el dinero y el tiempo disponibles para el estudio, la importancia del tema analizado, la confiabilidad que se espera de los resultados, las caracter\'isticas propias del fen\'omeno analizado, etc\'etera. 

As\'i, a partir de la muestra seleccionada se realizan algunos c\'alculos y se estima el valor de los par\'ametros de la poblaci\'on tales como la media, la varianza, la desviaci\'on est\'andar, o la forma de la distribuci\'on, etc.


El conjunto de los m\'etodos que se utilizan para medir las caracter\'isticas de la informaci\'on, para resumir los valores individuales, y para analizar los datos a fin de extraerles el m\'aximo de informaci\'on, es lo que se llama \textit{m\'etodos estad\'isticos}. Los m\'etodos de an\'alisis para la informaci\'on cuantitativa se pueden dividir en los siguientes seis pasos:

\begin{itemize}
    \item Definici\'on del problema.
    \item Recopilaci\'on de la informaci\'on existente.
    \item Obtenci\'on de informaci\'on original.
    \item Clasificaci\'on.
    \item Presentaci\'on.
    \item An\'alisis.
\end{itemize}

El centro de gravedad de la metodolog\'ia estad\'istica se empieza a desplazar t\'ecnicas de computaci\'on intensiva aplicadas a grandes masas de datos, y se empieza a considerar el m\'etodo estad\'istico como un proceso iterativo de b\'usqueda del modelo ideal. Las aplicaciones en este periodo de la Estad\'istica a la Econom\'ia conducen a una disciplina con contenido propio: la Econometr\'ia. La investigaci\'on estad\'istica en problemas militares durante la segunda guerra mundial y los nuevos m\'etodos de programaci\'on matem\'atica, dan lugar a la Investigaci\'on Operativa. El tratamiento de los datos de la investigaci\'on cient\'ifica tiene varias etapas:

\begin{itemize}
    \item En la etapa de recolecci\'on de datos del m\'etodo cient\'ifico, se define a la poblaci\'on de inter\'es y se selecciona una muestra o conjunto de personas representativas de la misma, se realizan experimentos o se emplean instrumentos ya existentes o de nueva creaci\'on, para medir los atributos de inter\'es necesarios para responder a las preguntas de investigaci\'on. Durante lo que es llamado trabajo de campo se obtienen los datos en crudo, es decir las respuestas directas de los sujetos uno por uno, se codifican (se les asignan valores a las respuestas), se capturan y se verifican para ser utilizados en las siguientes etapas.
    \item En la etapa de recuento, se organizan y ordenan los datos obtenidos de la muestra. Esta ser\'a descrita en la siguiente etapa utilizando la estad\'istica descriptiva, todas las investigaciones utilizan estad\'istica descriptiva, para conocer de manera organizada y resumida las caracter\'isticas de la muestra.
    \item En la etapa de an\'alisis se utilizan las pruebas estad\'isticas (estad\'istica inferencial) y en la interpretaci\'on se acepta o rechaza la hip\'otesis nula.
\end{itemize}

\section*{Niveles de medici\'on y tipos de variables}

Para poder emplear el m\'etodo estad\'istico en un estudio es necesario medir las variables. 

\begin{itemize}
    \item Medir: es asignar valores a las propiedades de los objetos bajo ciertas reglas, esas reglas son los niveles de medici\'on.
    \item Cuantificar: es asignar valores a algo tomando un patr\'on de referencia. Por ejemplo, cuantificar es ver cu\'antos hombres y cu\'antas mujeres hay.
\end{itemize}

\textbf{Variable:} es una caracter\'istica o propiedad que asume diferentes valores dentro de una poblaci\'on de inter\'es y cuya variaci\'on es susceptible de medirse.

Las variables pueden clasificarse de acuerdo al tipo de valores que puede tomar como:

\begin{itemize}
\item \textbf{Discretas o categ\'oricas} en las que los valores se relacionan a nombres, etiquetas o categor\'ias, no existe un significado num\'erico directo.
\item \textbf{Continuas} los valores tienen un correlato num\'erico directo, son continuos y susceptibles de fraccionarse y de poder utilizarse en operaciones aritm\'eticas.
\item \textbf{Dicot\'omica} s\'olo tienen dos valores posibles, la caracter\'istica est\'a ausente o presente.
\end{itemize}

En cuanto a una clasificaci\'on estad\'istica, las varibles pueden ser:

\begin{itemize}
\item \textbf{Aleatoria} Aquella en la cual desconocemos el valor porque fluct\'ua de acuerdo a un evento debido al azar.
\item \textbf{Determin\'istica} Aquella variable de la que se conoce el valor.
\item \textbf{Independiente} aquellas variables que son manipuladas por el investigador. Define los grupos.
\item \textbf{Dependiente} son mediciones que ocurren durante el experimento o tratamiento (resultado de la independiente), es la que se mide y compara entre los grupos.
\end{itemize}

En lo que tiene que ver con los \textbf{Niveles de Medici\'on} tenemoss distintos tipos de variable

\begin{itemize}
\item \textbf{Nominal:} Las propiedades de la medici\'on nominal son:
\begin{itemize}
\item Exhaustiva: implica a todas las opciones.
\item A los sujetos se les asignan categor\'ias, por lo que son mutuamente excluyentes. Es decir, la variable est\'a presente o no; tiene o no una caracter\'istica.
\end{itemize}
\item \textbf{Ordinal:} Las propiedades de la medici\'on ordinal son:
\begin{itemize}
\item El nivel ordinal posee transitividad, por lo que se tiene la capacidad de identificar que es mejor o mayor que otra, en ese sentido se pueden establecer jerarqu\'ias.
\item Las distancias entre un valor y otro no son iguales.
\end{itemize}
\item \textbf{Intervalo:} 
\begin{itemize}
\item El nivel de medici\'on intervalar requiere distancias iguales entre cada valor. Por lo general utiliza datos cuantitativos. Por ejemplo: temperatura, atributos psicol\'ogicos (CI, nivel de autoestima, pruebas de conocimientos, etc.)
\item Las unidades de calificaci\'on son equivalentes en todos los puntos de la escala. Una escala de intervalos implica: clasificaci\'on, magnitud y unidades de tama\~nos iguales (Brown, 2000).
\item Se pueden hacer operaciones aritm\'eticas.
\item Cuando se le pide al sujeto que califique una situaci\'on del 0 al 10 puede tomarse como un nivel de medici\'on de intervalo, siempre y cuando se incluya el 0.
\end{itemize}
\item \textbf{Raz\'on:} 
\begin{itemize}
\item La escala empieza a partir del 0 absoluto, por lo tanto incluye s\'olo los n\'umeros por su valor en s\'i, por lo que no pueden existir los n\'umeros con signo negativo. Por ejemplo: Peso corporal en kg., edad en a\~nos, estatura en cm.
\end{itemize}
\end{itemize}

\section*{Definiciones adicionales}

\begin{itemize}
    \item \textbf{Variable:} Consideraciones que una variable son una caracter\'istica o fen\'omeno que puede tomar distintos valores.
    \item \textbf{Dato:} Mediciones o cualidades que han sido recopiladas como resultado de observaciones.
    \item \textbf{Poblaci\'on:} Se considera el \'area de la cual son extra\'idos los datos. Es decir, es el conjunto de elementos o individuos que poseen una caracter\'istica com\'un y medible acerca de lo cual se desea informaci\'on. Es tambi\'en llamado Universo.
    \item \textbf{Muestra:} Es un subconjunto de la poblaci\'on, seleccionado de acuerdo a una regla o alg\'un plan de muestreo.
    \item \textbf{Censo:} Recopilaci\'on de todos los datos (de inter\'es para la investigaci\'on) de la poblaci\'on.
    \item \textbf{Estad\'istica:} Es una funci\'on o f\'ormula que depende de los datos de la muestra (es variable).
    \item \textbf{Par\'ametro:} Caracter\'istica medible de la poblaci\'on. Es un resumen num\'erico de alguna variable observada de la poblaci\'on. Los par\'ametros normales que se estudian son: \textit{La media poblacional, Proporci\'on.}
    \item \textbf{Estimador:} Un estimador de un par\'ametro es un estad\'istico que se emplea para conocer el par\'ametro desconocido.
    \item \textbf{Estad\'istico:} Es una funci\'on de los valores de la muestra. Es una variable aleatoria, cuyos valores dependen de la muestra seleccionada. Su distribuci\'on de probabilidad, se conoce como \textit{Distribuci\'on muestral del estad\'istico}.
    \item \textbf{Estimaci\'on:} Este t\'ermino indica que a partir de lo observado en una muestra (un resumen estad\'istico con las medidas que conocemos de Descriptiva) se extrapola o generaliza dicho resultado muestral a la poblaci\'on total, de modo que lo estimado es el valor generalizado a la poblaci\'on. Consiste en la b\'usqueda del valor de los par\'ametros poblacionales objeto de estudio. Puede ser puntual o por intervalo de confianza:
    \begin{itemize}
        \item \textit{Puntual:} cuando buscamos un valor concreto. Un estimador de un par\'ametro poblacional es una funci\'on de los datos muestrales. En pocas palabras, es una f\'ormula que depende de los valores obtenidos de una muestra, para realizar estimaciones. Lo que se pretende obtener es el valor exacto de un par\'ametro.
    \item \textit{Intervalo de confianza:} cuando determinamos un intervalo, dentro del cual se supone que va a estar el valor del par\'ametro que se busca con una cierta probabilidad. El intervalo de confianza est\'a determinado por dos valores dentro de los cuales afirmamos que est\'a el verdadero par\'ametro con cierta probabilidad. Son unos l\'imites o margen de variabilidad que damos al valor estimado, para poder afirmar, bajo un criterio de probabilidad, que el verdadero valor no los rebasar\'a.

Este intervalo contiene al par\'ametro estimado con una determinada certeza o nivel de confianza. 
\end{itemize}

En la estimaci\'on por intervalos se usan los siguientes conceptos:

\item Variabilidad del par\'ametro: Si no se conoce, puede obtenerse una aproximaci\'on en los datos o en un estudio piloto. Tambi\'en hay m\'etodos para calcular el tama\~no de la muestra que prescinden de este aspecto. Habitualmente se usa como medida de esta variabilidad la desviaci\'on t\'ipica poblacional.
\item Error de la estimaci\'on: Es una medida de su precisi\'on que se corresponde con la amplitud del intervalo de confianza. Cuanta m\'as precisi\'on se desee en la estimaci\'on de un par\'ametro, m\'as estrecho deber\'a ser el intervalo de confianza y, por tanto, menor el error, y m\'as sujetos deber\'an incluirse en la muestra estudiada. 
\item Nivel de confianza: Es la probabilidad de que el verdadero valor del par\'ametro estimado en la poblaci\'on se sit\'ue en el intervalo de confianza obtenido. El nivel de confianza se denota por $1-\alpha$
\item $p$-value : Tambi\'en llamado nivel de significaci\'on. Es la probabilidad (en tanto por uno) de fallar en nuestra estimaci\'on, esto es, la diferencia entre la certeza (1) y el nivel de confianza $1-\alpha$. 
\item Valor cr\'itico: Se representa por $Z_{\alpha/2}$. Es el valor de la abscisa en una determinada distribuci\'on que deja a su derecha un \'area igual a 1/2, siendo $1-\alpha$ el nivel de confianza. Normalmente los valores cr\'iticos est\'an tabulados o pueden calcularse en funci\'on de la distribuci\'on de la poblaci\'on.

\end{itemize}

Para un tama\~no fijo de la muestra, los conceptos de error y nivel de confianza van relacionados. Si admitimos un error mayor, esto es, aumentamos el tama\~no del intervalo de confianza, tenemos tambi\'en una mayor probabilidad de \'exito en nuestra estimaci\'on, es decir, un mayor nivel de confianza. Por tanto, un aspecto que debe de tenerse en cuenta es el tama\~no muestral, ya que para disminuir el error que se comente habr\'a que aumentar el tama\~no muestral. Esto se resolver\'a, para un intervalo de confianza cualquiera, despejando el tama\~no de la muestra en cualquiera de las formulas de los intervalos de confianza que veremos a continuaci\'on, a partir del error m\'aximo permitido. Los intervalos de confianza pueden ser unilaterales o bilaterales:

\begin{itemize}
    \item \textbf{Contraste de Hip\'otesis:} Consiste en determinar si es aceptable, partiendo de datos muestrales, que la caracter\'istica o el par\'ametro poblacional estudiado tome un determinado valor o est\'e dentro de unos determinados valores.
    \item \textbf{Nivel de Confianza:} Indica la proporci\'on de veces que acertar\'iamos al afirmar que el par\'ametro est\'a dentro del intervalo al seleccionar muchas muestras.
\end{itemize}

\section{Muestreo:} 

\textbf{Muestreo:} Una muestra es representativa en la medida que es imagen de la poblaci\'on. En general, podemos decir que el tama\~no de una muestra depender\'a principalmente de: \textit{Nivel de precisi\'on deseado, Recursos disponibles, Tiempo involucrado en la investigaci\'on.} Adem\'as el plan de muestreo debe considerar \textit{La poblaci\'on, Par\'ametros a medir}. Existe una gran cantidad de tipos de muestreo, en la pr\'actica los m\'as utilizados son los siguientes:


\begin{itemize}
    \item \textbf{MUESTREO ALEATORIO SIMPLE:} Es un m\'etodo de selecci\'on de $n$ unidades extra\'idas de $N$, de tal manera que cada una de las posibles muestras tiene la misma probabilidad de ser escogida. (En la pr\'actica, se enumeran las unidades de 1 a $N$, y a continuaci\'on se seleccionan $n$ n\'umeros aleatorios entre 1 y $N$, ya sea de tablas o de alguna urna con fichas numeradas).
    \item \textbf{MUESTREO ESTRATIFICADO ALEATORIO:} Se usa cuando la poblaci\'on est\'a agrupada en pocos estratos, cada uno de ellos son muchas entidades. Este muestreo consiste en sacar una muestra aleatoria simple de cada uno de los estratos. (Generalmente, de tama\~no proporcional al estrato).
    \item \textbf{MUESTREO SISTEM\'ATICO:} Se utiliza cuando las unidades de la poblaci\'on est\'an de alguna manera totalmente ordenadas. Para seleccionar una muestra de $n$ unidades, se divide la poblaci\'on en $n$ subpoblaciones de tama\~no $K = N/n$ y se toma al azar una unidad de la $K$ primeras y de ah\'i en adelante cada $K$-\'esima unidad.
    \item \textbf{MUESTREO POR CONGLOMERADO:} Se emplea cuando la poblaci\'on est\'a dividida en grupos o conglomerados peque\~nos. Consiste en obtener una muestra aleatoria simple de conglomerados y luego CENSAR cada uno de \'estos.
    \item \textbf{MUESTREO EN DOS ETAPAS (Biet\'apico):} En este caso la muestra se toma en dos pasos:
    \begin{itemize}
        \item Seleccionar una muestra de unidades primarias, y 
        \item Seleccionar una muestra de elementos a partir de cada unidad primaria escogida.
        \item \textit{Observaci\'on:} En la realidad es posible encontrarse con situaciones en las cuales no es posible aplicar libremente un tipo de muestreo, incluso estaremos obligados a mezclarlas en ocasiones.
    \end{itemize}
\end{itemize}


\section{Errores Estad\'isticos Comunes}

El prop\'osito de esta secci\'on es solamente indicar los malos usos comunes de datos estad\'isticos, sin incluir el uso de m\'etodos estad\'isticos complicados. Un estudiante deber\'ia estar alerta en relaci\'on con estos malos usos y deber\'ia hacer un gran esfuerzo para evitarlos a fin de ser un verdadero estad\'istico.

\textbf{Datos estad\'isticos inadecuados:} Los datos estad\'isticos son usados como la materia prima para un estudio estad\'istico. Cuando los datos son inadecuados, la conclusi\'on extra\'ida del estudio de los datos se vuelve obviamente inv\'alida. Por ejemplo, supongamos que deseamos encontrar el ingreso familiar t\'ipico del a\~no pasado en la ciudad Y de 50,000 familias y tenemos una muestra consistente del ingreso de solamente tres familias: 1 mill\'on, 2 millones y no ingreso. Si sumamos el ingreso de las tres familias y dividimos el total por 3, obtenemos un promedio de 1 mill\'on. Entonces, extraemos una conclusi\'on basada en la muestra de que el ingreso familiar promedio durante el a\~no pasado en la ciudad fue de 1 mill\'on. Es obvio que la conclusi\'on es falsa, puesto que las cifras son extremas y el tama\~no de la muestra es demasiado peque\~no; por lo tanto la muestra no es representativa. 

Hay muchas otras clases de datos inadecuados. Por ejemplo, algunos datos son respuestas inexactas de una encuesta, porque las preguntas usadas en la misma son vagas o enga\~nosas, algunos datos son toscas estimaciones porque no hay disponibles datos exactos o es demasiado costosa su obtenci\'on, y algunos datos son irrelevantes en un problema dado, porque el estudio estad\'istico no est\'a bien planeado. Al momento de recopilar los datos que ser\'an procesados se es susceptible de cometer errores as\'i como durante los c\'omputos de los mismos. No obstante, hay otros errores que no tienen nada que ver con la digitaci\'on y que no son tan f\'acilmente identificables. Algunos de \'estos errores son:

\begin{itemize}
    \item \textbf{Sesgo:} Es imposible ser completamente objetivo o no tener ideas preconcebidas antes de comenzar a estudiar un problema, y existen muchas maneras en que una perspectiva o estado mental pueda influir en la recopilaci\'on y en el an\'alisis de la informaci\'on. En estos casos se dice que hay un sesgo cuando el individuo da mayor peso a los datos que apoyan su opini\'on que a aquellos que la contradicen. Un caso extremo de sesgo ser\'ia la situaci\'on donde primero se toma una decisi\'on y despu\'es se utiliza el an\'alisis estad\'istico para justificar la decisi\'on ya tomada.
    \item \textbf{Datos No Comparables:} el establecer comparaciones es una de las partes m\'as importantes del an\'alisis estad\'istico, pero es extremadamente importante que tales comparaciones se hagan entre datos que sean comparables.
    \item \textbf{Proyecci\'on descuidada de tendencias:} la proyecci\'on simplista de tendencias pasadas hacia el futuro es uno de los errores que m\'as ha desacreditado el uso del an\'alisis estad\'istico.
    \item \textbf{Muestreo Incorrecto:} en la mayor\'ia de los estudios sucede que el volumen de informaci\'on disponible es tan inmenso que se hace necesario estudiar muestras, para derivar conclusiones acerca de la poblaci\'on a que pertenece la muestra. Si la muestra se selecciona correctamente, tendr\'a b\'asicamente las mismas propiedades que la poblaci\'on de la cual fue extra\'ida; pero si el muestreo se realiza incorrectamente, entonces puede suceder que los resultados no signifiquen nada.
\end{itemize}

\textbf{Sesgo} significa que un usuario d\'e los datos perjudicialmente de m\'as \'enfasis a los hechos, los cuales son empleados para mantener su predeterminada posici\'on u opini\'on. Los estad\'isticos son frecuentemente degradados por lemas tales como: \textit{Hay tres clases de mentiras: mentiras, mentiras reprobables y estad\'istica, y Las cifras no mienten, pero los mentirosos piensan}. Hay dos clases de sesgos: conscientes e inconscientes. Ambos son comunes en el an\'alisis estad\'istico. Hay numerosos ejemplos de sesgos conscientes. Un anunciante frecuentemente usa la estad\'istica para probar que su producto es muy superior al producto de su competidor. Un pol\'itico prefiere usar la estad\'istica para sostener su punto de vista. Gerentes y l\'ideres de trabajadores pueden simult\'aneamente situar sus respectivas cifras estad\'isticas sobre la misma tabla de trato para mostrar que sus rechazos o peticiones son justificadas. Es casi imposible que un sesgo inconsciente est\'e completamente ausente en un trabajo estad\'istico. En lo que respecta al ser humano, es dif\'icil obtener una actitud completamente objetiva al abordar un problema, aun cuando un cient\'ifico deber\'ia tener una mente abierta. Un estad\'istico deber\'ia estar enterado del hecho de que su interpretaci\'on de los resultados del an\'alisis estad\'istico est\'a influenciado por su propia experiencia, conocimiento y antecedentes con relaci\'on al problema dado.








%_________________________
\chapter{documento: Bases}
%_________________________

%---------------------------------------------------------
\section{An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------



\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

\end{Note}



%---------------------------------------------------------
\section{An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------


\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

\end{Note}





\subsection{Regresi\'on Lineal Simple (RLS)}




Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y  $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}

donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.






Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$,  estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.

Entonces la ecuaci\'on (\ref{Modelo.Regresion}) se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on 
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}





Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$,  $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$,  igualar a cero y  resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene 

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}





Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}

Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto





\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}




\subsection{Regresi\'on Lineal Simple (RLS)}





Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y  $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}





donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$,  estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.


Entonces la ecuaci\'on (\ref{Modelo.Regresion}) se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on 
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}





Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$,  igualar a cero y  resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}





Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}

Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}




%---------------------------------------------------------
\section{3. An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------




\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\end{itemize}

\end{Note}




\subsection{3.1 Regresi\'on Lineal Simple (RLS)}





\begin{itemize}

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 





\subsection{3.2 M\'etodo de M\'inimos Cuadrados}




Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}
donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$, estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.







Entonces la ecuaci\'on \ref{Modelo.Regresion} se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}






Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$, igualar a cero y resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ecs.Estimadores.Regresion}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}





entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}
Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}




\subsection{3.3 Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}


\begin{Note}
\begin{itemize}
\item Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo.

\item Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.
\end{itemize}
\end{Note}
A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)
\end{eqnarray*}
\begin{eqnarray*}
&=&\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)\right.\\
&+&\left.E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]=\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}
\end{eqnarray*}






por lo tanto 
\begin{equation}\label{Esperanza.Beta.1}
E\left(\hat{\beta}_{1}\right)=\beta_{1}
\end{equation}
\begin{Note}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado.
\end{Note}
Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}\\
&=&\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}}
\end{eqnarray*}






por lo tanto
\begin{equation}\label{Varianza.Beta.1}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}
\end{equation}
\begin{Prop}
\begin{eqnarray*}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray*}
\end{Prop}
Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray}






sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=MC_{E}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray*}



\subsection{3.4 Prueba de Hip\'otesis en RLS}




\begin{itemize}
\item Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza.

\item Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$.

\end{itemize}






Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:
\begin{centering}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,

\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.

\end{itemize}
donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. 
\end{centering}





De las ecuaciones (\ref{Ecs.Estimadores.Regresion}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1}) y (\ref{Varianza.Beta.1}).
Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}







Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}






\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}=\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}

\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}






\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
\end{eqnarray*}

\begin{eqnarray*}
&=&\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}






Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}
Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados}) se puede reescribir como 
\begin{equation}\label{Suma.Total.Cuadrados.Dos}
S_{yy}=SC_{R}+SC_{E}
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right)\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.






\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx}
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}
Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$.

El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente\medskip






\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 

La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1}), con $\hat{\beta}_{1,0}=0$, es decir
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}





Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.




\subsection{Estimaci\'on de Intervalos en RLS}




\begin{itemize}
\item Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros.

\item El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on.

\end{itemize}






Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por
\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\small{
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}}



%---------------------------------------------------------
\section{3. An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------



\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\end{itemize}

\end{Note}





\subsection{3.1 Regresi\'on Lineal Simple (RLS)}





\begin{itemize}

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 




\subsection{3.2 M\'etodo de M\'inimos Cuadrados}




Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}
donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$, estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.









Entonces la ecuaci\'on \ref{Modelo.Regresion} se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}
Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$, igualar a cero y resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}





evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene
\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ecs.Estimadores.Regresion}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}





entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}
Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}





\subsection{3.3 Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}





\begin{Note}
\begin{itemize}
\item Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo.

\item Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.
\end{itemize}
\end{Note}
A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)
\end{eqnarray*}






\begin{eqnarray*}
&=&\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)\right.\\
&+&\left.E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]=\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}
\end{eqnarray*}
por lo tanto 
\begin{equation}\label{Esperanza.Beta.1}
E\left(\hat{\beta}_{1}\right)=\beta_{1}
\end{equation}






\begin{Note}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado.
\end{Note}
Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}\\
&=&\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}}
\end{eqnarray*}







por lo tanto
\begin{equation}\label{Varianza.Beta.1}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}
\end{equation}
\begin{Prop}
\begin{eqnarray*}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray*}
\end{Prop}

Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray}






sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=MC_{E}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray*}






%\end{document}
\subsection{3.4 Prueba de Hip\'otesis en RLS}




\begin{itemize}
\item Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza.

\item Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$.

\end{itemize}


Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:
\begin{centering}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,

\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.

\end{itemize}

donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. 
\end{centering}




De las ecuaciones (\ref{Ecs.Estimadores.Regresion}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1}) y (\ref{Varianza.Beta.1}).

Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}







Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}








\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}=\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}






\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}

\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
\end{eqnarray*}





\begin{eqnarray*}
&=&\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}

Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}





Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados}) se puede reescribir como 
\begin{equation}\label{Suma.Total.Cuadrados.Dos}
S_{yy}=SC_{R}+SC_{E}
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right)\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.

\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx}
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}




Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$.
									
El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente\medskip

\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 





La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1}), con $\hat{\beta}_{1,0}=0$, es decir
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.





\subsection{Estimaci\'on de Intervalos en RLS}



\begin{itemize}
\item Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros.

\item El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on.

\end{itemize}

Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por




\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\small{
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}}


%\end{document}
\subsection{Predicci\'on}


Supongamos que se tiene un valor $x_{0}$ de inter\'es, entonces la estimaci\'on puntual de este nuevo valor
\begin{equation}
\hat{y}_{0}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}
\end{equation}
\begin{Note}
Esta nueva observaci\'on es independiente de las utilizadas para obtener el modelo de regresi\'on, por tanto, el intervalo en torno a la recta de regresi\'on es inapropiado, puesto que se basa \'unicamente en los datos empleados para ajustar el modelo de regresi\'on.\\

El intervalo de confianza en torno a la recta de regresi\'on se refiere a la respuesta media verdadera $x=x_{0}$, no a observaciones futuras.
\end{Note}






Sea $y_{0}$ la observaci\'on futura en $x=x_{0}$, y sea $\hat{y}_{0}$ dada en la ecuaci\'on anterior, el estimador de $y_{0}$. Si se define la variable aleatoria $$w=y_{0}-\hat{y}_{0},$$ esta se distribuye normalmente con media cero y varianza $$V\left(w\right)=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]$$
dado que $y_{0}$ es independiente de $\hat{y}_{0}$, por lo tanto el intervalo de predicci\'on al nivel $\alpha$ para futuras observaciones $x_{0}$ es


\begin{eqnarray*}
\hat{y}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}\leq y_{0}\\
\leq \hat{y}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}.
\end{eqnarray*}





\subsection{Prueba de falta de ajuste}
%\frametitle{Falta de ajuste}
Es com\'un encontrar que el modelo ajustado no satisface totalmente el modelo necesario para los datos, en este caso es preciso saber qu\'e tan bueno es el modelo propuesto. Para esto se propone la siguiente prueba de hip\'otesis:
\begin{itemize}
\item[$H_{0}:$ ]El modelo propuesto se ajusta adecuademente a los datos.
\item[$H_{1}:$ ]El modelo NO se ajusta a los datos.
\end{itemize}
La prueba implica dividir la suma de cuadrados del eror o del residuo en las siguientes dos componentes:
\begin{eqnarray*}
SC_{E}=SC_{EP}+SC_{FDA}
\end{eqnarray*}



%\frametitle{Falta de ajuste}
donde $SC_{EP}$ es la suma de cuadrados atribuibles al error puro, y $SC_{FDA}$ es la suma de cuadrados atribuible a la falta de ajuste del modelo.


%%\frametitle{Falta de ajuste}
%


\subsection{Coeficiente de Determinaci\'on}



La cantidad
\begin{equation}
R^{2}=\frac{SC_{R}}{S_{yy}}=1-\frac{SC_{E}}{S_{yy}}
\end{equation}
se denomina coeficiente de determinaci\'on y se utiliza para saber si el modelo de regresi\'on es suficiente o no. Se puede demostrar que $0\leq R^{2}\leq1$, una manera de interpretar este valor es que si $R^{2}=k$, entonces el modelo de regresi\'on explica el $k*100\%$ de la variabilidad en los datos.
$R^{2}$ 
\begin{itemize}
\item No mide la magnitud de la pendiente de la recta de regresi\'on
\item Un valor grande de $R^{2}$ no implica una pendiente empinada.
\item No mide la suficiencia del modelo.
\item Valores grandes de $R^{2}$ no implican necesariamente que el modelo de regresi\'on proporcionar\'a predicciones precisas para futuras observaciones.
\end{itemize}


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Introducci\'on}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Introducci\'on}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

La Estad\'istica es una ciencia formal que estudia la recolecci\'on, an\'alisis e interpretaci\'on de datos de una muestra representativa, ya sea para ayudar en la toma de decisiones o para explicar condiciones regulares o irregulares de alg\'un fen\'omeno o estudio aplicado, de ocurrencia en forma aleatoria o condicional. Sin embargo, la estad\'istica es m\'as que eso, es decir, es transversal a una amplia variedad de disciplinas, desde la f\'isica hasta las ciencias sociales, desde las ciencias de la salud hasta el control de calidad. Se usa para la toma de decisiones en \'areas de negocios o instituciones gubernamentales. Ahora bien, las t\'ecnicas estad\'isticas se aplican de manera amplia en mercadotecnia, contabilidad, control de calidad y en otras actividades; estudios de consumidores; an\'alisis de resultados en deportes; administradores de instituciones; en la educaci\'on; organismos pol\'iticos; m\'edicos; y por otras personas que intervienen en la toma de decisiones.

\begin{Def}
    La Estad\'istica es la ciencia cuyo objetivo es reunir una informaci\'on cuantitativa concerniente a individuos, grupos, series de hechos, etc. y deducir de ello gracias al an\'alisis de estos datos unos significados precisos o unas previsiones para el futuro.
\end{Def}

La estad\'istica, en general, es la ciencia que trata de la recopilaci\'on, organizaci\'on presentaci\'on, an\'alisis e interpretaci\'on de datos num\'ericos con el fin de realizar una toma de decisi\'on m\'as efectiva. Los m\'etodos estad\'isticos tradicionalmente se utilizan para prop\'ositos descriptivos, para organizar y resumir datos num\'ericos. La estad\'istica descriptiva, por ejemplo trata de la tabulaci\'on de datos, su presentaci\'on en forma gr\'afica o ilustrativa y el c\'alculo de medidas descriptivas.

%---------------------------------------------------------
\subsection{Historia de la Estad\'istica}
%---------------------------------------------------------
Es dif\'icil conocer los or\'igenes de la Estad\'istica. Desde los comienzos de la civilizaci\'on han existido formas sencillas de estad\'istica, pues ya se utilizaban representaciones gr\'aficas y otros s\'imbolos en pieles, rocas, palos de madera y paredes de cuevas para contar el n\'umero de personas, animales o ciertas cosas. Su origen empieza posiblemente en la isla de Cerde\~na, donde existen monumentos prehist\'oricos pertenecientes a los Nuragas, las primeros habitantes de la isla; estos monumentos constan de bloques de basalto superpuestos sin mortero y en cuyas paredes de encontraban grabados toscos signos que han sido interpretados con mucha verosimilidad como muescas que serv\'ian para llevar la cuenta del ganado y la caza. Los babilonios usaban ya peque\~nas tablillas de arcilla para recopilar datos en tablas sobre la producci\'on agr\'icola y los g\'eneros vendidos o cambiados mediante trueque. Otros vestigios pueden ser hallados en el antiguo Egipto, cuyos faraones lograron recopilar, hacia el a\~no 3050 antes de Cristo, prolijos datos relativos a la poblaci\'on y la riqueza del pa\'is. De acuerdo al historiador griego Her\'odoto, dicho registro de riqueza y poblaci\'on se hizo con el objetivo de preparar la construcci\'on de las pir\'amides. En el mismo Egipto, Rams\'es II hizo un censo de las tierras con el objeto de verificar un nuevo reparto. En el antiguo Israel la Biblia da referencias, en el libro de los N\'umeros, de los datos estad\'isticos obtenidos en dos recuentos de la poblaci\'on hebrea. El rey David por otra parte, orden\'o a Joab, general del ej\'ercito hacer un censo de Israel con la finalidad de conocer el n\'umero de la poblaci\'on. Tambi\'en los chinos efectuaron censos hace m\'as de cuarenta siglos. Los griegos efectuaron censos peri\'odicamente con fines tributarios, sociales (divisi\'on de tierras) y militares (c\'alculo de recursos y hombres disponibles). La investigaci\'on hist\'orica revela que se realizaron 69 censos para calcular los impuestos, determinar los derechos de voto y ponderar la potencia guerrera. \medskip

Fueron los romanos, maestros de la organizaci\'on pol\'itica, quienes mejor supieron emplear los recursos de la estad\'istica. Cada cinco a\~nos realizaban un censo de la poblaci\'on y sus funcionarios p\'ublicos ten\'ian la obligaci\'on de anotar nacimientos, defunciones y matrimonios, sin olvidar los recuentos peri\'odicos del ganado y de las riquezas contenidas en las tierras conquistadas. Para el nacimiento de Cristo suced\'ia uno de estos empadronamientos de la poblaci\'on bajo la autoridad del imperio. Durante los mil a\~nos siguientes a la ca\'ida del imperio Romano se realizaron muy pocas operaciones Estad\'isticas, con la notable excepci\'on de las relaciones de tierras pertenecientes a la Iglesia, compiladas por Pipino el Breve en el 758 y por Carlomagno en el 762 DC. Durante el siglo IX se realizaron en Francia algunos censos parciales de siervos. En Inglaterra, Guillermo el Conquistador recopil\'o el Domesday Book o libro del Gran Catastro para el a\~no 1086, un documento de la propiedad, extensi\'on y valor de las tierras de Inglaterra. Esa obra fue el primer compendio estad\'istico de Inglaterra.  Aunque Carlomagno, en Francia; y Guillermo el Conquistador, en Inglaterra, trataron de revivir la t\'ecnica romana, los m\'etodos estad\'isticos permanecieron casi olvidados durante la Edad Media.


Durante los siglos XV, XVI, y XVII, hombres como Leonardo de Vinci, Nicol\'as Cop\'ernico, Galileo, Neper, William Harvey, Sir Francis Bacon y Ren\'e Descartes, hicieron grandes operaciones al m\'etodo cient\'ifico, de tal forma que cuando se crearon los Estados Nacionales y surgi\'o como fuerza el comercio internacional exist\'ia ya un m\'etodo capaz de aplicarse a los datos econ\'omicos. Para el a\~no 1532 empezaron a registrarse en Inglaterra las defunciones debido al temor que Enrique VII ten\'ia por la peste.  M\'as o menos por la misma \'epoca, en Francia la ley exigi\'o a los cl\'erigos registrar los bautismos, fallecimientos y matrimonios. Durante un brote de peste que apareci\'o a fines de la d\'ecada de 1500, el gobierno ingl\'es comenz\'o a publicar estad\'istica semanales de los decesos. Esa costumbre continu\'o muchos a\~nos, y en 1632 estos Bills of Mortality (Cuentas de Mortalidad) conten\'ian los nacimientos y fallecimientos por sexo. En 1662, el capit\'an John Graunt us\'o documentos que abarcaban treinta a\~nos y efectu\'o predicciones sobre el n\'umero de personas que morir\'ian de varias enfermedades y sobre las proporciones de nacimientos de varones y mujeres que cabr\'ia esperar. El trabajo de Graunt, condensado en su obra \textit{Natural and Political Observations...Made upon the Bills of Mortality}, fue un esfuerzo innovador en el an\'alisis estad\'istico. Por el a\~no 1540 el alem\'an Sebasti\'an Muster realiz\'o una compilaci\'on estad\'istica de los recursos nacionales, comprensiva de datos sobre organizaci\'on pol\'itica, instrucciones sociales, comercio y poder\'io militar. 

Los eruditos del siglo XVII demostraron especial inter\'es por la Estad\'istica Demogr\'afica como resultado de la especulaci\'on sobre si la poblaci\'on aumentaba, decrec\'ia o permanec\'ia est\'atica. En los tiempos modernos tales m\'etodos fueron resucitados por algunos reyes que necesitaban conocer las riquezas monetarias y el potencial humano de sus respectivos pa\'ises. El primer empleo de los datos estad\'isticos para fines ajenos a la pol\'itica tuvo lugar en 1691 y estuvo a cargo de Gaspar Neumann, un profesor alem\'an que viv\'ia en Breslau. Este investigador se propuso destruir la antigua creencia popular de que en los a\~nos terminados en siete mor\'ia m\'as gente que en los restantes, y para lograrlo hurg\'o pacientemente en los archivos parroquiales de la ciudad. Despu\'es de revisar miles de partidas de defunci\'on pudo demostrar que en tales a\~nos no fallec\'ian m\'as personas que en los dem\'as. Los procedimientos de Neumann fueron conocidos por el astr\'onomo ingl\'es Halley, descubridor del cometa que lleva su nombre, quien los aplic\'o al estudio de la vida humana. 

Durante el siglo XVII y principios del XVIII, matem\'aticos como Bernoulli, Francis Maseres, Lagrange y Laplace desarrollaron la teor\'ia de probabilidades. No obstante durante cierto tiempo, la teor\'ia de las probabilidades limit\'o su aplicaci\'on a los juegos de azar y hasta el siglo XVIII no comenz\'o a aplicarse a los grandes problemas cient\'ificos. Godofredo Achenwall, profesor de la Universidad de Gotinga, acu\~n\'o en 1760 la palabra estad\'istica, que extrajo del t\'ermino italiano statista (estadista). Cre\'ia, y con sobrada raz\'on, que los datos de la nueva ciencia ser\'ian el aliado m\'as eficaz del gobernante consciente. La ra\'iz remota de la palabra se halla, por otra parte, en el t\'ermino latino status, que significa estado o situaci\'on; Esta etimolog\'ia aumenta el valor intr\'inseco de la palabra, por cuanto la estad\'istica revela el sentido cuantitativo de las m\'as variadas situaciones. Jacques Qu\'etelect es quien aplica las Estad\'isticas a las ciencias sociales. Este interpret\'o la teor\'ia de la probabilidad para su uso en las ciencias sociales y resolver la aplicaci\'on del principio de promedios y de la variabilidad a los fen\'omenos sociales. Qu\'etelect fue el primero en realizar la aplicaci\'on pr\'actica de todo el m\'etodo Estad\'istico, entonces conocido, a las diversas ramas de la ciencia. Entretanto, en el per\'iodo del 1800 al 1820 se desarrollaron dos conceptos matem\'aticos fundamentales para la teor\'ia Estad\'istica; la teor\'ia de los errores de observaci\'on, aportada por Laplace y Gauss; y la teor\'ia de los m\'inimos cuadrados desarrollada por Laplace, Gauss y Legendre. A finales del siglo XIX, Sir Francis Gaston ide\'o el m\'etodo conocido por Correlaci\'on, que ten\'ia por objeto medir la influencia relativa de los factores sobre las variables. De aqu\'i parti\'o el desarrollo del coeficiente de correlaci\'on creado por Karl Pearson y otros cultivadores de la ciencia biom\'etrica como J. Pease Norton, R. H. Hooker y G. Udny Yule, que efectuaron amplios estudios sobre la medida de las relaciones.

La historia de la estad\'istica est\'a resumida en tres grandes etapas o fases.

\begin{itemize}
    \item \textbf{Fase 1: Los Censos:} Desde el momento en que se constituye una autoridad pol\'itica, la idea de inventariar de una forma m\'as o menos regular la poblaci\'on y las riquezas existentes en el territorio est\'a ligada a la conciencia de soberan\'ia y a los primeros esfuerzos administrativos.
    \item \textbf{Fase 2: De la Descripci\'on de los Conjuntos a la Aritm\'etica Pol\'itica:} Las ideas mercantilistas extra\~nan una intensificaci\'on de este tipo de investigaci\'on. Colbert multiplica las encuestas sobre art\'iculos manufacturados, el comercio y la poblaci\'on: los intendentes del Reino env\'ian a Par\'is sus memorias. Vauban, m\'as conocido por sus fortificaciones o su Dime Royale, que es la primera propuesta de un impuesto sobre los ingresos, se se\~nala como el verdadero precursor de los sondeos. M\'as tarde, Buf\'on se preocupa de esos problemas antes de dedicarse a la historia natural. La escuela inglesa proporciona un nuevo progreso al superar la fase puramente descriptiva.

Sus tres principales representantes son Graunt, Petty y Halley. El pen\'ultimo es autor de la famosa Aritm\'etica Pol\'itica. Chaptal, ministro del interior franc\'es, publica en 1801 el primer censo general de poblaci\'on, desarrolla los estudios industriales, de las producciones y los cambios, haci\'endose sistem\'aticos durantes las dos terceras partes del siglo XIX.

\item \textbf{Fase 3: Estad\'istica y C\'alculo de Probabilidades:} El c\'alculo de probabilidades se incorpora r\'apidamente como un instrumento de an\'alisis extremadamente poderoso para el estudio de los fen\'omenos econ\'omicos y sociales y en general para el estudio de fen\'omenos cuyas causas son demasiados complejas para conocerlos totalmente y hacer posible su an\'alisis.

\end{itemize}


La Estad\'istica para su mejor estudio se ha dividido en dos grandes ramas: \textbf{la Estad\'istica Descriptiva y la Estad\'istica Inferencial}.

\begin{itemize}
    \item \textbf{Descriptiva:} consiste sobre todo en la presentaci\'on de datos en forma de tablas y gr\'aficas. Esta comprende cualquier actividad relacionada con los datos y est\'a dise\~nada para resumir o describir los mismos sin factores pertinentes adicionales; esto es, sin intentar inferir nada que vaya m\'as all\'a de los datos, como tales.
    \item \textbf{Inferencial:} se deriva de muestras, de observaciones hechas s\'olo acerca de una parte de un conjunto numeroso de elementos y esto implica que su an\'alisis requiere de generalizaciones que van m\'as all\'a de los datos. Como consecuencia, la caracter\'istica m\'as importante del reciente crecimiento de la estad\'istica ha sido un cambio en el \'enfasis de los m\'etodos que describen a m\'etodos que sirven para hacer generalizaciones. La Estad\'istica Inferencial investiga o analiza una poblaci\'on partiendo de una muestra tomada.
\end{itemize}

%---------------------------------------------------------
\subsection*{Estad\'istica Inferencial}
%---------------------------------------------------------

Los m\'etodos b\'asicos de la estad\'istica inferencial son la estimaci\'on y el contraste de hip\'otesis, que juegan un papel fundamental en la investigaci\'on. Por tanto, algunos de los objetivos que se persiguen son:

\begin{itemize}
    \item Calcular los par\'ametros de la distribuci\'on de medias o proporciones muestrales de tama\~no $n$, extra\'idas de una poblaci\'on de media y varianza conocidas.
    \item Estimar la media o la proporci\'on de una poblaci\'on a partir de la media o proporci\'on muestral.
    \item Utilizar distintos tama\~nos muestrales para controlar la confianza y el error admitido.
    \item Contrastar los resultados obtenidos a partir de muestras.
    \item Visualizar gr\'aficamente, mediante las respectivas curvas normales, las estimaciones realizadas.
\end{itemize}

En definitiva, la idea es, a partir de una poblaci\'on se extrae una muestra por algunos de los m\'etodos existentes, con la que se generan datos num\'ericos que se van a utilizar para generar estad\'isticos con los que realizar estimaciones o contrastes poblacionales. Existen dos formas de estimar par\'ametros: la \textit{estimaci\'on puntual} y la \textit{estimaci\'on por intervalo de confianza}. En la primera se busca, con base en los datos muestrales, un \'unico valor estimado para el par\'ametro. Para la segunda, se determina un intervalo dentro del cual se encuentra el valor del par\'ametro, con una probabilidad determinada.

Si el objetivo del tratamiento estad\'istico inferencial, es efectuar generalizaciones acerca de la estructura, composici\'on o comportamiento de las poblaciones no observadas, a partir de una parte de la poblaci\'on, ser\'a necesario que la proporci\'on de poblaci\'on examinada sea representativa del total. Por ello, la selecci\'on de la muestra requiere unos requisitos que lo garanticen, debe ser representativa y aleatoria. 

Adem\'as, la cantidad de elementos que integran la muestra (el tama\~no de la muestra) depende de m\'ultiples factores, como el dinero y el tiempo disponibles para el estudio, la importancia del tema analizado, la confiabilidad que se espera de los resultados, las caracter\'isticas propias del fen\'omeno analizado, etc\'etera. 

As\'i, a partir de la muestra seleccionada se realizan algunos c\'alculos y se estima el valor de los par\'ametros de la poblaci\'on tales como la media, la varianza, la desviaci\'on est\'andar, o la forma de la distribuci\'on, etc.


El conjunto de los m\'etodos que se utilizan para medir las caracter\'isticas de la informaci\'on, para resumir los valores individuales, y para analizar los datos a fin de extraerles el m\'aximo de informaci\'on, es lo que se llama \textit{m\'etodos estad\'isticos}. Los m\'etodos de an\'alisis para la informaci\'on cuantitativa se pueden dividir en los siguientes seis pasos:

\begin{itemize}
    \item Definici\'on del problema.
    \item Recopilaci\'on de la informaci\'on existente.
    \item Obtenci\'on de informaci\'on original.
    \item Clasificaci\'on.
    \item Presentaci\'on.
    \item An\'alisis.
\end{itemize}

El centro de gravedad de la metodolog\'ia estad\'istica se empieza a desplazar t\'ecnicas de computaci\'on intensiva aplicadas a grandes masas de datos, y se empieza a considerar el m\'etodo estad\'istico como un proceso iterativo de b\'usqueda del modelo ideal. Las aplicaciones en este periodo de la Estad\'istica a la Econom\'ia conducen a una disciplina con contenido propio: la Econometr\'ia. La investigaci\'on estad\'istica en problemas militares durante la segunda guerra mundial y los nuevos m\'etodos de programaci\'on matem\'atica, dan lugar a la Investigaci\'on Operativa. El tratamiento de los datos de la investigaci\'on cient\'ifica tiene varias etapas:

\begin{itemize}
    \item En la etapa de recolecci\'on de datos del m\'etodo cient\'ifico, se define a la poblaci\'on de inter\'es y se selecciona una muestra o conjunto de personas representativas de la misma, se realizan experimentos o se emplean instrumentos ya existentes o de nueva creaci\'on, para medir los atributos de inter\'es necesarios para responder a las preguntas de investigaci\'on. Durante lo que es llamado trabajo de campo se obtienen los datos en crudo, es decir las respuestas directas de los sujetos uno por uno, se codifican (se les asignan valores a las respuestas), se capturan y se verifican para ser utilizados en las siguientes etapas.
    \item En la etapa de recuento, se organizan y ordenan los datos obtenidos de la muestra. Esta ser\'a descrita en la siguiente etapa utilizando la estad\'istica descriptiva, todas las investigaciones utilizan estad\'istica descriptiva, para conocer de manera organizada y resumida las caracter\'isticas de la muestra.
    \item En la etapa de an\'alisis se utilizan las pruebas estad\'isticas (estad\'istica inferencial) y en la interpretaci\'on se acepta o rechaza la hip\'otesis nula.
\end{itemize}
%---------------------------------------------------------
\subsection*{Niveles de medici\'on y tipos de variables}
%---------------------------------------------------------
Para poder emplear el m\'etodo estad\'istico en un estudio es necesario medir las variables. 

\begin{itemize}
    \item Medir: es asignar valores a las propiedades de los objetos bajo ciertas reglas, esas reglas son los niveles de medici\'on.
    \item Cuantificar: es asignar valores a algo tomando un patr\'on de referencia. Por ejemplo, cuantificar es ver cu\'antos hombres y cu\'antas mujeres hay.
\end{itemize}

\textbf{Variable:} es una caracter\'istica o propiedad que asume diferentes valores dentro de una poblaci\'on de inter\'es y cuya variaci\'on es susceptible de medirse.

Las variables pueden clasificarse de acuerdo al tipo de valores que puede tomar como:

\begin{itemize}
\item \textbf{Discretas o categ\'oricas} en las que los valores se relacionan a nombres, etiquetas o categor\'ias, no existe un significado num\'erico directo.
\item \textbf{Continuas} los valores tienen un correlato num\'erico directo, son continuos y susceptibles de fraccionarse y de poder utilizarse en operaciones aritm\'eticas.
\item \textbf{Dicot\'omica} s\'olo tienen dos valores posibles, la caracter\'istica est\'a ausente o presente.
\end{itemize}

En cuanto a una clasificaci\'on estad\'istica, las varibles pueden ser:

\begin{itemize}
\item \textbf{Aleatoria} Aquella en la cual desconocemos el valor porque fluct\'ua de acuerdo a un evento debido al azar.
\item \textbf{Determin\'istica} Aquella variable de la que se conoce el valor.
\item \textbf{Independiente} aquellas variables que son manipuladas por el investigador. Define los grupos.
\item \textbf{Dependiente} son mediciones que ocurren durante el experimento o tratamiento (resultado de la independiente), es la que se mide y compara entre los grupos.
\end{itemize}

En lo que tiene que ver con los \textbf{Niveles de Medici\'on} tenemoss distintos tipos de variable

\begin{itemize}
\item \textbf{Nominal:} Las propiedades de la medici\'on nominal son:
\begin{itemize}
\item Exhaustiva: implica a todas las opciones.
\item A los sujetos se les asignan categor\'ias, por lo que son mutuamente excluyentes. Es decir, la variable est\'a presente o no; tiene o no una caracter\'istica.
\end{itemize}
\item \textbf{Ordinal:} Las propiedades de la medici\'on ordinal son:
\begin{itemize}
\item El nivel ordinal posee transitividad, por lo que se tiene la capacidad de identificar que es mejor o mayor que otra, en ese sentido se pueden establecer jerarqu\'ias.
\item Las distancias entre un valor y otro no son iguales.
\end{itemize}
\item \textbf{Intervalo:} 
\begin{itemize}
\item El nivel de medici\'on intervalar requiere distancias iguales entre cada valor. Por lo general utiliza datos cuantitativos. Por ejemplo: temperatura, atributos psicol\'ogicos (CI, nivel de autoestima, pruebas de conocimientos, etc.)
\item Las unidades de calificaci\'on son equivalentes en todos los puntos de la escala. Una escala de intervalos implica: clasificaci\'on, magnitud y unidades de tama\~nos iguales (Brown, 2000).
\item Se pueden hacer operaciones aritm\'eticas.
\item Cuando se le pide al sujeto que califique una situaci\'on del 0 al 10 puede tomarse como un nivel de medici\'on de intervalo, siempre y cuando se incluya el 0.
\end{itemize}
\item \textbf{Raz\'on:} 
\begin{itemize}
\item La escala empieza a partir del 0 absoluto, por lo tanto incluye s\'olo los n\'umeros por su valor en s\'i, por lo que no pueden existir los n\'umeros con signo negativo. Por ejemplo: Peso corporal en kg., edad en a\~nos, estatura en cm.
\end{itemize}
\end{itemize}
%---------------------------------------------------------
\subsection*{Definiciones adicionales}
%---------------------------------------------------------
\begin{itemize}
    \item \textbf{Variable:} Consideraciones que una variable son una caracter\'istica o fen\'omeno que puede tomar distintos valores.
    \item \textbf{Dato:} Mediciones o cualidades que han sido recopiladas como resultado de observaciones.
    \item \textbf{Poblaci\'on:} Se considera el \'area de la cual son extra\'idos los datos. Es decir, es el conjunto de elementos o individuos que poseen una caracter\'istica com\'un y medible acerca de lo cual se desea informaci\'on. Es tambi\'en llamado Universo.
    \item \textbf{Muestra:} Es un subconjunto de la poblaci\'on, seleccionado de acuerdo a una regla o alg\'un plan de muestreo.
    \item \textbf{Censo:} Recopilaci\'on de todos los datos (de inter\'es para la investigaci\'on) de la poblaci\'on.
    \item \textbf{Estad\'istica:} Es una funci\'on o f\'ormula que depende de los datos de la muestra (es variable).
    \item \textbf{Par\'ametro:} Caracter\'istica medible de la poblaci\'on. Es un resumen num\'erico de alguna variable observada de la poblaci\'on. Los par\'ametros normales que se estudian son: \textit{La media poblacional, Proporci\'on.}
    \item \textbf{Estimador:} Un estimador de un par\'ametro es un estad\'istico que se emplea para conocer el par\'ametro desconocido.
    \item \textbf{Estad\'istico:} Es una funci\'on de los valores de la muestra. Es una variable aleatoria, cuyos valores dependen de la muestra seleccionada. Su distribuci\'on de probabilidad, se conoce como \textit{Distribuci\'on muestral del estad\'istico}.
    \item \textbf{Estimaci\'on:} Este t\'ermino indica que a partir de lo observado en una muestra (un resumen estad\'istico con las medidas que conocemos de Descriptiva) se extrapola o generaliza dicho resultado muestral a la poblaci\'on total, de modo que lo estimado es el valor generalizado a la poblaci\'on. Consiste en la b\'usqueda del valor de los par\'ametros poblacionales objeto de estudio. Puede ser puntual o por intervalo de confianza:
    \begin{itemize}
        \item \textbf{\textit{Puntual:}} cuando buscamos un valor concreto. Un estimador de un par\'ametro poblacional es una funci\'on de los datos muestrales. En pocas palabras, es una f\'ormula que depende de los valores obtenidos de una muestra, para realizar estimaciones. Lo que se pretende obtener es el valor exacto de un par\'ametro.
    \item \textbf{\textit{Intervalo de confianza:}} cuando determinamos un intervalo, dentro del cual se supone que va a estar el valor del par\'ametro que se busca con una cierta probabilidad. El intervalo de confianza est\'a determinado por dos valores dentro de los cuales afirmamos que est\'a el verdadero par\'ametro con cierta probabilidad. Son unos l\'imites o margen de variabilidad que damos al valor estimado, para poder afirmar, bajo un criterio de probabilidad, que el verdadero valor no los rebasar\'a.

Este intervalo contiene al par\'ametro estimado con una determinada certeza o nivel de confianza. 
\end{itemize}

En la estimaci\'on por intervalos se usan los siguientes conceptos:

\item \textbf{Variabilidad del par\'ametro:} Si no se conoce, puede obtenerse una aproximaci\'on en los datos o en un estudio piloto. Tambi\'en hay m\'etodos para calcular el tama\~no de la muestra que prescinden de este aspecto. Habitualmente se usa como medida de esta variabilidad la desviaci\'on t\'ipica poblacional.
\item \textbf{Error de la estimaci\'on:} Es una medida de su precisi\'on que se corresponde con la amplitud del intervalo de confianza. Cuanta m\'as precisi\'on se desee en la estimaci\'on de un par\'ametro, m\'as estrecho deber\'a ser el intervalo de confianza y, por tanto, menor el error, y m\'as sujetos deber\'an incluirse en la muestra estudiada. 
\item \textbf{Nivel de confianza:} Es la probabilidad de que el verdadero valor del par\'ametro estimado en la poblaci\'on se sit\'ue en el intervalo de confianza obtenido. El nivel de confianza se denota por $1-\alpha$
\item \textbf{$p$-value:} Tambi\'en llamado nivel de significaci\'on. Es la probabilidad (en tanto por uno) de fallar en nuestra estimaci\'on, esto es, la diferencia entre la certeza (1) y el nivel de confianza $1-\alpha$. 
\item \textbf{Valor cr\'itico:} Se representa por $Z_{\alpha/2}$. Es el valor de la abscisa en una determinada distribuci\'on que deja a su derecha un \'area igual a 1/2, siendo $1-\alpha$ el nivel de confianza. Normalmente los valores cr\'iticos est\'an tabulados o pueden calcularse en funci\'on de la distribuci\'on de la poblaci\'on.

\end{itemize}

Para un tama\~no fijo de la muestra, los conceptos de error y nivel de confianza van relacionados. Si admitimos un error mayor, esto es, aumentamos el tama\~no del intervalo de confianza, tenemos tambi\'en una mayor probabilidad de \'exito en nuestra estimaci\'on, es decir, un mayor nivel de confianza. Por tanto, un aspecto que debe de tenerse en cuenta es el tama\~no muestral, ya que para disminuir el error que se comente habr\'a que aumentar el tama\~no muestral. Esto se resolver\'a, para un intervalo de confianza cualquiera, despejando el tama\~no de la muestra en cualquiera de las formulas de los intervalos de confianza que veremos a continuaci\'on, a partir del error m\'aximo permitido. Los intervalos de confianza pueden ser unilaterales o bilaterales:

\begin{itemize}
    \item \textbf{Contraste de Hip\'otesis:} Consiste en determinar si es aceptable, partiendo de datos muestrales, que la caracter\'istica o el par\'ametro poblacional estudiado tome un determinado valor o est\'e dentro de unos determinados valores.
    \item \textbf{Nivel de Confianza:} Indica la proporci\'on de veces que acertar\'iamos al afirmar que el par\'ametro est\'a dentro del intervalo al seleccionar muchas muestras.
\end{itemize}
%---------------------------------------------------------
\subsection{Muestreo:} 
%---------------------------------------------------------
\textbf{Muestreo:} Una muestra es representativa en la medida que es imagen de la poblaci\'on. En general, podemos decir que el tama\~no de una muestra depender\'a principalmente de: \textit{Nivel de precisi\'on deseado, Recursos disponibles, Tiempo involucrado en la investigaci\'on.} Adem\'as el plan de muestreo debe considerar \textit{La poblaci\'on, Par\'ametros a medir}. Existe una gran cantidad de tipos de muestreo, en la pr\'actica los m\'as utilizados son los siguientes:


\begin{itemize}
    \item \textbf{MUESTREO ALEATORIO SIMPLE:} Es un m\'etodo de selecci\'on de $n$ unidades extra\'idas de $N$, de tal manera que cada una de las posibles muestras tiene la misma probabilidad de ser escogida. (En la pr\'actica, se enumeran las unidades de 1 a $N$, y a continuaci\'on se seleccionan $n$ n\'umeros aleatorios entre 1 y $N$, ya sea de tablas o de alguna urna con fichas numeradas).
    \item \textbf{MUESTREO ESTRATIFICADO ALEATORIO:} Se usa cuando la poblaci\'on est\'a agrupada en pocos estratos, cada uno de ellos son muchas entidades. Este muestreo consiste en sacar una muestra aleatoria simple de cada uno de los estratos. (Generalmente, de tama\~no proporcional al estrato).
    \item \textbf{MUESTREO SISTEM\'ATICO:} Se utiliza cuando las unidades de la poblaci\'on est\'an de alguna manera totalmente ordenadas. Para seleccionar una muestra de $n$ unidades, se divide la poblaci\'on en $n$ subpoblaciones de tama\~no $K = N/n$ y se toma al azar una unidad de la $K$ primeras y de ah\'i en adelante cada $K$-\'esima unidad.
    \item \textbf{MUESTREO POR CONGLOMERADO:} Se emplea cuando la poblaci\'on est\'a dividida en grupos o conglomerados peque\~nos. Consiste en obtener una muestra aleatoria simple de conglomerados y luego CENSAR cada uno de \'estos.
    \item \textbf{MUESTREO EN DOS ETAPAS (Biet\'apico):} En este caso la muestra se toma en dos pasos:
    \begin{itemize}
        \item Seleccionar una muestra de unidades primarias, y 
        \item Seleccionar una muestra de elementos a partir de cada unidad primaria escogida.
        \item \textit{Observaci\'on:} En la realidad es posible encontrarse con situaciones en las cuales no es posible aplicar libremente un tipo de muestreo, incluso estaremos obligados a mezclarlas en ocasiones.
    \end{itemize}
\end{itemize}

%---------------------------------------------------------
\subsection{Errores Estad\'isticos Comunes}
%---------------------------------------------------------

El prop\'osito de esta secci\'on es solamente indicar los malos usos comunes de datos estad\'isticos, sin incluir el uso de m\'etodos estad\'isticos complicados. Un estudiante deber\'ia estar alerta en relaci\'on con estos malos usos y deber\'ia hacer un gran esfuerzo para evitarlos a fin de ser un verdadero estad\'istico.

\textbf{Datos estad\'isticos inadecuados:} Los datos estad\'isticos son usados como la materia prima para un estudio estad\'istico. Cuando los datos son inadecuados, la conclusi\'on extra\'ida del estudio de los datos se vuelve obviamente inv\'alida. Por ejemplo, supongamos que deseamos encontrar el ingreso familiar t\'ipico del a\~no pasado en la ciudad Y de 50,000 familias y tenemos una muestra consistente del ingreso de solamente tres familias: 1 mill\'on, 2 millones y no ingreso. Si sumamos el ingreso de las tres familias y dividimos el total por 3, obtenemos un promedio de 1 mill\'on. Entonces, extraemos una conclusi\'on basada en la muestra de que el ingreso familiar promedio durante el a\~no pasado en la ciudad fue de 1 mill\'on. Es obvio que la conclusi\'on es falsa, puesto que las cifras son extremas y el tama\~no de la muestra es demasiado peque\~no; por lo tanto la muestra no es representativa. 

Hay muchas otras clases de datos inadecuados. Por ejemplo, algunos datos son respuestas inexactas de una encuesta, porque las preguntas usadas en la misma son vagas o enga\~nosas, algunos datos son toscas estimaciones porque no hay disponibles datos exactos o es demasiado costosa su obtenci\'on, y algunos datos son irrelevantes en un problema dado, porque el estudio estad\'istico no est\'a bien planeado. Al momento de recopilar los datos que ser\'an procesados se es susceptible de cometer errores as\'i como durante los c\'omputos de los mismos. No obstante, hay otros errores que no tienen nada que ver con la digitaci\'on y que no son tan f\'acilmente identificables. Algunos de \'estos errores son:

\begin{itemize}
    \item \textbf{Sesgo:} Es imposible ser completamente objetivo o no tener ideas preconcebidas antes de comenzar a estudiar un problema, y existen muchas maneras en que una perspectiva o estado mental pueda influir en la recopilaci\'on y en el an\'alisis de la informaci\'on. En estos casos se dice que hay un sesgo cuando el individuo da mayor peso a los datos que apoyan su opini\'on que a aquellos que la contradicen. Un caso extremo de sesgo ser\'ia la situaci\'on donde primero se toma una decisi\'on y despu\'es se utiliza el an\'alisis estad\'istico para justificar la decisi\'on ya tomada.
    \item \textbf{Datos No Comparables:} el establecer comparaciones es una de las partes m\'as importantes del an\'alisis estad\'istico, pero es extremadamente importante que tales comparaciones se hagan entre datos que sean comparables.
    \item \textbf{Proyecci\'on descuidada de tendencias:} la proyecci\'on simplista de tendencias pasadas hacia el futuro es uno de los errores que m\'as ha desacreditado el uso del an\'alisis estad\'istico.
    \item \textbf{Muestreo Incorrecto:} en la mayor\'ia de los estudios sucede que el volumen de informaci\'on disponible es tan inmenso que se hace necesario estudiar muestras, para derivar conclusiones acerca de la poblaci\'on a que pertenece la muestra. Si la muestra se selecciona correctamente, tendr\'a b\'asicamente las mismas propiedades que la poblaci\'on de la cual fue extra\'ida; pero si el muestreo se realiza incorrectamente, entonces puede suceder que los resultados no signifiquen nada.
\end{itemize}

\textbf{Sesgo} significa que un usuario d\'e los datos perjudicialmente de m\'as \'enfasis a los hechos, los cuales son empleados para mantener su predeterminada posici\'on u opini\'on. Los estad\'isticos son frecuentemente degradados por lemas tales como: \textit{Hay tres clases de mentiras: mentiras, mentiras reprobables y estad\'istica, y Las cifras no mienten, pero los mentirosos piensan}. Hay dos clases de sesgos: conscientes e inconscientes. Ambos son comunes en el an\'alisis estad\'istico. Hay numerosos ejemplos de sesgos conscientes. Un anunciante frecuentemente usa la estad\'istica para probar que su producto es muy superior al producto de su competidor. Un pol\'itico prefiere usar la estad\'istica para sostener su punto de vista. Gerentes y l\'ideres de trabajadores pueden simult\'aneamente situar sus respectivas cifras estad\'isticas sobre la misma tabla de trato para mostrar que sus rechazos o peticiones son justificadas. Es casi imposible que un sesgo inconsciente est\'e completamente ausente en un trabajo estad\'istico. En lo que respecta al ser humano, es dif\'icil obtener una actitud completamente objetiva al abordar un problema, aun cuando un cient\'ifico deber\'ia tener una mente abierta. Un estad\'istico deber\'ia estar enterado del hecho de que su interpretaci\'on de los resultados del an\'alisis estad\'istico est\'a influenciado por su propia experiencia, conocimiento y antecedentes con relaci\'on al problema dado.

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Fundamentos}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

%---------------------------------------------------------
\subsection{Pruebas de Hipótesis}
%---------------------------------------------------------

\begin{itemize}
    \item Una hipótesis estadística es una afirmación acerca de la distribución de probabilidad de una variable aleatoria, a menudo involucran uno o más parámetros de la distribución.
    \item Las hipótesis son afirmaciones respecto a la población o distribución bajo estudio, no en torno a la muestra.
    \item La mayoría de las veces, la prueba de hipótesis consiste en determinar si la situación experimental ha cambiado.
    \item El interés principal es decidir sobre la veracidad o falsedad de una hipótesis, a este procedimiento se le llama \textit{prueba de hipótesis}.
    \item Si la información es consistente con la hipótesis, se concluye que esta es verdadera, de lo contrario que con base en la información, es falsa.
\end{itemize}

Una prueba de hipótesis está formada por cinco partes:
\begin{itemize}
    \item La hipótesis nula, denotada por $H_{0}$.
    \item La hipótesis alternativa, denotada por $H_{1}$.
    \item El estadístico de prueba y su valor $p$.
    \item La región de rechazo.
    \item La conclusión.
\end{itemize}

\begin{Def}
Las dos hipótesis en competencia son la \textbf{hipótesis alternativa $H_{1}$}, usualmente la que se desea apoyar, y la \textbf{hipótesis nula $H_{0}$}, opuesta a $H_{1}$.
\end{Def}
En general, es más fácil presentar evidencia de que $H_{1}$ es cierta, que demostrar que $H_{0}$ es falsa, es por eso que por lo regular se comienza suponiendo que $H_{0}$ es cierta, luego se utilizan los datos de la muestra para decidir si existe evidencia a favor de $H_{1}$, más que a favor de $H_{0}$, así se tienen dos conclusiones:
\begin{itemize}
    \item Rechazar $H_{0}$ y concluir que $H_{1}$ es verdadera.
    \item Aceptar, no rechazar, $H_{0}$ como verdadera.
\end{itemize}

\begin{Ejem}
Se desea demostrar que el salario promedio por hora en cierto lugar es distinto de $19$ usd, que es el promedio nacional. Entonces $H_{1}:\mu \neq 19$, y $H_{0}:\mu = 19$.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hipótesis de dos colas}.

\begin{Ejem}
Un determinado proceso produce un promedio de $5\%$ de piezas defectuosas. Se está interesado en demostrar que un simple ajuste en una máquina reducirá $p$, la proporción de piezas defectuosas producidas en este proceso. Entonces se tiene $H_{0}: p < 0.3$ y $H_{1}: p = 0.03$. Si se puede rechazar $H_{0}$, se concluye que el proceso ajustado produce menos del $5\%$ de piezas defectuosas.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hipótesis de una cola}.

La decisión de rechazar o aceptar la hipótesis nula está basada en la información contenida en una muestra proveniente de la población de interés. Esta información tiene estas formas:
\begin{itemize}
    \item \textbf{Estadístico de prueba:} un sólo número calculado a partir de la muestra.
    \item \textbf{$p$-value:} probabilidad calculada a partir del estadístico de prueba.
\end{itemize}

\begin{Def}
El $p$-value es la probabilidad de observar un estadístico de prueba tanto o más alejado del valor observado, si en realidad $H_{0}$ es verdadera. Valores grandes del estadístico de prueba y valores pequeños de $p$ significan que se ha observado un evento muy poco probable, si $H_{0}$ en realidad es verdadera.
\end{Def}

Todo el conjunto de valores que puede tomar el estadístico de prueba se divide en dos regiones. Un conjunto, formado de valores que apoyan la hipótesis alternativa y llevan a rechazar $H_{0}$, se denomina \textbf{región de rechazo}. El otro, conformado por los valores que sustentan la hipótesis nula, se le denomina \textbf{región de aceptación}. Cuando la región de rechazo está en la cola izquierda de la distribución, la prueba se denomina \textbf{prueba lateral izquierda}. Una prueba con región de rechazo en la cola derecha se le llama \textbf{prueba lateral derecha}. Si el estadístico de prueba cae en la región de rechazo, entonces se rechaza $H_{0}$. Si el estadístico de prueba cae en la región de aceptación, entonces la hipótesis nula se acepta o la prueba se juzga como no concluyente. Dependiendo del nivel de confianza que se desea agregar a las conclusiones de la prueba, y el \textbf{nivel de significancia $\alpha$}, el riesgo que está dispuesto a correr si se toma una decisión incorrecta.

\begin{Def}
Un \textbf{error de tipo I} para una prueba estadística es el error que se tiene al rechazar la hipótesis nula cuando es verdadera. El \textbf{nivel de significancia} para una prueba estadística de hipótesis es
\begin{eqnarray*}
\alpha &=& P\left\{\textrm{error tipo I}\right\} = P\left\{\textrm{rechazar equivocadamente } H_{0}\right\} \\
&=& P\left\{\textrm{rechazar } H_{0} \textrm{ cuando } H_{0} \textrm{ es verdadera}\right\}
\end{eqnarray*}
\end{Def}
Este valor $\alpha$ representa el valor máximo de riesgo tolerable de rechazar incorrectamente $H_{0}$. Una vez establecido el nivel de significancia, la región de rechazo se define para poder determinar si se rechaza $H_{0}$ con un cierto nivel de confianza.

%---------------------------------------------------------
\subsection{Muestras grandes: una media poblacional}
%---------------------------------------------------------


\begin{Def}
El \textbf{valor de $p$} (\textbf{$p$-value}) o nivel de significancia observado de un estadístico de prueba es el valor más pequeño de $\alpha$ para el cual $H_{0}$ se puede rechazar. El riesgo de cometer un error tipo $I$, si $H_{0}$ es rechazada con base en la información que proporciona la muestra.
\end{Def}

\begin{Note}
Valores pequeños de $p$ indican que el valor observado del estadístico de prueba se encuentra alejado del valor hipotético de $\mu$, es decir se tiene evidencia de que $H_{0}$ es falsa y por tanto debe de rechazarse.
\end{Note}

\begin{Note}
Valores grandes de $p$ indican que el estadístico de prueba observado no está alejado de la media hipotética y no apoya el rechazo de $H_{0}$.
\end{Note}

\begin{Def}
Si el valor de $p$ es menor o igual que el nivel de significancia $\alpha$, determinado previamente, entonces $H_{0}$ es rechazada y se puede concluir que los resultados son estadísticamente significativos con un nivel de confianza del $100 (1-\alpha)\%$.
\end{Def}
Es usual utilizar la siguiente clasificación de resultados:


\begin{center}
\begin{tabular}{|c||c|l|}\hline
$p$ & $H_{0}$ & Significativa \\ \hline
$p\leq 0.01$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. altamente significativos  y en contra de $H_{0}$\end{tabular} \\ \hline
$p\leq 0.05$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. Estad\'isticamente significativos  y en contra de $H_{0}$\end{tabular} \\ \hline
$p\leq 0.10$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. posiblemente significativos con Tendencia estad\'istica \\ y en contra de $H_{0}$\end{tabular} \\ \hline
$p> 0.10$ & no rechazada & \begin{tabular}[c]{@{}l@{}}Result.  estad\'isticamente no significativos y no rechazar $H_{0}$\end{tabular} \\ \hline
\end{tabular}
\end{center}

\begin{Note}
Para determinar el valor de $p$, encontrar el \'area en la cola despu\'es del estad\'istico de prueba. Si la prueba es de una cola, este es el valor de $p$. Si es de dos colas, \'este valor encontrado es la mitad del valor de $p$. Rechazar $H_{0}$ cuando el valor de $p<\alpha$.
\end{Note}

Hay dos tipos de errores al realizar una prueba de hip\'otesis
\begin{center}
\begin{tabular}{c|cc}
& $H_{0}$ es Verdadera & $H_{0}$ es Falsa\\\hline\hline
Rechazar $H_{0}$ & Error tipo I & $\surd$\\
Aceptar $H_{0}$ & $\surd$ & Error tipo II
\end{tabular}
\end{center}
\begin{Def}
La probabilidad de cometer el error tipo II se define por $\beta$ donde
\begin{eqnarray*}
\beta&=&P\left\{\textrm{error tipo II}\right\}=P\left\{\textrm{Aceptar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{Aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}
\end{eqnarray*}
\end{Def}

\begin{Note}
Cuando $H_{0}$ es falsa y $H_{1}$ es verdadera, no siempre es posible especificar un valor exacto de $\mu$, sino m\'as bien un rango de posibles valores.\medskip
En lugar de arriesgarse a tomar una decisi\'on incorrecta, es mejor conlcuir que \textit{no hay evidencia suficiente para rechazar $H_{0}$}, es decir en lugar de aceptar $H_{0}$, \textit{no rechazar $H_{0}$}.

\end{Note}
La bondad de una prueba estad\'istica se mide por el tama\~ no de $\alpha$ y $\beta$, ambas deben de ser peque\~ nas. Una manera muy efectiva de medir la potencia de la prueba es calculando el complemento del error tipo $II$:
\begin{eqnarray*}
1-\beta&= &P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{1}\textrm{ es verdadera}\right\}
\end{eqnarray*}
\begin{Def}
La \textbf{potencia de la prueba}, $1-\beta$, mide la capacidad de que la prueba funciona como se necesita.
\end{Def}

\begin{Ejem}
La producci\'on diariade una planta qu\'imica local ha promediado 880 toneladas en los \'ultimos a\~nos. A la gerente de control de calidad le gustar\'ia saber si este promedio ha cambiado en meses recientes. Ella selecciona al azar 50 d\'ias de la base de datos computarizada y calcula el promedio y la desviaci\'on est\'andar de las $n=50$  producciones como $\overline{x}=871$ toneladas y $s=21$ toneladas, respectivamente. Pruebe la hip\'otesis  apropiada usando $\alpha=0.05$.

La hip\'otesis nula apropiada es:
\begin{eqnarray*}
H_{0}&:& \mu=880\\
&&\textrm{ y la hip\'otesis alternativa }H_{1}\textrm{ es }\\
H_{1}&:& \mu\neq880
\end{eqnarray*}
el estimador puntual para $\mu$ es $\overline{x}$, entonces el estad\'istico de prueba es\medskip
\begin{eqnarray*}
z&=&\frac{\overline{x}-\mu_{0}}{s/\sqrt{n}}\\
&=&\frac{871-880}{21/\sqrt{50}}=-3.03
\end{eqnarray*}

Para esta prueba de  dos colas, hay que determinar los dos valores de $z_{\alpha/2}$, es decir,  $z_{\alpha/2}=\pm1.96$, como $z>z_{\alpha/2}$, $z$ cae en la zona de rechazo, por lo tanto  la gerente puede rechazar la hip\'otesis nula y concluir que el promedio efectivamente ha cambiado. La probabilidad de rechazar $H_{0}$ cuando esta es verdadera es de  $0.05$. Recordemos que el valor observado del estad\'istico de prueba es $z=-3.03$, la regi\'on de rechazo m\'as peque\~na que puede usarse y todav\'ia seguir rechazando $H_{0}$ es $|z|>3.03$, entonces $p=2(0.012)=0.0024$, que a su vez es menor que el nivel de significancia $\alpha$ asignado inicialmente, y adem\'as los resultados son  \textbf{altamente significativos}. Finalmente determinemos la potencia de la prueba cuando $\mu$ en realidad es igual a $870$ toneladas.

Recordar que la regi\'on de aceptaci\'on est\'a entre $-1.96$ y $1.96$, para $\mu=880$, equivalentemente $$874.18<\overline{x}<885.82$$
$\beta$ es la probabilidad de aceptar $H_{0}$ cuando $\mu=870$, calculemos los valores de $z$ correspondientes a $874.18$ y $885.82$ \medskip
Entonces
\begin{eqnarray*}
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{874.18-870}{21/\sqrt{50}}=1.41\\
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{885.82-870}{21/\sqrt{50}}=5.33
\end{eqnarray*}
por lo tanto
\begin{eqnarray*}
\beta&=&P\left\{\textrm{aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}=P\left\{874.18<\mu<885.82\textrm{ cuando }\mu=870\right\}\\
&=&P\left\{1.41<z<5.33\right\}=P\left\{1.41<z\right\}=1-0.9207=0.0793
\end{eqnarray*}
entonces, la potencia de la prueba es
$$1-\beta=1-0.0793=0.9207$$ que es la probabilidad de rechazar correctamente $H_{0}$ cuando $H_{0}$ es falsa.
\end{Ejem}

%---------------------------------------------------------
\subsubsection{Prueba de hip\'otesis para la diferencia entre dos medias poblacionales}
%---------------------------------------------------------

El estad\'istico que resume la informaci\'on muestral respecto a la diferencia en medias poblacionales $\left(\mu_{1}-\mu_{2}\right)$ es la diferencia de las medias muestrales $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, por tanto al probar la difencia entre las medias muestrales se verifica que la diferencia real entre las medias poblacionales difiere de un valor especificado, $\left(\mu_{1}-\mu_{2}\right)=D_{0}$, se puede usar el error est\'andar de $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, es decir

\begin{eqnarray}\sqrt{\frac{\sigma^{2}_{1}}{n_{1}}+\frac{\sigma^{2}_{2}}{n_{2}}}\end{eqnarray}
cuyo estimador est\'a dado por
\begin{eqnarray}SE=\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}\end{eqnarray}
El procedimiento para muestras grandes es:

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,
donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}z=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}\end{eqnarray}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

\begin{Ejem}
Para determinar si ser propietario de un autom\'ovil afecta el rendimiento acad\'emico de un estudiante, se tomaron dos muestras aleatorias de 100 estudiantes varones. El promedio de calificaciones para los $n_{1}=100$ no propietarios de un auto tuvieron un promedio y varianza de $\overline{x}_{1}=2.7$ y $s_{1}^{2}=0.36$, respectivamente, mientras que para para la segunda muestra con $n_{2}=100$ propietarios de un auto, se tiene $\overline{x}_{2}=2.54$ y $s_{2}^{2}=0.4$. Los datos presentan suficiente evidencia para indicar una diferencia en la media en el rendimiento acad\'emico entre propietarios y no propietarios de un autom\'ovil? Hacer pruebas para $\alpha=0.01,0.05$ y $\alpha=0.1$.

\begin{itemize}
\item Soluci\'on utilizando la t\'ecnica de regiones de rechazo:\medskip
realizando las operaciones
$z=1.84$, determinar si excede los valores de $z_{\alpha/2}$.
\item Soluci\'on utilizando el $p$-value:\medskip
Calcular el valor de $p$, la probabilidad de que $z$ sea mayor que $z=1.84$ o menor que $z=-1.84$, se tiene que $p=0.0658$. 

\item Si el intervalo de confianza que se construye contiene el valor del par\'ametro especificado por $H_{0}$, entonces ese valor es uno de los posibles valores del par\'ametro y $H_{0}$ no debe ser rechazada.

\item Si el valor hipot\'etico se encuentra fuera de los l\'imites de confianza, la hip\'otesis nula es rechazada al nivel de significancia $\alpha$.
\end{itemize}
\end{Ejem}

%---------------------------------------------------------
\subsubsection*{Prueba de Hip\'otesis para una Proporci\'on Binomial}
%---------------------------------------------------------

Para una muestra aleatoria de $n$ intentos id\'enticos, de una poblaci\'on binomial, la proporci\'on muesrtal $\hat{p}$ tiene una distribuci\'on aproximadamente normal cuando $n$ es grande, con media $p$ y error est\'andar
\begin{eqnarray}SE=\sqrt{\frac{pq}{n}}.\end{eqnarray}
La prueba de hip\'otesis de la forma
\begin{eqnarray*}
H_{0}&:&p=p_{0}\\
H_{1}&:&p>p_{0}\textrm{, o }p<p_{0}\textrm{ o }p\neq p_{0}
\end{eqnarray*}
El estad\'istico de prueba se construye con el mejor estimador de la proporci\'on verdadera, $\hat{p}$, con el estad\'istico de prueba $z$, que se distribuye normal est\'andar.

El procedimiento es
\begin{itemize}
\item[1) ] Hip\'otesis nula: $H_{0}:p=p_{0}$
\item[2) ] Hip\'otesis alternativa

\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:p>p_{0}$ & $p\neq p_{0}$\\
$H_{1}:p<p_{0}$ & \\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}
z=\frac{\hat{p}-p_{0}}{\sqrt{\frac{pq}{n}}},\hat{p}=\frac{x}{n}
\end{eqnarray}
donde $x$ es el n\'umero de \'exitos en $n$ intentos binomiales.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

%--------------------------------------------------------------------------------
\subsubsection{Prueba de Hip\'otesis diferencia entre dos Proporciones Binomiales}
%--------------------------------------------------------------------------------

Cuando se tienen dos muestras aleatorias independientes de dos poblaciones binomiales, el objetivo del experimento puede ser la diferencia $\left(p_{1}-p_{2}\right)$ en las proporciones de individuos u objetos que poseen una caracter\'istica especifica en las dos poblaciones. En este caso se pueden utilizar los estimadores de las dos proporciones $\left(\hat{p}_{1}-\hat{p}_{2}\right)$ con error est\'andar dado por
\begin{eqnarray}SE=\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}},\end{eqnarray}
considerando el estad\'istico $z$ con un nivel de significancia $\left(1-\alpha\right)100\%$

La hip\'otesis nula a probarse es de la forma
\begin{itemize}
\item[$H_{0}$: ] $p_{1}=p_{2}$ o equivalentemente $\left(p_{1}-p_{2}\right)=0$, contra una hip\'otesis alternativa $H_{1}$ de una o dos colas.
\end{itemize}

Para estimar el error est\'andar del estad\'istico $z$, se debe de utilizar el hecho de que suponiendo que $H_{0}$ es verdadera, las dos proporciones son iguales a alg\'un valor com\'un, $p$. Para obtener el mejor estimador de $p$ es
\begin{eqnarray}p=\frac{\textrm{n\'umero total de \'exitos}}{\textrm{N\'umero total de pruebas}}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}.\end{eqnarray}

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\left(p_{1}-p_{2}\right)=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(p_{1}-p_{2}\right)>0$ & $H_{1}:\left(p_{1}-p_{2}\right)\neq 0$\\ 
$H_{1}:\left(p_{1}-p_{2}\right)<0$&\\
\end{tabular}
\end{center}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}
z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}}=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{pq}{n_{1}}+\frac{pq}{n_{2}}}},
\end{eqnarray}
donde $\hat{p_{1}}=x_{1}/n_{1}$ y $\hat{p_{2}}=x_{2}/n_{2}$ , dado que el valor com\'un para $p_{1}$ y $p_{2}$ es $p$, entonces $\hat{p}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$ y por tanto el estad\'istico de prueba es
\begin{eqnarray}
z=\frac{\hat{p}_{1}-\hat{p}_{2}}{\sqrt{\hat{p}\hat{q}}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}.
\end{eqnarray}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{\alpha}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

%----------------------------------------------------------------
\subsection{Muestras Peque\~nas}
%----------------------------------------------------------------

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu=\mu_{0}$,
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu>\mu_{0}$ & $H_{1}:\mu\neq \mu_{0}$\\ 
$H_{1}:\mu<\mu0$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}
t=\frac{\overline{x}-\mu_{0}}{\sqrt{\frac{s^{2}}{n}}},
\end{eqnarray}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}
%------------------------------------------------------------------------------------
\subsubsection{Diferencia entre dos medias poblacionales: MAI}
%------------------------------------------------------------------------------------

Cuando los tama\~nos de muestra son peque\ ~nos, no se puede asegurar que las medias muestrales sean normales, pero si las poblaciones originales son normales, entonces la distribuci\'on muestral de la diferencia de las medias muestales, $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, ser\'a normal con media $\left(\mu_{1}-\mu_{2}\right)$ y error est\'andar \begin{eqnarray}ES=\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}.\end{eqnarray}


\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}t=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}\end{eqnarray}


donde \begin{eqnarray}s^{2}=\frac{\left(n_{1}-1\right)s_{1}^{2}+\left(n_{2}-1\right)s_{2}^{2}}{n_{1}+n_{2}-2}.\end{eqnarray}

\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}

Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.
\end{itemize}
%------------------------------------------------------------------------------------
\subsubsection{Diferencia entre dos medias poblacionales: Diferencias Pareadas}
%------------------------------------------------------------------------------------

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu_{d}=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:\mu_{d}$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu_{d}>0$ & $H_{1}:\mu_{d}\neq 0$\\ 
$H_{1}:\mu_{d}<0$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}
t=\frac{\overline{d}}{\sqrt{\frac{s_{d}^{2}}{n}}}
\end{eqnarray}
donde $n$ es el n\'umero de diferencias pareadas, $\overline{d}$ es la media de las diferencias muestrales, y $s_{d}$ es la desviaci\'on est\'andar de las diferencias muestrales.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.
\end{itemize}
%------------------------------------------------------------------------------------
\subsubsection{Inferencias con respecto a la Varianza Poblacional}
%------------------------------------------------------------------------------------
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\sigma^{2}=\sigma^{2}_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\sigma^{2}>\sigma^{2}_{0}$ & $H_{1}:\sigma^{2}\neq \sigma^{2}_{0}$\\ 
$H_{1}:\sigma^{2}<\sigma^{2}_{0}$&\\
\end{tabular}
\end{center}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}
\chi^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}_{0}},
\end{eqnarray}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$\chi^{2}>\chi^{2}_{\alpha}$ & \\
$\chi^{2}<\chi^{2}_{\left(1-\alpha\right)}$ cuando $H_{1}:\chi^{2}<\chi^{2}_{0}$&$\chi^{2}>\chi^{2}_{\alpha/2}$ o $\chi^{2}<\chi^{2}_{\left(1-\alpha/2\right)}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}

Los valores cr\'iticos de $\chi^{2}$,est\'an basados en $\left(n_{1}+\right)$ grados de libertad.

\end{itemize}

%------------------------------------------------------------------------------------
\subsubsection{Comparaci\'on de dos varianzas poblacionales}
%------------------------------------------------------------------------------------

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)>D_{0}$ & $H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)<D_{0}$&\\
\end{tabular}
\end{center}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}F=\frac{s_{1}^{2}}{s_{2}^{2}}\end{eqnarray}
donde $s_{1}^{2}$ es la varianza muestral m\'as grande.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$F>F_{\alpha}$ & $F>F_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

%---------------------------------------------------------
\subsection{Estimaci\'on por intervalos}
%---------------------------------------------------------

Recordemos que $S^{2}$ es un estimador insesgado de $\sigma^{2}$, entonces se tiene la siguiente definici\'on 
\begin{Def}
Sean $\hat{\theta}_{1}$ y $\hat{\theta}_{2}$ dos estimadores insesgados de $\theta$, par\'ametro poblacional. Si $\sigma_{\hat{\theta}_{1}}^{2}<\sigma_{\hat{\theta}_{2}}^{2}$, decimos que $\hat{\theta}_{1}$ un estimador m\'as eficaz de $\theta$ que $\hat{\theta}_{2}$.
\end{Def}

Algunas observaciones que es preciso realizar
\begin{Note}
\begin{enumerate}
\item[a) ]Para poblaciones normales, $\overline{X}$ y $\tilde{X}$ son estimadores insesgados de $\mu$, pero con $\sigma_{\overline{X}}^{2}<\sigma_{\tilde{X}_{2}}^{2}$.
%\end{Note}

%\begin{Note}
\item[b) ]Para las estimaciones por intervalos de $\theta$, un intervalo de la forma $\hat{\theta}_{L}<\theta<\hat{\theta}_{U}$,  $\hat{\theta}_{L}$ y $\hat{\theta}_{U}$ dependen del valor de $\hat{\theta}$.
\item[c) ]Para $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, si $n\rightarrow\infty$, entonces $\hat{\theta}\rightarrow\mu$.
%\end{Note}
\end{enumerate}
\end{Note}


\begin{Note}
Para $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, si $n\rightarrow\infty$, %entonces $\hat{\theta}\rightarrow\mu$.
\end{Note}


\begin{enumerate}
\item[d) ]Para $\hat{\theta}$ se determinan $\hat{\theta}_{L}$ y $\hat{\theta}_{U}$ de modo tal que 
\begin{eqnarray}
P\left\{\hat{\theta}_{L}<\hat{\theta}<\hat{\theta}_{U}\right\}=1-\alpha,
\end{eqnarray}
con $\alpha\in\left(0,1\right)$. Es decir, $\theta\in\left(\hat{\theta}_{L},\hat{\theta}_{U}\right)$ es un intervalo de confianza del $100\left(1-\alpha\right)\%$.

\item[e) ] De acuerdo con el TLC se espera que la distribuci\'on muestral de $\overline{X}$ se distribuye aproximadamente normal con media $\mu_{X}=\mu$ y desviaci\'on est\'andar $\sigma_{\overline{X}}=\frac{\sigma}{\sqrt{n}}$.

\end{enumerate}

Para $Z_{\alpha/2}$ se tiene $P\left\{-Z_{\alpha/2}<Z<Z_{\alpha/2}\right\}=1-\alpha$, donde $Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}$. Entonces
\begin{eqnarray}P\left\{-Z_{\alpha/2}<\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}<Z_{\alpha/2}\right\}=1-\alpha,\end{eqnarray} es equivalente a 
\begin{eqnarray}P\left\{\overline{X}-Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}<\mu<\overline{X}+Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right\}=1-\alpha.\end{eqnarray}

\begin{enumerate}
\item[f) ]Si $\overline{X}$ es la media muestral de una muestra de tama\~no $n$ de una poblaci\'on con varianza conocida $\sigma^{2}$, el intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\mu$ es \begin{eqnarray}\mu\in\left(\overline{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}},\overline{x}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right).\end{eqnarray}

\item[g) ] Para muestras peque\~nas de poblaciones no normales, no se puede esperar que el grado de confianza sea preciso.
\item[h) ] Para $n\geq30$, con distribuci\'on de forma no muy sesgada, se pueden tener buenos resultados.
\end{enumerate}

\begin{Teo}
Si $\overline{X}$ es un estimador de $\mu$, podemos tener $100\left(1-\alpha\right)\%$  de confianza en que el error no exceder\'a a $z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$, error entre $\overline{X}$ y $\mu$.
\end{Teo}

\begin{Teo}
Si $\overline{X}$ es un estimador de $\mu$, podemos tener $100\left(1-\alpha\right)\%$  de confianza en que el error no exceder\'a una cantidad $e$ cuando el tama\~no de la muestra es \begin{eqnarray}n=\left(\frac{z_{\alpha/2}\sigma}{e}\right)^{2}.\end{eqnarray}
\end{Teo}
\begin{Note}
Para intervalos unilaterales
\begin{eqnarray}P\left\{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}<Z_{\alpha}\right\}=1-\alpha\end{eqnarray}
\end{Note}

equivalentemente
\begin{eqnarray}P\left\{\mu<\overline{X}+Z_{\alpha}\frac{\sigma}{\sqrt{n}}\right\}=1-\alpha.\end{eqnarray}
Si $\overline{X}$ es la media de una muestra aleatoria de tama\~no $n$  a partir de una poblaci\'on con varianza $\sigma^{2}$, los l\'imites de confianza unilaterales del   $100\left(1-\alpha\right)\%$  de confianza para $\mu$ est\'an dados por
\begin{itemize}
\item[a) ] L\'imite unilateral superior: $\overline{x}+z_{\alpha}\frac{\sigma}{\sqrt{n}}$
\item[b) ] L\'imite unilateral inferior: $\overline{x}-z_{\alpha}\frac{\sigma}{\sqrt{n}}$
\item[c) ] Para $\sigma$ desconocida recordar que $T=\frac{\overline{x}-\mu}{s/\sqrt{n}}\sim t_{n-1}$, donde $s$ es la desviaci\'on est\'andar de la muestra. Entonces
\begin{eqnarray}
P\left\{-t_{\alpha/2}<T<t_{\alpha/2}\right\}=1-\alpha,\textrm{equivalentemente}\\
P\left\{\overline{X}-t_{\alpha/2}\frac{s}{\sqrt{n}}<\mu<\overline{X}+t_{\alpha/2}\frac{s}{\sqrt{n}}\right\}=1-\alpha.
\end{eqnarray}

\item[d) ] Un intervalo de confianza del $100\left(1-\alpha\right)\%$  de confianza para $\mu$, $\sigma^{2}$ desconocida y poblaci\'on normal es
 
 \begin{eqnarray}\mu\in\left(\overline{x}-t_{\alpha/2}\frac{s}{\sqrt{n}},\overline{x}+t_{\alpha/2}\frac{s}{\sqrt{n}}\right)\end{eqnarray}, 
 
 donde $t_{\alpha/2}$ es una $t$-student con $\nu=n-1$ grados de libertad.
\item[e) ] Los l\'imites unilaterales para $\mu$ con $\sigma$ desconocida son $\overline{X}-t_{\alpha/2}\frac{s}{\sqrt{n}}$ y $\overline{X}+t_{\alpha/2}\frac{s}{\sqrt{n}}$.

\item[f) ] Cuando la poblaci\'on no es normal, $\sigma$ desconocida y $n\geq30$, $\sigma$ se puede reemplazar por $s$ para obtener el intervalo de confianza para muestras grandes:
\begin{eqnarray}\overline{X}\pm t_{\alpha/2}\frac{s}{\sqrt{n}}.\end{eqnarray}

\item[g) ] El estimador de $\overline{X}$ de $\mu$,  $\sigma$ desconocida, la varianza de $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, el error est\'andar de $\overline{X}$ es $\sigma/\sqrt{n}$.

\item[h) ] Si $\sigma$ es desconocida y la poblaci\'on es normal, $s\rightarrow\sigma$ y se incluye el error est\'andar $s/\sqrt{n}$, entonces \begin{eqnarray}\overline{x}\pm t_{\alpha/2}\frac{s}{\sqrt{n}}.\end{eqnarray}
\end{itemize}

%---------------------------------------------------------
\subsubsection{Intervalos de confianza sobre la varianza}
%---------------------------------------------------------

Supongamos que  $X$ se distribuye normal $\left(\mu,\sigma^{2}\right)$, desconocidas. Sea $X_{1},X_{2},\ldots,X_{n}$ muestra aleatoria de tama\~no $n$ , $s^{2}$ la varianza muestral.

Se sabe que $X^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}}$ se distribuye $\chi^{2}_{n-1}$ grados de libertad. Su intervalo de confianza es
\begin{eqnarray}
\begin{array}{l}
P\left\{\chi^{2}_{1-\frac{\alpha}{2},n-1}\leq\chi^{2}\leq\chi^{2}_{\frac{\alpha}{2},n-1}\right\}=1-\alpha\\
P\left\{\chi^{2}_{1-\frac{\alpha}{2},n-1}\leq\frac{\left(n-1\right)s^{2}}{\sigma^{2}}\leq\chi^{2}_{\frac{\alpha}{2},n-1}\right\}=1-\alpha\\
P\left\{\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}}\leq\sigma^{2}\leq\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right\}=1-\alpha,
\end{array}
\end{eqnarray}
es decir

\begin{eqnarray}
\sigma^{2}\in\left[\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}},\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right],
\end{eqnarray}
los intervalos unilaterales son
\begin{eqnarray}
\sigma^{2}\in\left[\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}},\infty\right],
\end{eqnarray}
y
\begin{eqnarray}
\sigma^{2}\in\left[-\infty,\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right].
\end{eqnarray}

%---------------------------------------------------------
\subsubsection{Intervalos de confianza para proporciones}
%---------------------------------------------------------

Supongamos que se tienen una muestra de tama\~no $n$ de una poblaci\'on grande pero finita, y supongamos que $X$, $X\leq n$, pertenecen a la clase de inter\'es, entonces \begin{eqnarray}\hat{p}=\frac{\overline{X}}{n},\end{eqnarray} es el estimador puntual de la proporci\'on de la poblaci\'on que pertenece a dicha clase. $n$ y $p$ son los par\'ametros de la distribuci\'on binomial, entonces \begin{eqnarray}\hat{p}\sim N\left(p,\frac{p\left(1-p\right)}{n}\right)\end{eqnarray} aproximadamente si $p$ es distinto de $0$ y $1$; o si $n$ es suficientemente grande. Entonces
\begin{eqnarray}
Z=\frac{\hat{p}-p}{\sqrt{\frac{p\left(1-p\right)}{n}}}\sim N\left(0,1\right),\textrm{ aproximadamente.}
\end{eqnarray}
 
Entonces
\begin{eqnarray}
\begin{array}{l}
1-\alpha=P\left\{-z_{\alpha/2}\leq\frac{\hat{p}-p}{\sqrt{\frac{p\left(1-p\right)}{n}}}\leq z_{\alpha/2}\right\}\\
=P\left\{\hat{p}-z_{\alpha/2}\sqrt{\frac{p\left(1-p\right)}{n}}\leq p\leq \hat{p}+z_{\alpha/2}\sqrt{\frac{p\left(1-p\right)}{n}}\right\}
\end{array}
\end{eqnarray}
con $\sqrt{\frac{p\left(1-p\right)}{n}}$ error est\'andar del estimador puntual $p$. Una soluci\'on para determinar el intervalo de confianza del par\'ametro $p$ (desconocido) es

\begin{eqnarray}
1-\alpha=P\left\{\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\leq p\leq \hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right\}
\end{eqnarray}
entonces los intervalos de confianza, tanto unilaterales como de dos colas son: 
\begin{itemize}
\item[a) ] $p\in \left(\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}},\hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right)$,

\item[b) ] $p\in \left(-\infty,\hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right)$,

\item[c) ] $p\in \left(\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}},\infty\right)$;

\end{itemize}
para minimizar el error est\'andar, se propone que el tama\~no de la muestra sea \begin{eqnarray}n= \left(\frac{z_{\alpha/2}}{E}\right)^{2}p\left(1-p\right),\end{eqnarray} donde $$E=\mid p-\hat{p}\mid.$$


%---------------------------------------------------------
\subsubsection{Intervalos de confianza para dos muestras: Varianzas conocidas}
%---------------------------------------------------------

Sean $X_{1}$ y $X_{2}$ variables aleatorias independientes. $X_{1}$ con media desconocida $\mu_{1}$ y varianza conocida $\sigma_{1}^{2}$; y $X_{2}$ con media desconocida $\mu_{2}$ y varianza conocida $\sigma_{2}^{2}$. Se busca encontrar un intervalo de confianza de $100\left(1-\alpha\right)\%$ de la diferencia entre medias $\mu_{1}$ y $\mu_{2}$. Sean $X_{11},X_{12},\ldots,X_{1n_{1}}$ muestra aleatoria de $n_{1}$ observaciones de $X_{1}$, y sean $X_{21},X_{22},\ldots,X_{2n_{2}}$ muestra aleatoria de $n_{2}$ observaciones de $X_{2}$.\medskip

Sean $\overline{X}_{1}$ y $\overline{X}_{2}$, medias muestrales, entonces el estad\'sitico 
\begin{eqnarray}
Z=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}}\sim N\left(0,1\right),\end{eqnarray}
si $X_{1}$ y $X_{2}$ son normales o aproximadamente normales si se aplican las condiciones del Teorema de L\'imite Central respectivamente. Entonces se tiene
\begin{eqnarray}
1-\alpha&=& P\left\{-Z_{\alpha/2}\leq Z\leq Z_{\alpha/2}\right\}=P\left\{-Z_{\alpha/2}\leq \frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}}\leq Z_{\alpha/2}\right\}\\
&=&P\left\{\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\leq \mu_{1}-\mu_{2}\leq \left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right\}.
\end{eqnarray}

Entonces los intervalos de confianza unilaterales y de dos colas al $\left(1-\alpha\right)\%$ de confianza son 

\begin{itemize}
\item[a) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in \left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right],\end{eqnarray}

\item[b) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in \left[-\infty,\left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right],\end{eqnarray}

\item[c) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in \left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}},\infty\right].\end{eqnarray}
\end{itemize}

\begin{Note}
Si $\sigma_{1}$ y $\sigma_{2}$ son conocidas, o por lo menos se conoce una aproximaci\'on, y los tama\~nos de las muestras $n_{1}$ y $n_{2}$ son iguales, $n_{1}=n_{2}=n$, se puede determinar el tama\~no de la muestra para que el error al estimar $\mu_{1}-\mu_{2}$ usando $\overline{X}_{1}-\overline{X}_{2}$ sea menor que $E$ (valor del error deseado) al $\left(1-\alpha\right)\%$ de confianza. El tama\~no $n$ de la muestra requerido para cada muestra es
\begin{eqnarray}
n=\left(\frac{Z_{\alpha/2}}{E}\right)^{2}\left(\sigma_{1}^{2}+\sigma_{2}^{2}\right).
\end{eqnarray}
\end{Note}

%------------------------------------------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para dos muestras: Varianzas desconocidas e iguales}
%------------------------------------------------------------------------------------------------------------------

\begin{itemize}
\item[a) ] Si $n_{1},n_{2}\geq30$ se pueden utilizar los intervalos de la distribuci\'on normal para varianza conocida

\item[b) ] Si $n_{1},n_{2}$ son muestras peque\~nas, supongase que las poblaciones para $X_{1}$ y $X_{2}$ son normales con varianzas desconocidas y con base en el intervalo de confianza para distribuciones $t$-student
\end{itemize}


Supongamos que $X_{1}$ es una variable aleatoria con media $\mu_{1}$ y varianza $\sigma_{1}^{2}$, $X_{2}$ es una variable aleatoria con media $\mu_{2}$ y varianza $\sigma_{2}^{2}$. Todos los par\'ametros son desconocidos. Sin embargo sup\'ongase que es razonable considerar que $\sigma_{1}^{2}=\sigma_{2}^{2}=\sigma^{2}$.\medskip

Nuevamente sean $X_{1}$ y $X_{2}$ variables aleatorias independientes. $X_{1}$ con media desconocida $\mu_{1}$ y varianza muestral $S_{1}^{2}$; y $X_{2}$ con media desconocida $\mu_{2}$ y varianza muestral $S_{2}^{2}$. Dado que $S_{1}^{2}$ y $S_{2}^{2}$ son estimadores de $\sigma_{1}^{2}$, se propone el estimador $S$ de $\sigma^{2}$ como 

\begin{eqnarray}
S_{p}^{2}=\frac{\left(n_{1}-1\right)S_{1}^{2}+\left(n_{2}-1\right)S_{2}^{2}}{n_{1}+n_{2}-2},
\end{eqnarray}
entonces, el estad\'istico para $\mu_{1}-\mu_{2}$ es

\begin{eqnarray}
t_{\nu}=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}},
\end{eqnarray}
donde $t_{\nu}$ es una $t$ de student con $\nu=n_{1}+n_{2}-2$ grados de libertad.\medskip

Por lo tanto

\begin{eqnarray}
\begin{array}{l}
1-\alpha=P\left\{-t_{\alpha/2,\nu}\leq t\leq t_{\alpha/2,\nu}\right\}\\
=P\left\{\left(\overline{X}_{1}-\overline{X}_{2}\right)-t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\leq t\leq\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right\},
\end{array}
\end{eqnarray}

luego, los intervalos de confianza del $\left(1-\alpha\right)\%$ para $\mu_{1}-|mu_{2}$ son 
\begin{itemize}
\item[a) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)- t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right].\end{eqnarray}


\item[b) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in\left[-\infty,\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right].\end{eqnarray}

\item[c) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)- t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}},\infty\right].\end{eqnarray}
\end{itemize}


%------------------------------------------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para dos muestras: Varianzas desconocidas diferentes}
%------------------------------------------------------------------------------------------------------------------

Si no se tiene certeza de que $\sigma_{1}^{2}=\sigma_{2}^{2}$, se propone el estad\'istico
\begin{eqnarray}
t^{*}=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}}},
\end{eqnarray}
que se distribuye $t$-student con $\nu$ grados de libertad, donde

\begin{eqnarray}
\nu=\frac{\left(\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}\right)^{2}}{\frac{S_{1}^{2}/n_{1}}{n_{1}+1}+\frac{S_{2}^{2}/n_{2}}{n_{2}+1}}-2.
\end{eqnarray}


Entonces el intervalo de confianza de aproximadamente el $100\left(1-\alpha\right)\%$ para $\mu_{1}-\mu_{2}$ con $\sigma_{1}^{2}\neq\sigma_{2}^{2}$ es
\begin{eqnarray}
\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-t_{\alpha/2,\nu}\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+t_{\alpha/2,\nu}\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}}\right].
\end{eqnarray}

%------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para raz\'on de Varianzas}
%------------------------------------------------------------------------------

Supongamos que se toman dos muestras aleatorias independientes de las dos poblaciones de inter\'es. Sean $X_{1}$ y $X_{2}$ variables normales independientes con medias desconocidas $\mu_{1}$ y $\mu_{2}$ y varianzas desconocidas $\sigma_{1}^{2}$ y $\sigma_{2}^{2}$ respectivamente. Se busca un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\sigma_{1}^{2}/\sigma_{2}^{2}$. Supongamos $n_{1}$ y $n_{2}$ muestras aleatorias de $X_{1}$ y $X_{2}$ y sean $S_{1}^{2}$ y $S_{2}^{2}$ varianzas muestralres. Se sabe que 
\begin{eqnarray}F=\frac{S_{2}^{2}/\sigma_{2}^{2}}{S_{1}^{2}/\sigma_{1}^{2}},\end{eqnarray}
se distribuye $F$ con $n_{2}-1$ y $n_{1}-1$ grados de libertad.


Por lo tanto
\begin{eqnarray}
\begin{array}{l}
P\left\{F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq F\leq F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha,\\
P\left\{F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq \frac{S_{2}^{2}/\sigma_{2}^{2}}{S_{1}^{2}/\sigma_{1}^{2}}\leq F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha,
\end{array}
\end{eqnarray}
luego entonces
\begin{eqnarray}
P\left\{\frac{S_{1}^{2}}{S_{2}^{2}}F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq \frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\leq \frac{S_{1}^{2}}{S_{2}^{2}}F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha.
\end{eqnarray}
en consecuencia

\begin{eqnarray}
\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\in \left[\frac{S_{1}^{2}}{S_{2}^{2}}F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}, \frac{S_{1}^{2}}{S_{2}^{2}}F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right],
\end{eqnarray}
donde
\begin{eqnarray}
F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}=\frac{1}{F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}}.
\end{eqnarray}

%------------------------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para diferencia de proporciones}
%------------------------------------------------------------------------------------------------


Sean dos proporciones de inter\'es $p_{1}$ y $p_{2}$. Se busca un intervalo para $p_{1}-p_{2}$ al $100\left(1-\alpha\right)\%$. Sean dos muestras independientes de tama\~no $n_{1}$ y $n_{2}$ de poblaciones infinitas de modo que $X_{1}$ y $X_{2}$ variables aleatorias binomiales independientes con par\'ametros $\left(n_{1},p_{1}\right)$ y $\left(n_{2},p_{2}\right)$.  $X_{1}$ y $X_{2}$ son  el n\'umero de observaciones que pertenecen a la clase de inter\'es correspondientes. Entonces $\hat{p}_{1}=\frac{X_{1}}{n_{1}}$ y $\hat{p}_{2}=\frac{X_{2}}{n_{2}}$ son estimadores de $p_{1}$ y $p_{2}$ respectivamente. Supongamos que se cumple la aproximaci\'on  normal a la binomial, entonces

\begin{eqnarray}
Z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)-\left(p_{1}-p_{2}\right)}{\sqrt{\frac{p_{1}\left(1-p_{1}\right)}{n_{1}}-\frac{p_{2}\left(1-p_{2}\right)}{n_{2}}}}\sim N\left(0,1\right)\textrm{aproximadamente}
\end{eqnarray}
por tanto

\begin{eqnarray}
\left(\hat{p}_{1}-\hat{p}_{2}\right)-Z_{\frac{\alpha}{2}}\sqrt{\frac{\hat{p}_{1}\left(1-\hat{p}_{1}\right)}{n_{1}}+\frac{\hat{p}_{2}\left(1-\hat{p}_{2}\right)}{n_{2}}}\leq p_{1}-p_{2}\leq\left(\hat{p}_{1}-\hat{p}_{2}\right)+Z_{\frac{\alpha}{2}}\sqrt{\frac{\hat{p}_{1}\left(1-\hat{p}_{1}\right)}{n_{1}}-\frac{\hat{p}_{2}\left(1-\hat{p}_{2}\right)}{n_{2}}}
\end{eqnarray}



%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{An\'alisis de Regresion Lineal (RL)}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>


En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$. La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente.  A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.



%------------------------------------------------------------------
\subsection{Regresi\'on Lineal Simple (RLS)}
%------------------------------------------------------------------

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria. El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray*}
E\left(y|x\right)=\beta_{0}+\beta_{1}x,
\end{eqnarray*}
$\beta_{0}$ es la ordenada al or\'igen y  $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion.Original}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}

donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion.Original}, este se le llama \textit{modelo de regresi\'on lineal simple}. Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$,  estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on realiza por el \textbf{m\'etodos de m\'inimos cuadrados}.

Entonces la ecuaci\'on (\ref{Modelo.Regresion.Original}) se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos.Original}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on 
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}.
\end{eqnarray}

Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$,  $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$,  igualar a cero y  resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\textrm{, }\frac{\partial L}{\partial \beta_{1}}=0.
\end{eqnarray*}
Evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene 

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0,\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0,
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i},\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}.
\end{eqnarray*}

Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ec.Normales.Min.Cuadrados}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x},\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}},
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x.
\end{eqnarray}

Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2},\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right);
\end{eqnarray}
y por tanto

\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}.
\end{eqnarray}
%------------------------------------------------------------------
\subsection{Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}
%------------------------------------------------------------------

Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo. Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.

A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)+E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]\\
&=&\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}.
\end{eqnarray*}

Por lo tanto 
\begin{equation}\label{Esperanza.Beta.1.Original}
E\left(\hat{\beta}_{1}\right)=\beta_{1},
\end{equation}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado. Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}},
\end{eqnarray*}

por lo tanto

\begin{equation}\label{Varianza.Beta.1.Original}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}.
\end{equation}

Entonces tenemos la siguiente proposici\'on: 
\begin{Prop}
\begin{eqnarray}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray}
\end{Prop}
Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}:
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{eqnarray}
sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto,}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=\mathbf{MC_{E}}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray}

%------------------------------------------------------------------
\subsection{Prueba de Hip\'otesis en RLS}
%------------------------------------------------------------------

Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza. Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$. Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:

\begin{center}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,
\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.
\end{itemize}
\end{center}

donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. De las ecuaciones (\ref{Ec.Normales.Min.Cuadrados}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1.Original}) y (\ref{Varianza.Beta.1.Original}).
Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1.Original}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0.Original}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0.Original}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\\
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\\
\end{eqnarray*}

\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&+&\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
=\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\&+&\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}

Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados.Original}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}
Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados.Original}) se puede reescribir como: 
\begin{equation}\label{Suma.Total.Cuadrados.Dos.Original}
S_{yy}=SC_{R}+SC_{E},
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$:
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right),\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}.
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.

\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx},
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}
Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}},
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$. El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente

\begin{center}
\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 
\end{center}

La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1.Original}), con $\hat{\beta}_{1,0}=0$, es decir,
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}.
\end{equation}

Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}.
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.
%----------------------------------------------------------------
\subsection{Estimaci\'on de Intervalos en RLS}
%----------------------------------------------------------------

Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros. El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on. Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por
\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}

%------------------------------------------------------------------
\subsection{Predicci\'on}
%------------------------------------------------------------------

Supongamos que se tiene un valor $x_{0}$ de inter\'es, entonces la estimaci\'on puntual de este nuevo valor
\begin{equation}
\hat{y}_{0}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}.
\end{equation}

Esta nueva observaci\'on es independiente de las utilizadas para obtener el modelo de regresi\'on, por tanto, el intervalo en torno a la recta de regresi\'on es inapropiado, puesto que se basa \'unicamente en los datos empleados para ajustar el modelo de regresi\'on. El intervalo de confianza en torno a la recta de regresi\'on se refiere a la respuesta media verdadera $x=x_{0}$, no a observaciones futuras. Sea $y_{0}$ la observaci\'on futura en $x=x_{0}$, y sea $\hat{y}_{0}$ dada en la ecuaci\'on anterior, el estimador de $y_{0}$. Si se define la variable aleatoria $$w=y_{0}-\hat{y}_{0},$$ esta se distribuye normalmente con media cero y varianza 

\begin{eqnarray}\label{Eq.Varianza.w}
V\left(w\right)=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]\end{eqnarray}
dado que $y_{0}$ es independiente de $\hat{y}_{0}$, por lo tanto el intervalo de predicci\'on al nivel $\alpha$ para futuras observaciones $x_{0}$ es


\begin{eqnarray}\label{Intervalo.Confianza.w}
\hat{y}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}\leq y_{0}\leq \hat{y}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}.
\end{eqnarray}




%------------------------------------------------------------------
\subsection{Prueba de falta de ajuste}
%------------------------------------------------------------------

Es com\'un encontrar que el modelo ajustado no satisface totalmente el modelo necesario para los datos, en este caso es preciso saber qu\'e tan bueno es el modelo propuesto. Para esto se propone la siguiente prueba de hip\'otesis:
\begin{itemize}
\item[$H_{0}:$ ]El modelo propuesto se ajusta adecuademente a los datos.
\item[$H_{1}:$ ]El modelo NO se ajusta a los datos.
\end{itemize}
La prueba implica dividir la suma de cuadrados del eror o del residuo en las siguientes dos componentes:
\begin{eqnarray}\label{Suma.cuadrdos.errores}
SC_{E}=SC_{EP}+SC_{FDA}
\end{eqnarray}

donde $SC_{EP}$ es la suma de cuadrados atribuibles al error puro, y $SC_{FDA}$ es la suma de cuadrados atribuible a la falta de ajuste del modelo.

%------------------------------------------------------------------
\subsection{Coeficiente de Determinaci\'on}
%------------------------------------------------------------------


La cantidad
\begin{equation}\label{Coeficiente.Determinacion}
R^{2}=\frac{SC_{R}}{S_{yy}}=1-\frac{SC_{E}}{S_{yy}},
\end{equation}
se denomina coeficiente de determinaci\'on y se utiliza para saber si el modelo de regresi\'on es suficiente o no. Se puede demostrar que $0\leq R^{2}\leq1$, una manera de interpretar este valor es que si $R^{2}=k$, entonces el modelo de regresi\'on explica el $k*100\%$ de la variabilidad en los datos.
$R^{2}$ . Este coeficiente tiene las siguientes propiedades
\begin{itemize}
\item No mide la magnitud de la pendiente de la recta de regresi\'on.
\item Un valor grande de $R^{2}$ no implica una pendiente empinada.
\item No mide la suficiencia del modelo.
\item Valores grandes de $R^{2}$ no implican necesariamente que el modelo de regresi\'on proporcionar\'a predicciones precisas para futuras observaciones.
\end{itemize}


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Regresi\'on Log\'istica}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->


%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Introducci\'on}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>


La regresión logística es una técnica de modelado estadístico ampliamente utilizada en análisis de datos cuando el objetivo es predecir la probabilidad de un resultado binario, es decir, cuando la variable dependiente o respuesta tiene dos posibles categorías, como "éxito/fallo" o "sí/no". Esta técnica se emplea en una variedad de disciplinas, como la biomedicina, ciencias sociales, marketing y más, para resolver problemas donde la variable respuesta es discreta o categórica.\medskip

A diferencia de la regresión lineal, que asume una relación lineal entre las variables independientes y la variable dependiente y que produce valores en un rango continuo, la regresión logística está diseñada para manejar situaciones donde la respuesta es categórica. En su forma más común, la regresión logística binaria, el modelo predice la probabilidad de que un evento ocurra en función de una o más variables independientes. Este tipo de regresión toma la forma de un modelo no lineal, debido a la naturaleza discreta de la variable dependiente.\medskip


La regresión lineal busca modelar la relación entre una variable dependiente continua $Y$ y una o más variables independientes $X_1, X_2, \ldots, X_n$ mediante una ecuación de la forma:
\begin{eqnarray}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon,
\end{eqnarray}
donde $\beta_0, \beta_1, \ldots, \beta_n$ son los coeficientes del modelo y $\epsilon$ es el término de error. La regresión logística, en cambio, modela la probabilidad de que un evento ocurra (por ejemplo, éxito vs. fracaso) utilizando la función logística. La variable dependiente $Y$ es binaria, tomando valores de 0 o 1. La ecuación de la regresión logística es:
\begin{eqnarray}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n,
\end{eqnarray}
donde $p$ es la probabilidad de que $Y=1$. La función logística es:
\begin{eqnarray}
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}.
\end{eqnarray}

La regresión logística se utiliza en una variedad de campos para problemas de clasificación binaria, tales como:
\begin{itemize}
    \item \textbf{Medicina}: Predicción de la presencia o ausencia de una enfermedad.
    \item \textbf{Marketing}: Determinación de la probabilidad de que un cliente compre un producto.
    \item \textbf{Finanzas}: Evaluación del riesgo de crédito, es decir, si un cliente va a incumplir o no con un préstamo.
    \item \textbf{Seguridad}: Detección de fraudes o intrusiones.
\end{itemize}

%------------------------------------------------------
\subsection{Implementación Básica en R}
%------------------------------------------------------

Para implementar una regresión logística en R, primero es necesario instalar y cargar los paquetes necesarios. Aquí se muestra un ejemplo básico de implementación:

\begin{itemize}
    \item Descargue e instale R desde \texttt{https://cran.r-project.org/}.
    \item Descargue e instale RStudio desde \texttt{https://rstudio.com/products/rstudio/download/}.
\end{itemize}

\begin{verbatim}
# Instalación del paquete necesario
install.packages("stats")

# Carga del paquete
library(stats)

# Ejemplo de conjunto de datos
data <- data.frame(
  outcome = c(1, 0, 1, 0, 1, 1, 0, 1, 0, 0),
  predictor = c(2.3, 1.9, 3.1, 2.8, 3.6, 2.4, 2.1, 3.3, 2.2, 1.7)
)

# Ajuste del modelo de regresión logística
model <- glm(outcome ~ predictor, data = data, family = binomial)

# Resumen del modelo
summary(model)
\end{verbatim}

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Conceptos Básicos}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

La regresión logística es una técnica de modelado estadístico utilizada para predecir la probabilidad de un evento binario (es decir, un evento que tiene dos posibles resultados) en función de una o más variables independientes.  Un modelo de regresión logística describe cómo una variable dependiente binaria $Y$ (que puede tomar los valores $0$ o $1$) está relacionada con una o más variables independientes $X_1, X_2, \ldots, X_n$. A diferencia de la regresión lineal, que predice un valor continuo, la regresión logística predice una probabilidad que puede ser interpretada como la probabilidad de que $Y=1$ dado un conjunto de valores para $X_1, X_2, \ldots, X_n$.

%----------------------------------------------------------------
\subsection{Regresión Lineal}
%----------------------------------------------------------------

La regresión lineal es utilizada para predecir el valor de una variable dependiente continua en función de una o más variables independientes. El modelo de regresión lineal tiene la forma:
\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
\end{equation}
donde:
\begin{itemize}
    \item[a) ] $Y$ es la variable dependiente.
    \item[b) ] $\beta_0$ es la intersección con el eje $Y$ o término constante.
    \item[c) ] $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes que representan la relación entre las variables independientes y la variable dependiente.
    \item[d) ] $X_1, X_2, \ldots, X_n$ son las variables independientes.
    \item[e) ] $\epsilon$ es el término de error, que representa la desviación de los datos observados de los valores predichos por el modelo.
\end{itemize}

El objetivo de la regresión lineal es encontrar los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ que minimicen la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos. Este método se conoce como mínimos cuadrados ordinarios (OLS, por sus siglas en inglés). La función de costo a minimizar es:
\begin{equation}
J\left(\beta_0, \beta_1, \ldots, \beta_n\right) = \sum_{i=1}^{n}\left(y_i - \hat{y}_i\right)^2
\end{equation}
donde:
\begin{itemize}
    \item[a) ] $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación.
    \item[b) ] $\hat{y}_i$ es el valor predicho por el modelo para la $i$-ésima observación, dado por:
    \begin{equation}
    \hat{y}_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_n x_{in}
    \end{equation}
\end{itemize}

Para encontrar los valores óptimos de los coeficientes, se toman las derivadas parciales de la función de costo con respecto a cada coeficiente y se igualan a cero:
\begin{equation}
\frac{\partial J}{\partial \beta_j} = 0 \quad \text{para } j = 0, 1, \ldots, n
\end{equation}

Resolviendo este sistema de ecuaciones, se obtienen los valores de los coeficientes que minimizan la función de costo.
%----------------------------------------------------------------
\subsection{Regresión Logística}
%----------------------------------------------------------------

La deducción de la fórmula de la regresión logística comienza con la necesidad de modelar la probabilidad de un evento binario. Queremos encontrar una función que relacione las variables independientes con la probabilidad de que la variable dependiente tome el valor $1$. La probabilidad de que el evento ocurra, $P(Y=1)$, se denota como $p$. La probabilidad de que el evento no ocurra, $P(Y=0)$, es $1-p$. Los \textbf{odds} (chances) de que ocurra el evento se definen como:
\begin{equation}
\text{odds} = \frac{p}{1-p}
\end{equation}
Los odds indican cuántas veces más probable es que ocurra el evento frente a que no ocurra. Para simplificar el modelado de los \textit{odds}, se aplica el logaritmo natural, obteniendo la función \textbf{logit}:
\begin{equation}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right)
\end{equation}
La transformación logit es útil porque convierte el rango de la probabilidad (0, 1) al rango de números reales $\left(-\infty, \infty\right)$. La idea clave de la regresión logística es modelar la transformación logit de la probabilidad como una combinación lineal de las variables independientes:
\begin{equation}\label{Ec.logit}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{equation}
Aquí, $\beta_0$ es el t\'ermino constante y $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes asociados con las variables independientes $X_1, X_2, \ldots, X_n$. Para expresar $p$ en función de una combinación lineal de las variables independientes, invertimos la transformación logit. Partimos de la ecuación \ref{Ec.logit}, aplicando la funci\'on exponencial en ambos lados:
\begin{eqnarray}
\frac{p}{1-p} = e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}
\end{eqnarray}
Despejando $p$:
\begin{eqnarray}
p = \frac{e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}{1 + e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}
\end{eqnarray}

La expresión final que obtenemos es conocida como la \textbf{función logística}:
\begin{equation}\label{Eq.Logit1}
p = \frac{1}{1 + e^{-\left(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n\right)}}
\end{equation}
Esta función describe cómo las variables independientes se relacionan con la probabilidad de que el evento de interés ocurra. Los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ se estiman a partir de los datos utilizando el método de máxima verosimilitud.
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Método de Máxima Verosimilitud}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

Para estimar los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ en la regresión logística, utilizamos el método de máxima verosimilitud. La idea es encontrar los valores de los coeficientes que maximicen la probabilidad de observar los datos dados. Esta probabilidad se expresa mediante la función de verosimilitud $L$. La función de verosimilitud $L(\beta_0, \beta_1, \ldots, \beta_n)$ para un conjunto de $n$ observaciones se define como el producto de las probabilidades de las observaciones dadas las variables independientes:

\begin{equation}\label{Eq.Verosimilitud}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{equation}

donde:
\begin{itemize}
    \item $p_i$ es la probabilidad predicha de que $Y_i = 1$,
    \item $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación.
\end{itemize}



Trabajar directamente con esta función de verosimilitud puede ser complicado debido al \pageref{•}roducto de muchas probabilidades, especialmente si $n$ es grande. Para simplificar los cálculos, se utiliza el logaritmo de la función de verosimilitud, conocido como la función de log-verosimilitud. El uso del logaritmo simplifica significativamente la diferenciación y maximización de la función. La función de log-verosimilitud se define como:

\begin{equation}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}

Aquí, $\log$ representa el logaritmo natural. Esta transformación es válida porque el logaritmo es una función monótona creciente, lo que significa que maximizar la log-verosimilitud es equivalente a maximizar la verosimilitud original. En la regresión logística, la probabilidad $p_i$ está dada por la función logística:

\begin{equation}
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}}
\end{equation}

Sustituyendo esta expresión en la función de log-verosimilitud, obtenemos:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &= \sum_{i=1}^{n} \left[ y_i \log \left( \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}} \right) + \right. \nonumber \\
& \quad \left. (1 - y_i) \log \left( 1 - \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}} \right) \right]
\end{eqnarray*}

Simplificando esta expresión, notamos que:

\begin{eqnarray*}
\log \left( \frac{1}{1 + e^{-z}} \right) = -\log(1 + e^{-z})
\end{eqnarray*}

y

\begin{eqnarray*}
\log \left( 1 - \frac{1}{1 + e^{-z}} \right) = \log \left( \frac{e^{-z}}{1 + e^{-z}} \right) = -z - \log(1 + e^{-z})
\end{eqnarray*}

Aplicando estas identidades, la función de log-verosimilitud se convierte en:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &=& \sum_{i=1}^{n} \left[ y_i (-\log(1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})})) + \right. \nonumber \\
&& \quad \left. (1 - y_i) \left( -(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}) \right) \right]
\end{eqnarray*}

Simplificando aún más, obtenemos:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &=& \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})\right.\\
& -&\left. \log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{eqnarray*}


Para simplificar aún más la notación, podemos utilizar notación matricial. Definimos la matriz $\mathbf{X}$ de tamaño $n \times (k+1)$ y el vector de coeficientes $\boldsymbol{\beta}$ de tamaño $(k+1) \times 1$ como sigue:

\begin{equation}\label{Eq.Matricial1}
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1k} \\
1 & x_{21} & x_{22} & \ldots & x_{2k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nk}
\end{bmatrix}, \quad
\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix}
\end{equation}

Entonces, la expresión para la función de log-verosimilitud es:

\begin{equation}\label{Eq.LogLikelihood1}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i (\mathbf{X}_i \boldsymbol{\beta}) - \log(1 + e^{\mathbf{X}_i \boldsymbol{\beta}}) \right]
\end{equation}

donde $\mathbf{X}_i$ es la $i$-ésima fila de la matriz $\mathbf{X}$.  Esta notación matricial simplifica la implementación y la derivación de los estimadores de los coeficientes en la regresión logística. Utilizando métodos numéricos, como el algoritmo de Newton-Raphson, se pueden encontrar los coeficientes que maximizan la función de log-verosimilitud. Para maximizar la función de log-verosimilitud, derivamos esta función con respecto a cada uno de los coeficientes $\beta_j$ y encontramos los puntos críticos. La derivada parcial de la función de log-verosimilitud con respecto a $\beta_j$ es:

\begin{eqnarray}\label{Eq.1.14}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} \left[ y_i X_{ij} - \frac{X_{ij} e^{\mathbf{X}_i \boldsymbol{\beta}}}{1 + e^{\mathbf{X}_i \boldsymbol{\beta}}} \right]
\end{eqnarray}

Simplificando, esta derivada se puede expresar como:

\begin{eqnarray}\label{Eq.PrimeraDerivada}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} X_{ij} (y_i - p_i),\textrm{ donde }p_i = \frac{1}{1 + e^{-\mathbf{X}_i \boldsymbol{\beta}}}
\end{eqnarray}

Para encontrar los coeficientes que maximizan la log-verosimilitud, resolvemos el sistema de ecuaciones 
\begin{eqnarray*}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = 0 \textrm{ para todos los }j = 0, 1, \ldots, k. 
\end{eqnarray*}
Este sistema de ecuaciones no tiene una solución analítica cerrada, por lo que se resuelve numéricamente utilizando métodos iterativos como el algoritmo de Newton-Raphson.

\section{Método de Newton-Raphson}

El método de Newton-Raphson es un algoritmo iterativo que se utiliza para encontrar las raíces de una función. En el contexto de la regresión logística, se utiliza para maximizar la función de log-verosimilitud encontrando los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$. Este m\'etodo se basa en una aproximación de segundo orden de la función objetivo. Dado un valor inicial de los coeficientes $\boldsymbol{\beta}^{(0)}$, se actualiza iterativamente el valor de los coeficientes utilizando la fórmula:

\begin{equation}\label{Eq.Criterio0}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \mathbf{H}(\boldsymbol{\beta}^{(t)}) \right]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
\end{equation}

donde:
\begin{itemize}
    \item $\boldsymbol{\beta}^{(t)}$ es el vector de coeficientes en la $t$-ésima iteración.
    \item $\nabla \log L(\boldsymbol{\beta}^{(t)})$ es el gradiente de la función de log-verosimilitud con respecto a los coeficientes $\boldsymbol{\beta}$:

\begin{equation}\label{Eq.Gradiente1}
\nabla \log L(\boldsymbol{\beta}) = \mathbf{X}^T (\mathbf{y} - \mathbf{p})
\end{equation}

donde $\mathbf{y}$ es el vector de valores observados y $\mathbf{p}$ es el vector de probabilidades.
    \item $\mathbf{H}(\boldsymbol{\beta}^{(t)})$ es la matriz Hessiana (matriz de segundas derivadas) evaluada en $\boldsymbol{\beta}^{(t)}$:
\begin{equation}\label{Eq.Hessiana1}
\mathbf{H}(\boldsymbol{\beta}) = -\mathbf{X}^T \mathbf{W} \mathbf{X}
\end{equation}

donde $\mathbf{W}$ es una matriz diagonal de pesos con elementos $w_i = p_i (1 - p_i)$.

\end{itemize}

En resumen:

\begin{Algthm}\label{Algoritmo1}
El algoritmo Newton-Raphson para la regresión logística se puede resumir en los siguientes pasos:
\begin{enumerate}
    \item Inicializar el vector de coeficientes $\boldsymbol{\beta}^{(0)}$ (por ejemplo, con ceros o valores pequeños aleatorios).
    \item Calcular el gradiente $\nabla \log L(\boldsymbol{\beta}^{(t)})$ y la matriz Hessiana $\mathbf{H}(\boldsymbol{\beta}^{(t)})$ en la iteración $t$.
    \item Actualizar los coeficientes utilizando la fórmula:
    \begin{equation}\label{Eq.Criterio1}
    \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \mathbf{H}(\boldsymbol{\beta}^{(t)}) \right]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
    \end{equation}
    \item Repetir los pasos 2 y 3 hasta que la diferencia entre $\boldsymbol{\beta}^{(t+1)}$ y $\boldsymbol{\beta}^{(t)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}
\end{Algthm}

En resumen, el método de Newton-Raphson permite encontrar los coeficientes que maximizan la función de log-verosimilitud de manera eficiente. 

\section{Espec\'ificando}
En espec\'ifico para un conjunto de $n$ observaciones, la función de verosimilitud $L$ se define como el producto de las probabilidades individuales de observar cada dato:
\begin{equation}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{equation}
donde $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación y $p_i$ es la probabilidad predicha de que $Y_i = 1$. Aquí, $p_i$ es dado por la función logística:
\begin{equation}
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}}
\end{equation}

Tomando el logaritmo:
\begin{equation}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}

Sustituyendo $p_i$:
\begin{equation}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{equation}

Dado que el objetivo es encontrar los valores de $\beta_0, \beta_1, \ldots, \beta_n$ que maximicen la función de log-verosimilitud.  Para $\beta_j$, la derivada parcial de la función de log-verosimilitud es:
\begin{equation}
\frac{\partial \log L}{\partial \beta_j} = \sum_{i=1}^{n} \left[ y_i X_{ij} - \frac{X_{ij} e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}}{1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}} \right]
\end{equation}

Esto se simplifica a (comparar con la ecuaci\'on \ref{Eq.1.14}):
\begin{eqnarray}\label{Eq.1.25}
\frac{\partial \log L}{\partial \beta_j} = \sum_{i=1}^{n} X_{ij} (y_i - p_i)
\end{eqnarray}


Para maximizar la log-verosimilitud, resolvemos el sistema de ecuaciones $\frac{\partial \log L}{\partial \beta_j} = 0$ para todos los $j$ de 0 a $n$., mismo que se resuelve numéricamente utilizando métodos el algoritmo de Newton-Raphson. El método de Newton-Raphson se basa en una aproximación de segundo orden de la función objetivo. Dado un valor inicial de los coeficientes $\beta^{(0)}$, se iterativamente actualiza el valor de los coeficientes utilizando la fórmula:
\begin{equation}\label{Eq.Criterio1.5}
\beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
\end{equation}
donde:
\begin{itemize}
    \item $\beta^{(k)}$ es el vector de coeficientes en la $k$-ésima iteración.
    \item $\mathbf{g}(\beta^{(k)})$ es el gradiente (vector de primeras derivadas) evaluado en $\beta^{(k)}$:
\begin{equation}\label{Eq.Gradiente2}
\mathbf{g}(\beta) = \frac{\partial \log L}{\partial \beta} = \sum_{i=1}^{n} \mathbf{X}_i (y_i - p_i)
\end{equation}
donde $\mathbf{X}_i$ es el vector de valores de las variables independientes para la $i$-ésima observación (comparar con ecuaci\'on \ref{Eq.Gradiente1}).

    \item $\mathbf{H}(\beta^{(k)})$ es la matriz Hessiana (matriz de segundas derivadas) evaluada en $\beta^{(k)}$:
\begin{equation}\label{Eq.Hessiana2}
\mathbf{H}(\beta) = \frac{\partial^2 \log L}{\partial \beta \partial \beta^T} = -\sum_{i=1}^{n} p_i (1 - p_i) \mathbf{X}_i \mathbf{X}_i^T,
\end{equation}
comparar con ecuaci\'on \ref{Eq.Hessiana1}
\end{itemize}

\begin{Algthm} \label{Algoritmo2}
Los pasos del algoritmo Newton-Raphson para la regresión logística son:
\begin{enumerate}
    \item Inicializar el vector de coeficientes $\beta^{(0)}$ (por ejemplo, con ceros o valores pequeños aleatorios).
    \item Calcular el gradiente $\mathbf{g}(\beta^{(k)})$ y la matriz Hessiana $\mathbf{H}(\beta^{(k)})$ en la iteración $k$.
    \item Actualizar los coeficientes utilizando la fórmula:
    \begin{equation}\label{Eq.Criterio2}
    \beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
    \end{equation}
    \item Repetir los pasos 2 y 3 hasta que la diferencia entre $\beta^{(k+1)}$ y $\beta^{(k)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}
\end{Algthm}
Como se puede observar la diferencia entre el Algoritmo \ref{Algoritmo1} y el Algoritmo \ref{Algoritmo2} son m\'inimas

\section*{Notas finales}

En el contexto de la regresión logística, los vectores $X_1, X_2, \ldots, X_n$ representan las variables independientes. Cada $X_j$ es un vector columna que contiene los valores de la variable independiente $j$ para cada una de las $n$ observaciones. Es decir,

\begin{equation}
X_j = \begin{bmatrix}
x_{1j} \\
x_{2j} \\
\vdots \\
x_{nj}
\end{bmatrix}
\end{equation}

Para simplificar la notación y los cálculos, a menudo combinamos todos los vectores de variables independientes en una única matriz de diseño $\mathbf{X}$ de tamaño $n \times (k+1)$, donde $n$ es el número de observaciones y $k+1$ es el número de variables independientes más el término de intercepto. La primera columna de $\mathbf{X}$ corresponde a un vector de unos para el término de intercepto, y las demás columnas corresponden a los valores de las variables independientes:

\begin{equation}
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1k} \\
1 & x_{21} & x_{22} & \ldots & x_{2k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nk}
\end{bmatrix}
\end{equation}
revisar la ecuaci\'on \ref{Eq.Matricial1}. De esta forma, el modelo logit puede ser escrito de manera compacta utilizando la notación matricial:

\begin{equation}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \mathbf{X} \boldsymbol{\beta}
\end{equation}

donde $\boldsymbol{\beta}$ es el vector de coeficientes:

\begin{equation}
\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix}
\end{equation}

Así, la probabilidad $p$ se puede expresar como:

\begin{equation}\label{Eq.Logit2}
p = \frac{1}{1 + e^{-\mathbf{X} \boldsymbol{\beta}}}
\end{equation}

Comparar la ecuaci\'on anterior con la ecuaci\'on \ref{Eq.Logit1}. Esta notación matricial simplifica la implementación y la derivación de los estimadores de los coeficientes en la regresión logística. Para estimar los coeficientes $\boldsymbol{\beta}$ en la regresión logística, se utiliza el método de máxima verosimilitud. La función de verosimilitud $L(\boldsymbol{\beta})$ se define como el producto de las probabilidades de las observaciones dadas las variables independientes, recordemos la ecuaci\'on \ref{Eq.Verosimilitud}:

\begin{eqnarray}
L(\boldsymbol{\beta}) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{eqnarray}


donde $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación, y $p_i$ es la probabilidad predicha de que $Y_i = 1$.  La función de log-verosimilitud, que es más fácil de maximizar, se obtiene tomando el logaritmo natural de la función de verosimilitud (\ref{Eq.LogLikelihood1}):

\begin{equation}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}

Sustituyendo $p_i = \frac{1}{1 + e^{-\mathbf{X}_i \boldsymbol{\beta}}}$, donde $\mathbf{X}_i$ es la $i$-ésima fila de la matriz de diseño $\mathbf{X}$, obtenemos:

\begin{equation}\label{Eq.LogLikelihood2}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i (\mathbf{X}_i \boldsymbol{\beta}) - \log(1 + e^{\mathbf{X}_i \boldsymbol{\beta}}) \right]
\end{equation}

Para encontrar los valores de $\boldsymbol{\beta}$ que maximizan la función de log-verosimilitud, se utiliza un algoritmo iterativo como el método de Newton-Raphson. Este método requiere calcular el gradiente y la matriz Hessiana de la función de log-verosimilitud.


El gradiente de la función de log-verosimilitud con respecto a $\boldsymbol{\beta}$ es (\ref{Eq.Gradiente1} y \ref{Eq.Gradiente2}):

\begin{equation}
\nabla \log L(\boldsymbol{\beta}) = \mathbf{X}^T (\mathbf{y} - \mathbf{p})
\end{equation}

donde $\mathbf{y}$ es el vector de valores observados y $\mathbf{p}$ es el vector de probabilidades predichas.

La matriz Hessiana de la función de log-verosimilitud es (\ref{Eq.Hessiana1} y \ref{Eq.Hessiana2}):

\begin{equation}
\mathbf{H}(\boldsymbol{\beta}) = -\mathbf{X}^T \mathbf{W} \mathbf{X}
\end{equation}

donde $\mathbf{W}$ es una matriz diagonal de pesos con elementos $w_i = p_i (1 - p_i)$.

El método de Newton-Raphson actualiza los coeficientes $\boldsymbol{\beta}$ de la siguiente manera:

\begin{equation}\label{Eq.Criterio3}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - [\mathbf{H}(\boldsymbol{\beta}^{(t)})]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
\end{equation}

Iterando este proceso hasta que la diferencia entre $\boldsymbol{\beta}^{(t+1)}$ y $\boldsymbol{\beta}^{(t)}$ sea menor que un umbral predefinido (\ref{Eq.Criterio0}, \ref{Eq.Criterio1}, \ref{Eq.Criterio1.5} y \ref{Eq.Criterio2}), se obtienen los estimadores de máxima verosimilitud para los coeficientes de la regresión logística.


\chapter{Elementos de Probabilidad}
\section{Introducci\'on}

Los fundamentos de probabilidad y estad\'istica son esenciales para comprender y aplicar t\'ecnicas de an\'alisis de datos y modelado estad\'istico, incluyendo la regresi\'on lineal y log\'istica. Este cap\'itulo proporciona una revisi\'on de los conceptos clave en probabilidad y estad\'istica que son relevantes para estos m\'etodos.

\section{Probabilidad}

La probabilidad es una medida de la incertidumbre o el grado de creencia en la ocurrencia de un evento. Los conceptos fundamentales incluyen:

\subsection{Espacio Muestral y Eventos}

El espacio muestral, denotado como $S$, es el conjunto de todos los posibles resultados de un experimento aleatorio. Un evento es un subconjunto del espacio muestral. Por ejemplo, si lanzamos un dado, el espacio muestral es:
\begin{eqnarray*}
S = \{1, 2, 3, 4, 5, 6\}
\end{eqnarray*}
Un evento podr\'ia ser obtener un n\'umero par:
\begin{eqnarray*}
E = \{2, 4, 6\}
\end{eqnarray*}

\subsection{Definiciones de Probabilidad}

Existen varias definiciones de probabilidad, incluyendo la probabilidad cl\'asica, la probabilidad frecuentista y la probabilidad bayesiana.

\subsubsection{Probabilidad Cl\'asica}

La probabilidad cl\'asica se define como el n\'umero de resultados favorables dividido por el n\'umero total de resultados posibles:
\begin{eqnarray*}
P(E) = \frac{|E|}{|S|}
\end{eqnarray*}
donde $|E|$ es el n\'umero de elementos en el evento $E$ y $|S|$ es el n\'umero de elementos en el espacio muestral $S$.

\subsubsection{Probabilidad Frecuentista}

La probabilidad frecuentista se basa en la frecuencia relativa de ocurrencia de un evento en un gran n\'umero de repeticiones del experimento:
\begin{eqnarray*}
P(E) = \lim_{n \to \infty} \frac{n_E}{n}
\end{eqnarray*}
donde $n_E$ es el n\'umero de veces que ocurre el evento $E$ y $n$ es el n\'umero total de repeticiones del experimento.

\subsubsection{Probabilidad Bayesiana}

La probabilidad bayesiana se interpreta como un grado de creencia actualizado a medida que se dispone de nueva informaci\'on. Se basa en el teorema de Bayes:
\begin{eqnarray*}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{eqnarray*}
donde $P(A|B)$ es la probabilidad de $A$ dado $B$, $P(B|A)$ es la probabilidad de $B$ dado $A$, $P(A)$ y $P(B)$ son las probabilidades de $A$ y $B$ respectivamente.

\section{Estad\'istica Bayesiana}

La estad\'istica bayesiana proporciona un enfoque coherente para el an\'alisis de datos basado en el teorema de Bayes. Los conceptos fundamentales incluyen:

\subsection{Prior y Posterior}

\subsubsection{Distribuci\'on Prior}

La distribuci\'on prior (apriori) representa nuestra creencia sobre los par\'ametros antes de observar los datos. Es una distribuci\'on de probabilidad que refleja nuestra incertidumbre inicial sobre los par\'ametros. Por ejemplo, si creemos que un par\'ametro $\theta$ sigue una distribuci\'on normal con media $\mu_0$ y varianza $\sigma_0^2$, nuestra prior ser\'ia:
\begin{eqnarray*}
P(\theta) = \frac{1}{\sqrt{2\pi\sigma_0^2}} e^{-\frac{(\theta-\mu_0)^2}{2\sigma_0^2}}
\end{eqnarray*}

\subsubsection{Verosimilitud}

La verosimilitud (likelihood) es la probabilidad de observar los datos dados los par\'ametros. Es una funci\'on de los par\'ametros $\theta$ dada una muestra de datos $X$:
\begin{eqnarray*}
L(\theta; X) = P(X|\theta)
\end{eqnarray*}
donde $X$ son los datos observados y $\theta$ son los par\'ametros del modelo.

\subsubsection{Distribuci\'on Posterior}

La distribuci\'on posterior (a posteriori) combina la informaci\'on de la prior y la verosimilitud utilizando el teorema de Bayes. Representa nuestra creencia sobre los par\'ametros despu\'es de observar los datos:
\begin{eqnarray*}
P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)}
\end{eqnarray*}
donde $P(\theta|X)$ es la distribuci\'on posterior, $P(X|\theta)$ es la verosimilitud, $P(\theta)$ es la prior y $P(X)$ es la probabilidad marginal de los datos.

La probabilidad marginal de los datos $P(X)$ se puede calcular como:
\begin{eqnarray*}
P(X) = \int_{\Theta} P(X|\theta)P(\theta) d\theta
\end{eqnarray*}
donde $\Theta$ es el espacio de todos los posibles valores del par\'ametro $\theta$.

\section{Distribuciones de Probabilidad}

Las distribuciones de probabilidad describen c\'omo se distribuyen los valores de una variable aleatoria. Existen distribuciones de probabilidad discretas y continuas.

\subsection{Distribuciones Discretas}

Una variable aleatoria discreta toma un n\'umero finito o contable de valores. Algunas distribuciones discretas comunes incluyen:

\subsubsection{Distribuci\'on Binomial}

La distribuci\'on binomial describe el n\'umero de \'exitos en una serie de ensayos de Bernoulli independientes y con la misma probabilidad de \'exito. La funci\'on de probabilidad es:
\begin{eqnarray*}
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
\end{eqnarray*}
donde $X$ es el n\'umero de \'exitos, $n$ es el n\'umero de ensayos, $p$ es la probabilidad de \'exito en cada ensayo, y $\binom{n}{k}$ es el coeficiente binomial.

La funci\'on generadora de momentos (MGF) para la distribuci\'on binomial es:
\begin{eqnarray*}
M_X(t) = \left( 1 - p + pe^t \right)^n
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria binomial son:
\begin{eqnarray*}
E(X) &=& np \\
\text{Var}(X) &=& np(1-p)
\end{eqnarray*}

\subsubsection{Distribuci\'on de Poisson}

La distribuci\'on de Poisson describe el n\'umero de eventos que ocurren en un intervalo de tiempo fijo o en un \'area fija. La funci\'on de probabilidad es:
\begin{eqnarray*}
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
\end{eqnarray*}
donde $X$ es el n\'umero de eventos, $\lambda$ es la tasa media de eventos por intervalo, y $k$ es el n\'umero de eventos observados.

La funci\'on generadora de momentos (MGF) para la distribuci\'on de Poisson es:
\begin{eqnarray*}
M_X(t) = e^{\lambda (e^t - 1)}
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria de Poisson son:
\begin{eqnarray*}
E(X) &=& \lambda \\
\text{Var}(X) &=& \lambda
\end{eqnarray*}

\subsection{Distribuciones Continuas}

Una variable aleatoria continua toma un n\'umero infinito de valores en un intervalo continuo. Algunas distribuciones continuas comunes incluyen:

\subsubsection{Distribuci\'on Normal}

La distribuci\'on normal, tambi\'en conocida como distribuci\'on gaussiana, es una de las distribuciones m\'as importantes en estad\'istica. La funci\'on de densidad de probabilidad es:
\begin{eqnarray*}
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{eqnarray*}
donde $x$ es un valor de la variable aleatoria, $\mu$ es la media, y $\sigma$ es la desviaci\'on est\'andar.

La funci\'on generadora de momentos (MGF) para la distribuci\'on normal es:
\begin{eqnarray*}
M_X(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria normal son:
\begin{eqnarray*}
E(X) &=& \mu \\
\text{Var}(X) &=& \sigma^2
\end{eqnarray*}

\subsubsection{Distribuci\'on Exponencial}

La distribuci\'on exponencial describe el tiempo entre eventos en un proceso de Poisson. La funci\'on de densidad de probabilidad es:
\begin{eqnarray*}
f(x) = \lambda e^{-\lambda x}
\end{eqnarray*}
donde $x$ es el tiempo entre eventos y $\lambda$ es la tasa media de eventos.

La funci\'on generadora de momentos (MGF) para la distribuci\'on exponencial es:
\begin{eqnarray*}
M_X(t) = \frac{\lambda}{\lambda - t}, \quad \text{para } t < \lambda
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria exponencial son:
\begin{eqnarray*}
E(X) &=& \frac{1}{\lambda} \\
\text{Var}(X) &=& \frac{1}{\lambda^2}
\end{eqnarray*}

\section{Estad\'istica Descriptiva}

La estad\'istica descriptiva resume y describe las caracter\'isticas de un conjunto de datos. Incluye medidas de tendencia central, medidas de dispersi\'on y medidas de forma.

\subsection{Medidas de Tendencia Central}

Las medidas de tendencia central incluyen la media, la mediana y la moda.

\subsubsection{Media}

La media aritm\'etica es la suma de los valores dividida por el n\'umero de valores:
\begin{eqnarray*}
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
\end{eqnarray*}
donde $x_i$ son los valores de la muestra y $n$ es el tama\~no de la muestra.

\subsubsection{Mediana}

La mediana es el valor medio cuando los datos est\'an ordenados. Si el n\'umero de valores es impar, la mediana es el valor central. Si es par, es el promedio de los dos valores centrales.

\subsubsection{Moda}

La moda es el valor que ocurre con mayor frecuencia en un conjunto de datos.

\subsection{Medidas de Dispersi\'on}

Las medidas de dispersi\'on incluyen el rango, la varianza y la desviaci\'on est\'andar.

\subsubsection{Rango}

El rango es la diferencia entre el valor m\'aximo y el valor m\'inimo de los datos:
\begin{eqnarray*}
Rango = x_{\text{max}} - x_{\text{min}}
\end{eqnarray*}

\subsubsection{Varianza}

La varianza es la media de los cuadrados de las diferencias entre los valores y la media:
\begin{eqnarray*}
\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
\end{eqnarray*}

\subsubsection{Desviaci\'on Est\'andar}

La desviaci\'on est\'andar es la ra\'iz cuadrada de la varianza:
\begin{eqnarray*}
\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{eqnarray*}

\section{Inferencia Estad\'istica}

La inferencia estad\'istica es el proceso de sacar conclusiones sobre una poblaci\'on a partir de una muestra. Incluye la estimaci\'on de par\'ametros y la prueba de hip\'otesis.

\subsection{Estimaci\'on de Par\'ametros}

La estimaci\'on de par\'ametros implica el uso de datos muestrales para estimar los par\'ametros de una poblaci\'on.

\subsubsection{Estimador Puntual}

Un estimador puntual proporciona un \'unico valor como estimaci\'on de un par\'ametro de la poblaci\'on. Por ejemplo, la media muestral $\bar{x}$ es un estimador puntual de la media poblacional $\mu$. Otros ejemplos de estimadores puntuales son:

\begin{itemize}
    \item \textbf{Mediana muestral ($\tilde{x}$)}: Estimador de la mediana poblacional.
    \item \textbf{Varianza muestral ($s^2$)}: Estimador de la varianza poblacional $\sigma^2$, definido como:
    \begin{eqnarray*}
    s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
    \end{eqnarray*}
    \item \textbf{Desviaci\'on est\'andar muestral ($s$)}: Estimador de la desviaci\'on est\'andar poblacional $\sigma$, definido como:
    \begin{eqnarray*}
    s = \sqrt{s^2}
    \end{eqnarray*}
\end{itemize}

\subsubsection{Propiedades de los Estimadores Puntuales}

Los estimadores puntuales deben cumplir ciertas propiedades deseables, como:

\begin{itemize}
    \item \textbf{Insesgadez}: Un estimador es insesgado si su valor esperado es igual al valor del par\'ametro que estima.
    \begin{eqnarray*}
    E(\hat{\theta}) = \theta
    \end{eqnarray*}
    \item \textbf{Consistencia}: Un estimador es consistente si converge en probabilidad al valor del par\'ametro a medida que el tama\~no de la muestra tiende a infinito.
    \item \textbf{Eficiencia}: Un estimador es eficiente si tiene la varianza m\'as baja entre todos los estimadores insesgados.
\end{itemize}

\subsubsection{Estimador por Intervalo}

Un estimador por intervalo proporciona un rango de valores dentro del cual se espera que se encuentre el par\'ametro poblacional con un cierto nivel de confianza. Por ejemplo, un intervalo de confianza para la media es:
\begin{eqnarray*}
\left( \bar{x} - z \frac{\sigma}{\sqrt{n}}, \bar{x} + z \frac{\sigma}{\sqrt{n}} \right)
\end{eqnarray*}
donde $z$ es el valor cr\'itico correspondiente al nivel de confianza deseado, $\sigma$ es la desviaci\'on est\'andar poblacional y $n$ es el tama\~no de la muestra.

\subsection{Prueba de Hip\'otesis}

La prueba de hip\'otesis es un procedimiento para decidir si una afirmaci\'on sobre un par\'ametro poblacional es consistente con los datos muestrales.

\subsubsection{Hip\'otesis Nula y Alternativa}

La hip\'otesis nula ($H_0$) es la afirmaci\'on que se somete a prueba, y la hip\'otesis alternativa ($H_a$) es la afirmaci\'on que se acepta si se rechaza la hip\'otesis nula.

\subsubsection{Nivel de Significancia}

El nivel de significancia ($\alpha$) es la probabilidad de rechazar la hip\'otesis nula cuando es verdadera. Un valor com\'unmente utilizado es $\alpha = 0.05$.

\subsubsection{Estad\'istico de Prueba}

El estad\'istico de prueba es una medida calculada a partir de los datos muestrales que se utiliza para decidir si se rechaza la hip\'otesis nula. Por ejemplo, en una prueba $t$ para la media:
\begin{eqnarray*}
t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}
\end{eqnarray*}
donde $\bar{x}$ es la media muestral, $\mu_0$ es la media poblacional bajo la hip\'otesis nula, $s$ es la desviaci\'on est\'andar muestral y $n$ es el tama\~no de la muestra.

\subsubsection{P-valor}

El p-valor es la probabilidad de obtener un valor del estad\'istico de prueba al menos tan extremo como el observado, bajo la suposici\'on de que la hip\'otesis nula es verdadera. Si el p-valor es menor que el nivel de significancia $\alpha$, se rechaza la hip\'otesis nula. El p-valor se interpreta de la siguiente manera:

\begin{itemize}
    \item \textbf{P-valor bajo (p < 0.05)}: Evidencia suficiente para rechazar la hip\'otesis nula.
    \item \textbf{P-valor alto (p > 0.05)}: No hay suficiente evidencia para rechazar la hip\'otesis nula.
\end{itemize}

\subsubsection{Tipos de Errores}

En la prueba de hip\'otesis, se pueden cometer dos tipos de errores:

\begin{itemize}
    \item \textbf{Error Tipo I ($\alpha$)}: Rechazar la hip\'otesis nula cuando es verdadera.
    \item \textbf{Error Tipo II ($\beta$)}: No rechazar la hip\'otesis nula cuando es falsa.
\end{itemize}

\subsubsection{Tabla de Errores en la Prueba de Hip\'otesis}

A continuaci\'on se presenta una tabla que muestra los posibles resultados en una prueba de hip\'otesis, incluyendo los falsos positivos (error tipo I) y los falsos negativos (error tipo II):

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Hip\'otesis Nula Verdadera} & \textbf{Hip\'otesis Nula Falsa} \\
\hline
\textbf{Rechazar $H_0$} & Error Tipo I ($\alpha$) & Aceptar $H_a$ \\
\hline
\textbf{No Rechazar $H_0$} & Aceptar $H_0$ & Error Tipo II ($\beta$) \\
\hline
\end{tabular}
\caption{Resultados de la Prueba de Hip\'otesis}
\label{tab:hypothesis_testing}
\end{table}



\chapter{Matemáticas Detrás de la Regresión Logística}
\section{Introducci\'on}

La regresi\'on log\'istica es una t\'ecnica de modelado estad\'istico utilizada para predecir la probabilidad de un evento binario en funci\'on de una o m\'as variables independientes. Este cap\'itulo profundiza en las matem\'aticas subyacentes a la regresi\'on log\'istica, incluyendo la funci\'on log\'istica, la funci\'on de verosimilitud, y los m\'etodos para estimar los coeficientes del modelo.

\section{Funci\'on Log\'istica}

La funci\'on log\'istica es la base de la regresi\'on log\'istica. Esta funci\'on transforma una combinaci\'on lineal de variables independientes en una probabilidad.

\subsection{Definici\'on}

La funci\'on log\'istica se define como:
\begin{eqnarray*}
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}
\end{eqnarray*}
donde $p$ es la probabilidad de que el evento ocurra, $\beta_0, \beta_1, \ldots, \beta_n$ son los coeficientes del modelo, y $X_1, X_2, \ldots, X_n$ son las variables independientes.

\subsection{Propiedades}

La funci\'on log\'istica tiene varias propiedades importantes:
\begin{itemize}
    \item \textbf{Rango}: La funci\'on log\'istica siempre produce un valor entre 0 y 1, lo que la hace adecuada para modelar probabilidades.
    \item \textbf{Monoton\'ia}: La funci\'on es mon\'otona creciente, lo que significa que a medida que la combinaci\'on lineal de variables independientes aumenta, la probabilidad tambi\'en aumenta.
    \item \textbf{Simetr\'ia}: La funci\'on log\'istica es sim\'etrica en torno a $p = 0.5$.
\end{itemize}

\section{Funci\'on de Verosimilitud}

La funci\'on de verosimilitud se utiliza para estimar los coeficientes del modelo de regresi\'on log\'istica. Esta funci\'on mide la probabilidad de observar los datos dados los coeficientes del modelo.

\subsection{Definici\'on}

Para un conjunto de $n$ observaciones, la funci\'on de verosimilitud $L$ se define como el producto de las probabilidades individuales de observar cada dato:
\begin{eqnarray*}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{eqnarray*}
donde $y_i$ es el valor observado de la variable dependiente para la $i$-\'esima observaci\'on y $p_i$ es la probabilidad predicha de que $Y_i = 1$.

\subsection{Funci\'on de Log-Verosimilitud}

Para simplificar los c\'alculos, trabajamos con el logaritmo de la funci\'on de verosimilitud, conocido como la funci\'on de log-verosimilitud. Tomar el logaritmo convierte el producto en una suma:
\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{eqnarray*}

Sustituyendo $p_i$:
\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{eqnarray*}

\section{Estimaci\'on de Coeficientes}

Los coeficientes del modelo de regresi\'on log\'istica se estiman maximizando la funci\'on de log-verosimilitud. Este proceso generalmente se realiza mediante m\'etodos iterativos como el algoritmo de Newton-Raphson.

\subsection{Gradiente y Hessiana}

Para maximizar la funci\'on de log-verosimilitud, necesitamos calcular su gradiente y su matriz Hessiana.

\subsubsection{Gradiente}

El gradiente de la funci\'on de log-verosimilitud con respecto a los coeficientes $\beta$ es:
\begin{eqnarray*}
\mathbf{g}(\beta) = \frac{\partial \log L}{\partial \beta} = \sum_{i=1}^{n} \mathbf{X}_i (y_i - p_i)
\end{eqnarray*}
donde $\mathbf{X}_i$ es el vector de valores de las variables independientes para la $i$-\'esima observaci\'on.

\subsubsection{Hessiana}

La matriz Hessiana de la funci\'on de log-verosimilitud con respecto a los coeficientes $\beta$ es:
\begin{eqnarray*}
\mathbf{H}(\beta) = \frac{\partial^2 \log L}{\partial \beta \partial \beta^T} = -\sum_{i=1}^{n} p_i (1 - p_i) \mathbf{X}_i \mathbf{X}_i^T
\end{eqnarray*}

\subsection{Algoritmo Newton-Raphson}

El algoritmo Newton-Raphson se utiliza para encontrar los valores de los coeficientes que maximizan la funci\'on de log-verosimilitud. El algoritmo se puede resumir en los siguientes pasos:
\begin{enumerate}
    \item Inicializar el vector de coeficientes $\beta^{(0)}$ (por ejemplo, con ceros o valores peque\~nos aleatorios).
    \item Calcular el gradiente $\mathbf{g}(\beta^{(k)})$ y la matriz Hessiana $\mathbf{H}(\beta^{(k)})$ en la iteraci\'on $k$.
    \item Actualizar los coeficientes utilizando la f\'ormula:
    \begin{eqnarray*}
    \beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
    \end{eqnarray*}
    \item Repetir los pasos 2 y 3 hasta que la diferencia entre $\beta^{(k+1)}$ y $\beta^{(k)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}

\section{Validaci\'on del Modelo}

Una vez que se han estimado los coeficientes del modelo de regresi\'on log\'istica, es importante validar el modelo para asegurarse de que proporciona predicciones precisas.

\subsection{Curva ROC y AUC}

La curva ROC (Receiver Operating Characteristic) es una herramienta gr\'afica utilizada para evaluar el rendimiento de un modelo de clasificaci\'on binaria. El \'area bajo la curva (AUC) mide la capacidad del modelo para distinguir entre las clases.

\subsection{Matriz de Confusi\'on}

La matriz de confusi\'on es una tabla que resume el rendimiento de un modelo de clasificaci\'on al comparar las predicciones del modelo con los valores reales. Los t\'erminos en la matriz de confusi\'on incluyen verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos.



\chapter{Preparación de Datos y Selección de Variables}


\section{Introducci\'on}

La preparaci\'on de datos y la selecci\'on de variables son pasos cruciales en el proceso de modelado estad\'istico. Un modelo bien preparado y con las variables adecuadas puede mejorar significativamente la precisi\'on y la interpretabilidad del modelo. Este cap\'itulo proporciona una revisi\'on detallada de las t\'ecnicas de limpieza de datos, tratamiento de datos faltantes, codificaci\'on de variables categ\'oricas y selecci\'on de variables.

\section{Importancia de la Preparaci\'on de Datos}

La calidad de los datos es fundamental para el \'exito de cualquier an\'alisis estad\'istico. Los datos sin limpiar pueden llevar a modelos inexactos y conclusiones err\'oneas. La preparaci\'on de datos incluye varias etapas:
\begin{itemize}
    \item Limpieza de datos
    \item Tratamiento de datos faltantes
    \item Codificaci\'on de variables categ\'oricas
    \item Selecci\'on y transformaci\'on de variables
\end{itemize}

\section{Limpieza de Datos}

La limpieza de datos es el proceso de detectar y corregir (o eliminar) los datos incorrectos, incompletos o irrelevantes. Este proceso incluye:
\begin{itemize}
    \item Eliminaci\'on de duplicados
    \item Correcci\'on de errores tipogr\'aficos
    \item Consistencia de formato
    \item Tratamiento de valores extremos (outliers)
\end{itemize}

\section{Tratamiento de Datos Faltantes}

Los datos faltantes son un problema com\'un en los conjuntos de datos y pueden afectar la calidad de los modelos. Hay varias estrategias para manejar los datos faltantes:
\begin{itemize}
    \item \textbf{Eliminaci\'on de Datos Faltantes}: Se eliminan las filas o columnas con datos faltantes.
    \item \textbf{Imputaci\'on}: Se reemplazan los valores faltantes con estimaciones, como la media, la mediana o la moda.
    \item \textbf{Modelos Predictivos}: Se utilizan modelos predictivos para estimar los valores faltantes.
\end{itemize}

\subsection{Imputaci\'on de la Media}

Una t\'ecnica com\'un es reemplazar los valores faltantes con la media de la variable. Esto se puede hacer de la siguiente manera:
\begin{eqnarray*}
x_i = \begin{cases} 
      x_i & \text{si } x_i \text{ no es faltante} \\
      \bar{x} & \text{si } x_i \text{ es faltante}
   \end{cases}
\end{eqnarray*}
donde $\bar{x}$ es la media de la variable.

\section{Codificaci\'on de Variables Categ\'oricas}

Las variables categ\'oricas deben ser convertidas a un formato num\'erico antes de ser usadas en un modelo de regresi\'on log\'istica. Hay varias t\'ecnicas para codificar variables categ\'oricas:

\subsection{Codificaci\'on One-Hot}

La codificaci\'on one-hot crea una columna binaria para cada categor\'ia. Por ejemplo, si tenemos una variable categ\'orica con tres categor\'ias (A, B, C), se crean tres columnas:
\begin{eqnarray*}
\text{A} &=& [1, 0, 0] \\
\text{B} &=& [0, 1, 0] \\
\text{C} &=& [0, 0, 1]
\end{eqnarray*}

\subsection{Codificaci\'on Ordinal}

La codificaci\'on ordinal asigna un valor entero \'unico a cada categor\'ia, preservando el orden natural de las categor\'ias. Por ejemplo:
\begin{eqnarray*}
\text{Bajo} &=& 1 \\
\text{Medio} &=& 2 \\
\text{Alto} &=& 3
\end{eqnarray*}

\section{Selecci\'on de Variables}

La selecci\'on de variables es el proceso de elegir las variables m\'as relevantes para el modelo. Existen varias t\'ecnicas para la selecci\'on de variables:

\subsection{M\'etodos de Filtrado}

Los m\'etodos de filtrado seleccionan variables basadas en criterios estad\'isticos, como la correlaci\'on o la chi-cuadrado. Algunas t\'ecnicas comunes incluyen:
\begin{itemize}
    \item \textbf{An\'alisis de Correlaci\'on}: Se seleccionan variables con alta correlaci\'on con la variable dependiente y baja correlaci\'on entre ellas.
    \item \textbf{Pruebas de Chi-cuadrado}: Se utilizan para variables categ\'oricas para determinar la asociaci\'on entre la variable independiente y la variable dependiente.
\end{itemize}

\subsection{M\'etodos de Wrapper}

Los m\'etodos de wrapper eval\'uan m\'ultiples combinaciones de variables y seleccionan la combinaci\'on que optimiza el rendimiento del modelo. Ejemplos incluyen:
\begin{itemize}
    \item \textbf{Selecci\'on hacia Adelante}: Comienza con un modelo vac\'io y agrega variables una por una, seleccionando la variable que mejora m\'as el modelo en cada paso.
    \item \textbf{Selecci\'on hacia Atr\'as}: Comienza con todas las variables y elimina una por una, removiendo la variable que tiene el menor impacto en el modelo en cada paso.
    \item \textbf{Selecci\'on Paso a Paso}: Combina la selecci\'on hacia adelante y hacia atr\'as, agregando y eliminando variables seg\'un sea necesario.
\end{itemize}

\subsection{M\'etodos Basados en Modelos}

Los m\'etodos basados en modelos utilizan t\'ecnicas de regularizaci\'on como Lasso y Ridge para seleccionar variables. Estas t\'ecnicas a\~naden un t\'ermino de penalizaci\'on a la funci\'on de costo para evitar el sobreajuste.

\subsubsection{Regresi\'on Lasso}

La regresi\'on Lasso (Least Absolute Shrinkage and Selection Operator) a\~nade una penalizaci\'on $L_1$ a la funci\'on de costo:
\begin{eqnarray*}
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
\end{eqnarray*}
donde $\lambda$ es el par\'ametro de regularizaci\'on que controla la cantidad de penalizaci\'on.

\subsubsection{Regresi\'on Ridge}

La regresi\'on Ridge a\~nade una penalizaci\'on $L_2$ a la funci\'on de costo:
\begin{eqnarray*}
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
\end{eqnarray*}
donde $\lambda$ es el par\'ametro de regularizaci\'on.

\section{Implementaci\'on en R}

\subsection{Limpieza de Datos}

Para ilustrar la limpieza de datos en R, considere el siguiente conjunto de datos:
\begin{verbatim}
data <- data.frame(
  var1 = c(1, 2, 3, NA, 5),
  var2 = c("A", "B", "A", "B", "A"),
  var3 = c(10, 15, 10, 20, 25)
)

# Eliminaci\'on de filas con datos faltantes
data_clean <- na.omit(data)

# Imputaci\'on de la media
data$var1[is.na(data$var1)] <- mean(data$var1, na.rm = TRUE)
\end{verbatim}

\subsection{Codificaci\'on de Variables Categ\'oricas}

Para codificar variables categ\'oricas, utilice la funci\'on `model.matrix`:
\begin{verbatim}
data <- data.frame(
  var1 = c(1, 2, 3, 4, 5),
  var2 = c("A", "B", "A", "B", "A")
)

# Codificaci\'on one-hot
data_onehot <- model.matrix(~ var2 - 1, data = data)
\end{verbatim}

\subsection{Selecci\'on de Variables}

Para la selecci\'on de variables, utilice el paquete `caret`:
\begin{verbatim}
library(caret)

# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)
trainIndex <- createDataPartition(data$var1, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dataTrain <- data[trainIndex,]
dataTest <- data[-trainIndex,]

# Modelo de regresi\'on log\'istica
model <- train(var1 ~ ., data = dataTrain, method = "glm", family = "binomial")

# Selecci\'on de variables
model <- step(model, direction = "both")
summary(model)
\end{verbatim}



\chapter{Evaluación del Modelo y Validación Cruzada}


\section{Introducción}

Evaluar la calidad y el rendimiento de un modelo de regresión logística es crucial para asegurar que las predicciones sean precisas y útiles. Este capítulo se centra en las técnicas y métricas utilizadas para evaluar modelos de clasificación binaria, así como en la validación cruzada, una técnica para evaluar la generalización del modelo.

\section{Métricas de Evaluación del Modelo}

Las métricas de evaluación permiten cuantificar la precisión y el rendimiento de un modelo. Algunas de las métricas más comunes incluyen:

\subsection{Curva ROC y AUC}

La curva ROC (Receiver Operating Characteristic) es una representación gráfica de la sensibilidad (verdaderos positivos) frente a 1 - especificidad (falsos positivos). El área bajo la curva (AUC) mide la capacidad del modelo para distinguir entre las clases.

\begin{eqnarray*}
\text{Sensibilidad} &=& \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\text{Especificidad} &=& \frac{\text{TN}}{\text{TN} + \text{FP}}
\end{eqnarray*}

\subsection{Matriz de Confusión}

La matriz de confusión es una tabla que muestra el rendimiento del modelo comparando las predicciones con los valores reales. Los términos incluyen:
\begin{itemize}
    \item \textbf{Verdaderos Positivos (TP)}: Predicciones correctas de la clase positiva.
    \item \textbf{Falsos Positivos (FP)}: Predicciones incorrectas de la clase positiva.
    \item \textbf{Verdaderos Negativos (TN)}: Predicciones correctas de la clase negativa.
    \item \textbf{Falsos Negativos (FN)}: Predicciones incorrectas de la clase negativa.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Predicción Positiva} & \textbf{Predicción Negativa} \\
\hline
\textbf{Real Positiva} & TP & FN \\
\hline
\textbf{Real Negativa} & FP & TN \\
\hline
\end{tabular}
\caption{Matriz de Confusión}
\label{tab:confusion_matrix}
\end{table}

\subsection{Precisión, Recall y F1-Score}

\begin{eqnarray*}
\text{Precisión} &=& \frac{\text{TP}}{\text{TP} + \text{FP}} \\
\text{Recall} &=& \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\text{F1-Score} &=& 2 \cdot \frac{\text{Precisión} \cdot \text{Recall}}{\text{Precisión} + \text{Recall}}
\end{eqnarray*}

\subsection{Log-Loss}

La pérdida logarítmica (Log-Loss) mide la precisión de las probabilidades predichas. La fórmula es:
\begin{eqnarray*}
\text{Log-Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{eqnarray*}
donde $y_i$ son los valores reales y $p_i$ son las probabilidades predichas.

\section{Validación Cruzada}

La validación cruzada es una técnica para evaluar la capacidad de generalización de un modelo. Existen varios tipos de validación cruzada:

\subsection{K-Fold Cross-Validation}

En K-Fold Cross-Validation, los datos se dividen en K subconjuntos. El modelo se entrena K veces, cada vez utilizando K-1 subconjuntos para el entrenamiento y el subconjunto restante para la validación.

\begin{eqnarray*}
\text{Error Medio} = \frac{1}{K} \sum_{k=1}^{K} \text{Error}_k
\end{eqnarray*}

\subsection{Leave-One-Out Cross-Validation (LOOCV)}

En LOOCV, cada observación se usa una vez como conjunto de validación y las restantes como conjunto de entrenamiento. Este método es computacionalmente costoso pero útil para conjuntos de datos pequeños.

\section{Ajuste y Sobreajuste del Modelo}

El ajuste adecuado del modelo es crucial para evitar el sobreajuste (overfitting) y el subajuste (underfitting).

\subsection{Sobreajuste}

El sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento, capturando ruido y patrones irrelevantes. Los síntomas incluyen una alta precisión en el entrenamiento y baja precisión en la validación.

\subsection{Subajuste}

El subajuste ocurre cuando un modelo no captura los patrones subyacentes de los datos. Los síntomas incluyen baja precisión tanto en el entrenamiento como en la validación.

\subsection{Regularización}

La regularización es una técnica para prevenir el sobreajuste añadiendo un término de penalización a la función de costo. Las técnicas comunes incluyen:
\begin{itemize}
    \item \textbf{Regresión Lasso (L1)}
    \item \textbf{Regresión Ridge (L2)}
\end{itemize}

\section{Implementación en R}

\subsection{Evaluación del Modelo}

\begin{verbatim}
# Cargar el paquete necesario
library(caret)

# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)
trainIndex <- createDataPartition(data$var1, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dataTrain <- data[trainIndex,]
dataTest <- data[-trainIndex,]

# Entrenar el modelo de regresión logística
model <- train(var1 ~ ., data = dataTrain, method = "glm", family = "binomial")

# Predicciones en el conjunto de prueba
predictions <- predict(model, dataTest)

# Matriz de confusión
confusionMatrix(predictions, dataTest$var1)
\end{verbatim}

\subsection{Validación Cruzada}

\begin{verbatim}
# K-Fold Cross-Validation
control <- trainControl(method = "cv", number = 10)
model_cv <- train(var1 ~ ., data = dataTrain, method = "glm", 
                  family = "binomial", trControl = control)

# Evaluación del modelo con validación cruzada
print(model_cv)
\end{verbatim}



\chapter{Diagnóstico del Modelo y Ajuste de Parámetros}


\section{Introducci\'on}

El diagn\'ostico del modelo y el ajuste de par\'ametros son pasos esenciales para mejorar la precisi\'on y la robustez de los modelos de regresi\'on log\'istica. Este cap\'itulo se enfoca en las t\'ecnicas para diagnosticar problemas en los modelos y en m\'etodos para ajustar los par\'ametros de manera \'optima.

\section{Diagn\'ostico del Modelo}

El diagn\'ostico del modelo implica evaluar el rendimiento del modelo y detectar posibles problemas, como el sobreajuste, la multicolinealidad y la influencia de puntos de datos individuales.

\subsection{Residuos}

Los residuos son las diferencias entre los valores observados y los valores predichos por el modelo. El an\'alisis de residuos puede revelar patrones que indican problemas con el modelo.

\begin{eqnarray*}
\text{Residuo}_i = y_i - \hat{y}_i
\end{eqnarray*}

\subsubsection{Residuos Estudiantizados}

Los residuos estudiantizados se ajustan por la variabilidad del residuo y se utilizan para detectar outliers.

\begin{eqnarray*}
r_i = \frac{\text{Residuo}_i}{\hat{\sigma} \sqrt{1 - h_i}}
\end{eqnarray*}
donde $h_i$ es el leverage del punto de datos.

\subsection{Influencia}

La influencia mide el impacto de un punto de datos en los coeficientes del modelo. Los puntos con alta influencia pueden distorsionar el modelo.

\subsubsection{Distancia de Cook}

La distancia de Cook es una medida de la influencia de un punto de datos en los coeficientes del modelo.

\begin{eqnarray*}
D_i = \frac{r_i^2}{p} \cdot \frac{h_i}{1 - h_i}
\end{eqnarray*}
donde $p$ es el n\'umero de par\'ametros en el modelo.

\subsection{Multicolinealidad}

La multicolinealidad ocurre cuando dos o m\'as variables independientes est\'an altamente correlacionadas. Esto puede inflar las varianzas de los coeficientes y hacer que el modelo sea inestable.

\subsubsection{Factor de Inflaci\'on de la Varianza (VIF)}

El VIF mide cu\'anto se inflan las varianzas de los coeficientes debido a la multicolinealidad.

\begin{eqnarray*}
\text{VIF}_j = \frac{1}{1 - R_j^2}
\end{eqnarray*}
donde $R_j^2$ es el coeficiente de determinaci\'on de la regresi\'on de la variable $j$ contra todas las dem\'as variables.

\section{Ajuste de Par\'ametros}

El ajuste de par\'ametros implica seleccionar los valores \'optimos para los hiperpar\'ametros del modelo. Esto puede mejorar el rendimiento y prevenir el sobreajuste.

\subsection{Grid Search}

El grid search es un m\'etodo exhaustivo para ajustar los par\'ametros. Se define una rejilla de posibles valores de par\'ametros y se eval\'ua el rendimiento del modelo para cada combinaci\'on.

\subsection{Random Search}

El random search selecciona aleatoriamente combinaciones de valores de par\'ametros dentro de un rango especificado. Es menos exhaustivo que el grid search, pero puede ser m\'as eficiente.

\subsection{Bayesian Optimization}

La optimizaci\'on bayesiana utiliza modelos probabil\'isticos para seleccionar iterativamente los valores de par\'ametros m\'as prometedores.

\section{Implementaci\'on en R}

\subsection{Diagn\'ostico del Modelo}

\begin{verbatim}
# Cargar el paquete necesario
library(car)

# Residuos estudentizados
dataTrain$resid <- rstudent(model)
hist(dataTrain$resid, breaks = 20, main = "Residuos Estudentizados")

# Distancia de Cook
dataTrain$cook <- cooks.distance(model)
plot(dataTrain$cook, type = "h", main = "Distancia de Cook")

# Factor de Inflaci\'on de la Varianza
vif_values <- vif(model)
print(vif_values)
\end{verbatim}

\subsection{Ajuste de Par\'ametros}

\begin{verbatim}
# Grid Search con caret
control <- trainControl(method = "cv", number = 10)
tune_grid <- expand.grid(.alpha = c(0, 0.5, 1), .lambda = seq(0.01, 0.1, by = 0.01))

model_tune <- train(var1 ~ ., data = dataTrain, method = "glmnet", 
                    trControl = control, tuneGrid = tune_grid)

print(model_tune)
\end{verbatim}



\chapter{Interpretación de los Resultados}

\section{Introducci\'on}

Interpretar correctamente los resultados de un modelo de regresi\'on log\'istica es esencial para tomar decisiones informadas. Este cap\'itulo se centra en la interpretaci\'on de los coeficientes del modelo, las odds ratios, los intervalos de confianza y la significancia estad\'istica.

\section{Coeficientes de Regresi\'on Log\'istica}

Los coeficientes de regresi\'on log\'istica representan la relaci\'on entre las variables independientes y la variable dependiente en t\'erminos de log-odds. 

\subsection{Interpretaci\'on de los Coeficientes}

Cada coeficiente $\beta_j$ en el modelo de regresi\'on log\'istica se interpreta como el cambio en el log-odds de la variable dependiente por unidad de cambio en la variable independiente $X_j$.

\begin{eqnarray*}
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{eqnarray*}

\subsection{Signo de los Coeficientes}

\begin{itemize}
    \item \textbf{Coeficiente Positivo}: Un coeficiente positivo indica que un aumento en la variable independiente est\'a asociado con un aumento en el log-odds de la variable dependiente.
    \item \textbf{Coeficiente Negativo}: Un coeficiente negativo indica que un aumento en la variable independiente est\'a asociado con una disminuci\'on en el log-odds de la variable dependiente.
\end{itemize}

\section{Odds Ratios}

Las odds ratios proporcionan una interpretaci\'on m\'as intuitiva de los coeficientes de regresi\'on log\'istica. La odds ratio para una variable independiente $X_j$ se calcula como $e^{\beta_j}$.

\subsection{C\'alculo de las Odds Ratios}

\begin{eqnarray*}
\text{OR}_j = e^{\beta_j}
\end{eqnarray*}

\subsection{Interpretaci\'on de las Odds Ratios}

\begin{itemize}
    \item \textbf{OR > 1}: Un OR mayor que 1 indica que un aumento en la variable independiente est\'a asociado con un aumento en las odds de la variable dependiente.
    \item \textbf{OR < 1}: Un OR menor que 1 indica que un aumento en la variable independiente est\'a asociado con una disminuci\'on en las odds de la variable dependiente.
    \item \textbf{OR = 1}: Un OR igual a 1 indica que la variable independiente no tiene efecto sobre las odds de la variable dependiente.
\end{itemize}

\section{Intervalos de Confianza}

Los intervalos de confianza proporcionan una medida de la incertidumbre asociada con los estimadores de los coeficientes. Un intervalo de confianza del 95\% para un coeficiente $\beta_j$ indica que, en el 95\% de las muestras, el intervalo contendr\'a el valor verdadero de $\beta_j$.

\subsection{C\'alculo de los Intervalos de Confianza}

Para calcular un intervalo de confianza del 95\% para un coeficiente $\beta_j$, utilizamos la f\'ormula:
\begin{eqnarray*}
\beta_j \pm 1.96 \cdot \text{SE}(\beta_j)
\end{eqnarray*}
donde $\text{SE}(\beta_j)$ es el error est\'andar de $\beta_j$.

\section{Significancia Estad\'istica}

La significancia estad\'istica se utiliza para determinar si los coeficientes del modelo son significativamente diferentes de cero. Esto se eval\'ua mediante pruebas de hip\'otesis.

\subsection{Prueba de Hip\'otesis}

Para cada coeficiente $\beta_j$, la hip\'otesis nula $H_0$ es que $\beta_j = 0$. La hip\'otesis alternativa $H_a$ es que $\beta_j \neq 0$.

\subsection{P-valor}

El p-valor indica la probabilidad de obtener un coeficiente tan extremo como el observado, asumiendo que la hip\'otesis nula es verdadera. Un p-valor menor que el nivel de significancia $\alpha$ (t\'ipicamente 0.05) indica que podemos rechazar la hip\'otesis nula.

\section{Implementaci\'on en R}

\subsection{C\'alculo de Coeficientes y Odds Ratios}

\begin{verbatim}
# Cargar el paquete necesario
library(broom)

# Entrenar el modelo de regresi\'on log\'istica
model <- glm(var1 ~ ., data = dataTrain, family = "binomial")

# Coeficientes del modelo
coef(model)

# Odds ratios
exp(coef(model))
\end{verbatim}

\subsection{Intervalos de Confianza}

\begin{verbatim}
# Intervalos de confianza para los coeficientes
confint(model)

# Intervalos de confianza para las odds ratios
exp(confint(model))
\end{verbatim}

\subsection{P-valores y Significancia Estad\'istica}

\begin{verbatim}
# Resumen del modelo con p-valores
summary(model)
\end{verbatim}



\chapter{Regresión Logística Multinomial y Análisis de Supervivencia}

\section{Introducci\'on}

La regresi\'on log\'istica multinomial y el an\'alisis de supervivencia son extensiones de la regresi\'on log\'istica binaria. Este cap\'itulo se enfoca en las t\'ecnicas y aplicaciones de estos m\'etodos avanzados.

\section{Regresi\'on Log\'istica Multinomial}

La regresi\'on log\'istica multinomial se utiliza cuando la variable dependiente tiene m\'as de dos categor\'ias.

\subsection{Modelo Multinomial}

El modelo de regresi\'on log\'istica multinomial generaliza el modelo binario para manejar m\'ultiples categor\'ias. La probabilidad de que una observaci\'on pertenezca a la categor\'ia $k$ se expresa como:

\begin{eqnarray*}
P(Y = k) = \frac{e^{\beta_{0k} + \beta_{1k} X_1 + \ldots + \beta_{nk} X_n}}{\sum_{j=1}^{K} e^{\beta_{0j} + \beta_{1j} X_1 + \ldots + \beta_{nj} X_n}}
\end{eqnarray*}

\subsection{Estimaci\'on de Par\'ametros}

Los coeficientes del modelo multinomial se estiman utilizando m\'axima verosimilitud, similar a la regresi\'on log\'istica binaria.

\section{An\'alisis de Supervivencia}

El an\'alisis de supervivencia se utiliza para modelar el tiempo hasta que ocurre un evento de inter\'es, como la muerte o la falla de un componente.

\subsection{Funci\'on de Supervivencia}

La funci\'on de supervivencia $S(t)$ describe la probabilidad de que una observaci\'on sobreviva m\'as all\'a del tiempo $t$:

\begin{eqnarray*}
S(t) = P(T > t)
\end{eqnarray*}

\subsection{Modelo de Riesgos Proporcionales de Cox}

El modelo de Cox es un modelo de regresi\'on semiparam\'etrico utilizado para analizar datos de supervivencia:

\begin{eqnarray*}
h(t|X) = h_0(t) e^{\beta_1 X_1 + \ldots + \beta_p X_p}
\end{eqnarray*}
donde $h(t|X)$ es la tasa de riesgo en el tiempo $t$ dado el vector de covariables $X$ y $h_0(t)$ es la tasa de riesgo basal.

\section{Implementaci\'on en R}

\subsection{Regresi\'on Log\'istica Multinomial}

\begin{verbatim}
# Cargar el paquete necesario
library(nnet)

# Entrenar el modelo de regresi\'on log\'istica multinomial
model_multinom <- multinom(var1 ~ ., data = dataTrain)

# Resumen del modelo
summary(model_multinom)
\end{verbatim}

\subsection{An\'alisis de Supervivencia}

\begin{verbatim}
# Cargar el paquete necesario
library(survival)

# Crear el objeto de supervivencia
surv_object <- Surv(time = data$time, event = data$status)

# Ajustar el modelo de Cox
model_cox <- coxph(surv_object ~ var1 + var2, data = data)

# Resumen del modelo
summary(model_cox)
\end{verbatim}



\chapter{Implementación de Regresión Logística en Datos Reales}
\section{Introducci\'on}

Implementar un modelo de regresi\'on log\'istica en datos reales implica varias etapas, desde la limpieza de datos hasta la evaluaci\'on y validaci\'on del modelo. Este cap\'itulo presenta un ejemplo pr\'actico de la implementaci\'on de un modelo de regresi\'on log\'istica utilizando un conjunto de datos real.

\section{Conjunto de Datos}

Para este ejemplo, utilizaremos un conjunto de datos disponible p\'ublicamente que contiene informaci\'on sobre clientes bancarios. El objetivo es predecir si un cliente suscribir\'a un dep\'osito a plazo fijo.

\section{Preparaci\'on de Datos}

\subsection{Carga y Exploraci\'on de Datos}

Primero, cargamos y exploramos el conjunto de datos para entender su estructura y contenido.

\begin{verbatim}
# Cargar el paquete necesario
library(dplyr)

# Cargar el conjunto de datos
data <- read.csv("bank.csv")

# Explorar los datos
str(data)
summary(data)
\end{verbatim}

\subsection{Limpieza de Datos}

El siguiente paso es limpiar los datos, lo que incluye tratar los valores faltantes y eliminar las duplicidades.

\begin{verbatim}
# Eliminar duplicados
data <- data %>% distinct()

# Imputar valores faltantes (si existen)
data <- data %>% mutate_if(is.numeric, ~ifelse(is.na(.), mean(., na.rm = TRUE), .))
\end{verbatim}

\subsection{Codificaci\'on de Variables Categ\'oricas}

Convertimos las variables categ\'oricas en variables num\'ericas utilizando la codificaci\'on one-hot.

\begin{verbatim}
# Codificaci\'on one-hot de variables categ\'oricas
data <- data %>% mutate(across(where(is.factor), ~ as.numeric(as.factor(.))))
\end{verbatim}

\section{Divisi\'on de Datos}

Dividimos los datos en conjuntos de entrenamiento y prueba.

\begin{verbatim}
# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)
trainIndex <- createDataPartition(data$y, p = .8, list = FALSE, times = 1)
dataTrain <- data[trainIndex,]
dataTest <- data[-trainIndex,]
\end{verbatim}

\section{Entrenamiento del Modelo}

Entrenamos un modelo de regresi\'on log\'istica utilizando el conjunto de entrenamiento.

\begin{verbatim}
# Entrenar el modelo de regresi\'on log\'istica
model <- glm(y ~ ., data = dataTrain, family = "binomial")

# Resumen del modelo
summary(model)
\end{verbatim}

\section{Evaluaci\'on del Modelo}

Evaluamos el rendimiento del modelo utilizando el conjunto de prueba.

\begin{verbatim}
# Predicciones en el conjunto de prueba
predictions <- predict(model, dataTest, type = "response")

# Convertir probabilidades a etiquetas
predicted_labels <- ifelse(predictions > 0.5, 1, 0)

# Matriz de confusi\'on
confusionMatrix(predicted_labels, dataTest$y)
\end{verbatim}

\section{Interpretaci\'on de los Resultados}

Interpretamos los coeficientes del modelo y las odds ratios.

\begin{verbatim}
# Coeficientes del modelo
coef(model)

# Odds ratios
exp(coef(model))
\end{verbatim}



\chapter{Resumen y Proyecto Final}
\section{Resumen de Conceptos Clave}

En este curso, hemos cubierto una variedad de conceptos y t\'ecnicas esenciales para la regresi\'on log\'istica. Los conceptos clave incluyen:

\begin{itemize}
    \item \textbf{Fundamentos de Probabilidad y Estad\'istica}: Comprensi\'on de distribuciones de probabilidad, medidas de tendencia central y dispersi\'on, inferencia estad\'istica y pruebas de hip\'otesis.
    \item \textbf{Regresi\'on Log\'istica}: Modelo de regresi\'on log\'istica binaria y multinomial, interpretaci\'on de coeficientes y odds ratios, m\'etodos de estimaci\'on y validaci\'on.
    \item \textbf{Preparaci\'on de Datos}: Limpieza de datos, tratamiento de valores faltantes, codificaci\'on de variables categ\'oricas y selecci\'on de variables.
    \item \textbf{Evaluaci\'on del Modelo}: Curva ROC, AUC, matriz de confusi\'on, precisi\'on, recall, F1-score y validaci\'on cruzada.
    \item \textbf{Diagn\'ostico del Modelo}: An\'alisis de residuos, influencia, multicolinealidad y ajuste de par\'ametros.
    \item \textbf{An\'alisis de Supervivencia}: Modelos de supervivencia, funci\'on de supervivencia y modelos de riesgos proporcionales de Cox.
\end{itemize}

\section{Buenas Pr\'acticas}

Al implementar modelos de regresi\'on log\'istica, es importante seguir buenas pr\'acticas para garantizar la precisi\'on y la robustez de los modelos. Algunas buenas pr\'acticas incluyen:

\begin{itemize}
    \item \textbf{Exploraci\'on y Preparaci\'on de Datos}: Realizar un an\'alisis exploratorio exhaustivo y preparar los datos adecuadamente antes de construir el modelo.
    \item \textbf{Evaluaci\'on y Validaci\'on del Modelo}: Utilizar m\'etricas adecuadas para evaluar el rendimiento del modelo y validar el modelo utilizando t\'ecnicas como la validaci\'on cruzada.
    \item \textbf{Interpretaci\'on de Resultados}: Interpretar correctamente los coeficientes del modelo y las odds ratios, y comunicar los resultados de manera clara y concisa.
    \item \textbf{Revisi\'on y Ajuste del Modelo}: Diagnosticar problemas en el modelo y ajustar los par\'ametros para mejorar el rendimiento.
\end{itemize}

\section{Proyecto Final}

Para aplicar los conceptos y t\'ecnicas aprendidos en este curso, te proponemos realizar un proyecto final utilizando un conjunto de datos de tu elecci\'on. El proyecto debe incluir las siguientes etapas:

\subsection{Selecci\'on del Conjunto de Datos}

Elige un conjunto de datos relevante que contenga una variable dependiente binaria o multinomial y varias variables independientes.

\subsection{Exploraci\'on y Preparaci\'on de Datos}

Realiza un an\'alisis exploratorio de los datos y prepara los datos para el modelado. Esto incluye la limpieza de datos, el tratamiento de valores faltantes y la codificaci\'on de variables categ\'oricas.

\subsection{Entrenamiento y Evaluaci\'on del Modelo}

Entrena un modelo de regresi\'on log\'istica utilizando el conjunto de datos preparado y eval\'ua su rendimiento utilizando m\'etricas apropiadas.

\subsection{Interpretaci\'on de Resultados}

Interpreta los coeficientes del modelo y las odds ratios, y proporciona una explicaci\'on clara de los resultados.

\subsection{Presentaci\'on del Proyecto}

Presenta tu proyecto en un informe detallado que incluya la descripci\'on del conjunto de datos, los pasos de preparaci\'on y modelado, los resultados del modelo y las conclusiones.



%==<>====<>====<>====<>====<>====<>====<>====<>====<>====<>====
\part{SEGUNDA PARTE: ANALISIS DE SUPERVIVENCIA}
%==<>====<>====<>====<>====<>====<>====<>====<>====<>====<>====

\chapter{Introducción al Análisis de Supervivencia}

\section{Conceptos Básicos}
El análisis de supervivencia es una rama de la estad\'istica que se ocupa del análisis del tiempo que transcurre hasta que ocurre un evento de inter\'es, com\'unmente referido como "tiempo de falla". Este campo es ampliamente utilizado en medicina, biolog\'ia, ingenier\'ia, ciencias sociales, y otros campos.

\section{Definici\'on de Eventos y Tiempos}
En el análisis de supervivencia, un "evento" se refiere a la ocurrencia de un evento espec\'ifico, como la muerte, la falla de un componente, la reca\'ida de una enfermedad, etc. El "tiempo de supervivencia" es el tiempo que transcurre desde un punto de inicio definido hasta la ocurrencia del evento.

\section{Censura}
La censura ocurre cuando la informaci\'on completa sobre el tiempo hasta el evento no está disponible para todos los individuos en el estudio. Hay tres tipos principales de censura:
\begin{itemize}
    \item \textbf{Censura a la derecha:} Ocurre cuando el evento de inter\'es no se ha observado para algunos sujetos antes del final del estudio.
    \item \textbf{Censura a la izquierda:} Ocurre cuando el evento de inter\'es ocurri\'o antes del inicio del periodo de observaci\'on.
    \item \textbf{Censura por intervalo:} Ocurre cuando el evento de inter\'es se sabe que ocurri\'o en un intervalo de tiempo, pero no se conoce el momento exacto.
\end{itemize}

\section{Funci\'on de Supervivencia}
La funci\'on de supervivencia, $S(t)$, se define como la probabilidad de que un individuo sobreviva más allá de un tiempo $t$. Matemáticamente, se expresa como:
\begin{eqnarray*}
S(t) = P(T > t)
\end{eqnarray*}
donde $T$ es una variable aleatoria que representa el tiempo hasta el evento. La funci\'on de supervivencia tiene las siguientes propiedades:
\begin{itemize}
    \item $S(0) = 1$: Esto indica que al inicio (tiempo $t=0$), la probabilidad de haber experimentado el evento es cero, por lo tanto, la supervivencia es del 100%.
    \item $\lim_{t \to \infty} S(t) = 0$: A medida que el tiempo tiende al infinito, la probabilidad de que cualquier individuo a\'un no haya experimentado el evento tiende a cero.
    \item $S(t)$ es una funci\'on no creciente: Esto significa que a medida que el tiempo avanza, la probabilidad de supervivencia no aumenta.
\end{itemize}

\section{Funci\'on de Densidad de Probabilidad}
La funci\'on de densidad de probabilidad $f(t)$ describe la probabilidad de que el evento ocurra en un instante de tiempo espec\'ifico. Se define como:
\begin{eqnarray*}
f(t) = \frac{dF(t)}{dt}
\end{eqnarray*}
donde $F(t)$ es la funci\'on de distribuci\'on acumulada, $F(t) = P(T \leq t)$. La relaci\'on entre $S(t)$ y $f(t)$ es:
\begin{eqnarray*}
f(t) = -\frac{dS(t)}{dt}
\end{eqnarray*}

\section{Funci\'on de Riesgo}
La funci\'on de riesgo, $\lambda(t)$, tambi\'en conocida como funci\'on de tasa de fallas o hazard rate, se define como la tasa instant\'anea de ocurrencia del evento en el tiempo $t$, dado que el individuo ha sobrevivido hasta el tiempo $t$. Matem\'aticamente, se expresa como:
\begin{eqnarray*}
\lambda(t) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t}
\end{eqnarray*}
Esto se puede reescribir usando $f(t)$ y $S(t)$ como:
\begin{eqnarray*}
\lambda(t) = \frac{f(t)}{S(t)}
\end{eqnarray*}

\section{Relaci\'on entre Funci\'on de Supervivencia y Funci\'on de Riesgo}
La funci\'on de supervivencia y la funci\'on de riesgo est\'an relacionadas a trav\'es de la siguiente ecuaci\'on:
\begin{eqnarray*}
S(t) = \exp\left(-\int_0^t \lambda(u) \, du\right)
\end{eqnarray*}
Esta f\'ormula se deriva del hecho de que la funci\'on de supervivencia es la probabilidad acumulativa de no haber experimentado el evento hasta el tiempo $t$, y $\lambda(t)$ es la tasa instant\'anea de ocurrencia del evento.

La funci\'on de riesgo tambi\'en puede ser expresada como:
\begin{eqnarray*}
\lambda(t) = -\frac{d}{dt} \log S(t)
\end{eqnarray*}

\section{Deducci\'on de la Funci\'on de Supervivencia}
La relaci\'on entre la funci\'on de supervivencia y la funci\'on de riesgo se puede deducir integrando la funci\'on de riesgo:
\begin{eqnarray*}
S(t) &=& \exp\left(-\int_0^t \lambda(u) \, du\right) \\
\log S(t) &=& -\int_0^t \lambda(u) \, du \\
\frac{d}{dt} \log S(t) &=& -\lambda(t) \\
\lambda(t) &=& -\frac{d}{dt} \log S(t)
\end{eqnarray*}

\section{Ejemplo de C\'alculo}
Supongamos que tenemos una muestra de tiempos de supervivencia $T_1, T_2, \ldots, T_n$. Podemos estimar la funci\'on de supervivencia emp\'irica como:
\begin{eqnarray*}
\hat{S}(t) = \frac{\text{N\'umero de individuos que sobreviven m\'as all\'a de } t}{\text{N\'umero total de individuos en riesgo en } t}
\end{eqnarray*}
y la funci\'on de riesgo emp\'irica como:
\begin{eqnarray*}
\hat{\lambda}(t) = \frac{\text{N\'umero de eventos en } t}{\text{N\'umero de individuos en riesgo en } t}
\end{eqnarray*}

\section{Conclusi\'on}
El an\'alisis de supervivencia es una herramienta poderosa para analizar datos de tiempo hasta evento. Entender los conceptos b\'asicos como la funci\'on de supervivencia y la funci\'on de riesgo es fundamental para el an\'alisis m\'as avanzado.


\chapter{Función de Supervivencia y Función de Riesgo}
\section{Introducci\'on}
Este cap\'itulo profundiza en la definici\'on y propiedades de la funci\'on de supervivencia y la funci\'on de riesgo, dos conceptos fundamentales en el análisis de supervivencia. Entender estas funciones y su relaci\'on es crucial para modelar y analizar datos de tiempo hasta evento.

\section{Funci\'on de Supervivencia}
La funci\'on de supervivencia, $S(t)$, describe la probabilidad de que un individuo sobreviva más allá de un tiempo $t$. Formalmente, se define como:
\begin{eqnarray*}
S(t) = P(T > t)
\end{eqnarray*}
donde $T$ es una variable aleatoria que representa el tiempo hasta el evento.

\subsection{Propiedades de la Funci\'on de Supervivencia}
La funci\'on de supervivencia tiene varias propiedades importantes:
\begin{itemize}
    \item $S(0) = 1$: Indica que la probabilidad de haber experimentado el evento en el tiempo 0 es cero.
    \item $\lim_{t \to \infty} S(t) = 0$: A medida que el tiempo tiende al infinito, la probabilidad de supervivencia tiende a cero.
    \item $S(t)$ es una funci\'on no creciente: A medida que el tiempo avanza, la probabilidad de supervivencia no aumenta.
\end{itemize}

\subsection{Derivaci\'on de $S(t)$}
Si la funci\'on de densidad de probabilidad $f(t)$ del tiempo de supervivencia $T$ es conocida, la funci\'on de supervivencia puede derivarse como:
\begin{eqnarray*}
S(t) &=& P(T > t) \\
     &=& 1 - P(T \leq t) \\
     &=& 1 - F(t) \\
     &=& 1 - \int_0^t f(u) \, du
\end{eqnarray*}
donde $F(t)$ es la funci\'on de distribuci\'on acumulada.

\subsection{Ejemplo de Cálculo de $S(t)$}
Consideremos un ejemplo donde el tiempo de supervivencia $T$ sigue una distribuci\'on exponencial con tasa $\lambda$. La funci\'on de densidad de probabilidad $f(t)$ es:
\begin{eqnarray*}
f(t) = \lambda e^{-\lambda t}, \quad t \geq 0
\end{eqnarray*}
La funci\'on de distribuci\'on acumulada $F(t)$ es:
\begin{eqnarray*}
F(t) = \int_0^t \lambda e^{-\lambda u} \, du = 1 - e^{-\lambda t}
\end{eqnarray*}
Por lo tanto, la funci\'on de supervivencia $S(t)$ es:
\begin{eqnarray*}
S(t) = 1 - F(t) = e^{-\lambda t}
\end{eqnarray*}

\section{Funci\'on de Riesgo}
La funci\'on de riesgo, $\lambda(t)$, proporciona la tasa instant\'anea de ocurrencia del evento en el tiempo $t$, dado que el individuo ha sobrevivido hasta el tiempo $t$. Matem\'aticamente, se define como:
\begin{eqnarray*}
\lambda(t) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t}
\end{eqnarray*}

\subsection{Relaci\'on entre $\lambda(t)$ y $f(t)$}
La funci\'on de riesgo se puede relacionar con la funci\'on de densidad de probabilidad $f(t)$ y la funci\'on de supervivencia $S(t)$ de la siguiente manera:
\begin{eqnarray*}
\lambda(t) &=& \frac{f(t)}{S(t)}
\end{eqnarray*}

\subsection{Derivaci\'on de $\lambda(t)$}
La derivaci\'on de $\lambda(t)$ se basa en la definici\'on condicional de la probabilidad:
\begin{eqnarray*}
\lambda(t) &=& \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t} \\
           &=& \lim_{\Delta t \to 0} \frac{\frac{P(t \leq T < t + \Delta t \text{ y } T \geq t)}{P(T \geq t)}}{\Delta t} \\
           &=& \lim_{\Delta t \to 0} \frac{\frac{P(t \leq T < t + \Delta t)}{P(T \geq t)}}{\Delta t} \\
           &=& \frac{f(t)}{S(t)}
\end{eqnarray*}

\section{Relaci\'on entre Funci\'on de Supervivencia y Funci\'on de Riesgo}
La funci\'on de supervivencia y la funci\'on de riesgo est\'an estrechamente relacionadas. La relaci\'on se expresa mediante la siguiente ecuaci\'on:
\begin{eqnarray*}
S(t) = \exp\left(-\int_0^t \lambda(u) \, du\right)
\end{eqnarray*}

\subsection{Deducci\'on de la Relaci\'on}
Para deducir esta relaci\'on, consideramos la derivada logar\'itmica de la funci\'on de supervivencia:
\begin{eqnarray*}
S(t) &=& \exp\left(-\int_0^t \lambda(u) \, du\right) \\
\log S(t) &=& -\int_0^t \lambda(u) \, du \\
\frac{d}{dt} \log S(t) &=& -\lambda(t) \\
\lambda(t) &=& -\frac{d}{dt} \log S(t)
\end{eqnarray*}

\section{Interpretaci\'on de la Funci\'on de Riesgo}
La funci\'on de riesgo, $\lambda(t)$, se interpreta como la tasa instant\'anea de ocurrencia del evento por unidad de tiempo, dado que el individuo ha sobrevivido hasta el tiempo $t$. Es una medida local del riesgo de falla en un instante espec\'ifico.

\subsection{Ejemplo de C\'alculo de $\lambda(t)$}
Consideremos nuevamente el caso donde el tiempo de supervivencia $T$ sigue una distribuci\'on exponencial con tasa $\lambda$. La funci\'on de densidad de probabilidad $f(t)$ es:
\begin{eqnarray*}
f(t) = \lambda e^{-\lambda t}
\end{eqnarray*}
La funci\'on de supervivencia $S(t)$ es:
\begin{eqnarray*}
S(t) = e^{-\lambda t}
\end{eqnarray*}
La funci\'on de riesgo $\lambda(t)$ se calcula como:
\begin{eqnarray*}
\lambda(t) &=& \frac{f(t)}{S(t)} \\
           &=& \frac{\lambda e^{-\lambda t}}{e^{-\lambda t}} \\
           &=& \lambda
\end{eqnarray*}
En este caso, $\lambda(t)$ es constante y igual a $\lambda$, lo que es una caracter\'istica de la distribuci\'on exponencial.

\section{Funciones de Riesgo Acumulada y Media Residual}
La funci\'on de riesgo acumulada $H(t)$ se define como:
\begin{eqnarray*}
H(t) = \int_0^t \lambda(u) \, du
\end{eqnarray*}
Esta funci\'on proporciona la suma acumulada de la tasa de riesgo hasta el tiempo $t$.

La funci\'on de vida media residual $e(t)$ se define como la esperanza del tiempo de vida restante dado que el individuo ha sobrevivido hasta el tiempo $t$:
\begin{eqnarray*}
e(t) = \mathbb{E}[T - t \mid T > t] = \int_t^\infty S(u) \, du
\end{eqnarray*}

\section{Ejemplo de C\'alculo de Funci\'on de Riesgo Acumulada y Vida Media Residual}
Consideremos nuevamente la distribuci\'on exponencial con tasa $\lambda$. La funci\'on de riesgo acumulada $H(t)$ es:
\begin{eqnarray*}
H(t) &=& \int_0^t \lambda \, du \\
     &=& \lambda t
\end{eqnarray*}

La funci\'on de vida media residual $e(t)$ es:
\begin{eqnarray*}
e(t) &=& \int_t^\infty e^{-\lambda u} \, du \\
     &=& \left[ \frac{-1}{\lambda} e^{-\lambda u} \right]_t^\infty \\
     &=& \frac{1}{\lambda} e^{-\lambda t} \\
     &=& \frac{1}{\lambda}
\end{eqnarray*}
En este caso, la vida media residual es constante e igual a $\frac{1}{\lambda}$, otra caracter\'istica de la distribuci\'on exponencial.

\section{Conclusi\'on}
La funci\'on de supervivencia y la funci\'on de riesgo son herramientas fundamentales en el an\'alisis de supervivencia. Entender su definici\'on, propiedades, y la relaci\'on entre ellas es esencial para modelar y analizar correctamente los datos de tiempo hasta evento. Las funciones de riesgo acumulada y vida media residual proporcionan informaci\'on adicional sobre la din\'amica del riesgo a lo largo del tiempo.



\chapter{Estimador de Kaplan-Meier}

\section{Introducci\'on}
El estimador de Kaplan-Meier, tambi\'en conocido como la funci\'on de supervivencia emp\'irica, es una herramienta no param\'etrica para estimar la funci\'on de supervivencia a partir de datos censurados. Este m\'etodo es especialmente \'util cuando los tiempos de evento están censurados a la derecha.

\section{Definici\'on del Estimador de Kaplan-Meier}
El estimador de Kaplan-Meier se define como:
\begin{eqnarray*}
\hat{S}(t) = \prod_{t_i \leq t} \left(1 - \frac{d_i}{n_i}\right)
\end{eqnarray*}
donde:
\begin{itemize}
    \item $t_i$ es el tiempo del $i$-\'esimo evento,
    \item $d_i$ es el n\'umero de eventos que ocurren en $t_i$,
    \item $n_i$ es el n\'umero de individuos en riesgo justo antes de $t_i$.
\end{itemize}

\section{Propiedades del Estimador de Kaplan-Meier}
El estimador de Kaplan-Meier tiene las siguientes propiedades:
\begin{itemize}
    \item Es una funci\'on escalonada que disminuye en los tiempos de los eventos observados.
    \item Puede manejar datos censurados a la derecha.
    \item Proporciona una estimaci\'on no param\'etrica de la funci\'on de supervivencia.
\end{itemize}

\subsection{Funci\'on Escalonada}
La funci\'on escalonada del estimador de Kaplan-Meier significa que $\hat{S}(t)$ permanece constante entre los tiempos de los eventos y disminuye en los tiempos de los eventos. Matem\'aticamente, si $t_i$ es el tiempo del $i$-\'esimo evento, entonces:
\begin{eqnarray*}
\hat{S}(t) = \hat{S}(t_i) \quad \text{para} \ t_i \leq t < t_{i+1}
\end{eqnarray*}

\subsection{Manejo de Datos Censurados}
El estimador de Kaplan-Meier maneja datos censurados a la derecha al ajustar la estimaci\'on de la funci\'on de supervivencia s\'olo en los tiempos en que ocurren eventos. Si un individuo es censurado antes de experimentar el evento, no contribuye a la disminuci\'on de $\hat{S}(t)$ en el tiempo de censura. Esto asegura que la censura no sesga la estimaci\'on de la supervivencia.

\subsection{Estimaci\'on No Param\'etrica}
El estimador de Kaplan-Meier es no param\'etrico porque no asume ninguna forma espec\'ifica para la distribuci\'on de los tiempos de supervivencia. En cambio, utiliza la informaci\'on emp\'irica disponible para estimar la funci\'on de supervivencia.

\section{Deducci\'on del Estimador de Kaplan-Meier}
La deducci\'on del estimador de Kaplan-Meier se basa en el principio de probabilidad condicional. Consideremos un conjunto de tiempos de supervivencia observados $t_1, t_2, \ldots, t_k$ con eventos en cada uno de estos tiempos. El estimador de la probabilidad de supervivencia m\'as all\'a del tiempo $t$ es el producto de las probabilidades de sobrevivir m\'as all\'a de cada uno de los tiempos de evento observados hasta $t$.

\subsection{Probabilidad Condicional}
La probabilidad de sobrevivir m\'as all\'a de $t_i$, dado que el individuo ha sobrevivido justo antes de $t_i$, es:
\begin{eqnarray*}
P(T > t_i \mid T \geq t_i) = 1 - \frac{d_i}{n_i}
\end{eqnarray*}
donde $d_i$ es el n\'umero de eventos en $t_i$ y $n_i$ es el n\'umero de individuos en riesgo justo antes de $t_i$.

\subsection{Producto de Probabilidades Condicionales}
La probabilidad de sobrevivir m\'as all\'a de un tiempo $t$ cualquiera, dada la secuencia de tiempos de evento, es el producto de las probabilidades condicionales de sobrevivir m\'as all\'a de cada uno de los tiempos de evento observados hasta $t$. As\'i, el estimador de Kaplan-Meier se obtiene como:
\begin{eqnarray*}
\hat{S}(t) = \prod_{t_i \leq t} \left(1 - \frac{d_i}{n_i}\right)
\end{eqnarray*}

\section{Ejemplo de C\'alculo}
Supongamos que tenemos los siguientes tiempos de supervivencia observados para cinco individuos: 2, 3, 5, 7, 8. Supongamos adem\'as que tenemos censura a la derecha en el tiempo 10. Los tiempos de evento y el n\'umero de individuos en riesgo justo antes de cada evento son:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
Tiempo ($t_i$) & Eventos ($d_i$) & En Riesgo ($n_i$) \\
\hline
2 & 1 & 5 \\
3 & 1 & 4 \\
5 & 1 & 3 \\
7 & 1 & 2 \\
8 & 1 & 1 \\
\hline
\end{tabular}
\caption{Ejemplo de c\'alculo del estimador de Kaplan-Meier}
\end{table}

Usando estos datos, el estimador de Kaplan-Meier se calcula como:
\begin{eqnarray*}
\hat{S}(2) &=& 1 - \frac{1}{5} = 0.8 \\
\hat{S}(3) &=& 0.8 \times \left(1 - \frac{1}{4}\right) = 0.8 \times 0.75 = 0.6 \\
\hat{S}(5) &=& 0.6 \times \left(1 - \frac{1}{3}\right) = 0.6 \times 0.6667 = 0.4 \\
\hat{S}(7) &=& 0.4 \times \left(1 - \frac{1}{2}\right) = 0.4 \times 0.5 = 0.2 \\
\hat{S}(8) &=& 0.2 \times \left(1 - \frac{1}{1}\right) = 0.2 \times 0 = 0 \\
\end{eqnarray*}

\section{Intervalos de Confianza para el Estimador de Kaplan-Meier}
Para calcular intervalos de confianza para el estimador de Kaplan-Meier, se puede usar la transformaci\'on logar\'itmica y la aproximaci\'on normal. Un intervalo de confianza aproximado para $\log(-\log(\hat{S}(t)))$ se obtiene como:
\begin{eqnarray*}
\log(-\log(\hat{S}(t))) \pm z_{\alpha/2} \sqrt{\frac{1}{d_i(n_i - d_i)}}
\end{eqnarray*}
donde $z_{\alpha/2}$ es el percentil correspondiente de la distribuci\'on normal est\'andar.

\section{Transformaci\'on Logar\'itmica Inversa}
La transformaci\'on logar\'itmica inversa se utiliza para obtener los l\'imites del intervalo de confianza para $S(t)$:
\begin{eqnarray*}
\hat{S}(t) = \exp\left(-\exp\left(\log(-\log(\hat{S}(t))) \pm z_{\alpha/2} \sqrt{\frac{1}{d_i(n_i - d_i)}}\right)\right)
\end{eqnarray*}

\section{C\'alculo Detallado de Intervalos de Confianza}
Para un c\'alculo m\'as detallado de los intervalos de confianza, consideremos un tiempo espec\'ifico $t_j$. La varianza del estimador de Kaplan-Meier en $t_j$ se puede estimar usando Greenwood's formula:
\begin{eqnarray*}
\text{Var}(\hat{S}(t_j)) = \hat{S}(t_j)^2 \sum_{t_i \leq t_j} \frac{d_i}{n_i(n_i - d_i)}
\end{eqnarray*}
El intervalo de confianza aproximado para $\hat{S}(t_j)$ es entonces:
\begin{eqnarray*}
\hat{S}(t_j) \pm z_{\alpha/2} \sqrt{\text{Var}(\hat{S}(t_j))}
\end{eqnarray*}

\section{Ejemplo de Intervalo de Confianza}
Supongamos que en el ejemplo anterior queremos calcular el intervalo de confianza para $\hat{S}(3)$. Primero, calculamos la varianza:
\begin{eqnarray*}
\text{Var}(\hat{S}(3)) &=& \hat{S}(3)^2 \left( \frac{1}{5 \times 4} + \frac{1}{4 \times 3} \right) \\
                       &=& 0.6^2 \left( \frac{1}{20} + \frac{1}{12} \right) \\
                       &=& 0.36 \left( 0.05 + 0.0833 \right) \\
                       &=& 0.36 \times 0.1333 \\
                       &=& 0.048
\end{eqnarray*}
El intervalo de confianza es entonces:
\begin{eqnarray*}
0.6 \pm 1.96 \sqrt{0.048} = 0.6 \pm 1.96 \times 0.219 = 0.6 \pm 0.429
\end{eqnarray*}
Por lo tanto, el intervalo de confianza para $\hat{S}(3)$ es aproximadamente $(0.171, 1.029)$. Dado que una probabilidad no puede exceder 1, ajustamos el intervalo a $(0.171, 1.0)$.

\section{Interpretaci\'on del Estimador de Kaplan-Meier}
El estimador de Kaplan-Meier proporciona una estimaci\'on emp\'irica de la funci\'on de supervivencia que es f\'acil de interpretar y calcular. Su capacidad para manejar datos censurados lo hace especialmente \'util en estudios de supervivencia.

\section{Conclusi\'on}
El estimador de Kaplan-Meier es una herramienta poderosa para estimar la funci\'on de supervivencia en presencia de datos censurados. Su c\'alculo es relativamente sencillo y proporciona una estimaci\'on no param\'etrica robusta de la supervivencia a lo largo del tiempo. La interpretaci\'on adecuada de este estimador y su intervalo de confianza asociado es fundamental para el an\'alisis de datos de supervivencia.



\chapter{Comparación de Curvas de Supervivencia}

\section{Introducci\'on}
Comparar curvas de supervivencia es crucial para determinar si existen diferencias significativas en las tasas de supervivencia entre diferentes grupos. Las pruebas de hip\'otesis, como el test de log-rank, son herramientas comunes para esta comparaci\'on.

\section{Test de Log-rank}
El test de log-rank se utiliza para comparar las curvas de supervivencia de dos o más grupos. La hip\'otesis nula es que no hay diferencia en las funciones de riesgo entre los grupos.

\subsection{F\'ormula del Test de Log-rank}
El estad\'istico del test de log-rank se define como:
\begin{eqnarray*}
\chi^2 = \frac{\left(\sum_{i=1}^k (O_i - E_i)\right)^2}{\sum_{i=1}^k V_i}
\end{eqnarray*}
donde:
\begin{itemize}
    \item $O_i$ es el n\'umero observado de eventos en el grupo $i$.
    \item $E_i$ es el n\'umero esperado de eventos en el grupo $i$.
    \item $V_i$ es la varianza del n\'umero de eventos en el grupo $i$.
\end{itemize}

\subsection{Cálculo de $E_i$ y $V_i$}
El n\'umero esperado de eventos $E_i$ y la varianza $V_i$ se calculan como:
\begin{eqnarray*}
E_i &=& \frac{d_i \cdot n_i}{n} \\
V_i &=& \frac{d_i \cdot (n - d_i) \cdot n_i \cdot (n - n_i)}{n^2 \cdot (n - 1)}
\end{eqnarray*}
donde:
\begin{itemize}
    \item $d_i$ es el n\'umero total de eventos en el grupo $i$.
    \item $n_i$ es el n\'umero de individuos en riesgo en el grupo $i$.
    \item $n$ es el n\'umero total de individuos en todos los grupos.
\end{itemize}

\section{Ejemplo de C\'alculo del Test de Log-rank}
Supongamos que tenemos dos grupos con los siguientes datos de eventos:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Grupo & Tiempo ($t_i$) & Eventos ($O_i$) & En Riesgo ($n_i$) \\
\hline
1 & 2 & 1 & 5 \\
1 & 4 & 1 & 4 \\
2 & 3 & 1 & 4 \\
2 & 5 & 1 & 3 \\
\hline
\end{tabular}
\caption{Ejemplo de datos para el test de log-rank}
\end{table}

Calculemos $E_i$ y $V_i$ para cada grupo:

\begin{eqnarray*}
E_1 &=& \frac{2 \cdot 5}{9} + \frac{2 \cdot 4}{8} = \frac{10}{9} + \frac{8}{8} = 1.11 + 1 = 2.11 \\
V_1 &=& \frac{2 \cdot 7 \cdot 5 \cdot 4}{81 \cdot 8} = \frac{2 \cdot 7 \cdot 5 \cdot 4}{648} = \frac{280}{648} = 0.432 \\
E_2 &=& \frac{2 \cdot 4}{9} + \frac{2 \cdot 3}{8} = \frac{8}{9} + \frac{6}{8} = 0.89 + 0.75 = 1.64 \\
V_2 &=& \frac{2 \cdot 7 \cdot 4 \cdot 4}{81 \cdot 8} = \frac{2 \cdot 7 \cdot 4 \cdot 4}{648} = \frac{224}{648} = 0.346 \\
\end{eqnarray*}

El estad\'istico de log-rank se calcula como:
\begin{eqnarray*}
\chi^2 &=& \frac{\left((1 - 2.11) + (1 - 1.64)\right)^2}{0.432 + 0.346} \\
       &=& \frac{\left(-1.11 - 0.64\right)^2}{0.778} \\
       &=& \frac{3.04}{0.778} \\
       &=& 3.91
\end{eqnarray*}

El valor p se puede obtener comparando $\chi^2$ con una distribuci\'on $\chi^2$ con un grado de libertad (dado que estamos comparando dos grupos).

\section{Interpretaci\'on del Test de Log-rank}
Un valor p peque\~no (generalmente menos de 0.05) indica que hay una diferencia significativa en las curvas de supervivencia entre los grupos. Un valor p grande sugiere que no hay suficiente evidencia para rechazar la hip\'otesis nula de que las curvas de supervivencia son iguales.

\section{Pruebas Alternativas}
Adem\'as del test de log-rank, existen otras pruebas para comparar curvas de supervivencia, como el test de Wilcoxon (Breslow), que da m\'as peso a los eventos en tiempos tempranos.

\section{Conclusi\'on}
El test de log-rank es una herramienta esencial para comparar curvas de supervivencia entre diferentes grupos. Su c\'alculo se basa en la diferencia entre los eventos observados y esperados en cada grupo, y su interpretaci\'on puede ayudar a identificar diferencias significativas en la supervivencia.



\chapter{Modelos de Riesgos Proporcionales de Cox}

\section{Introducci\'on}
El modelo de riesgos proporcionales de Cox, propuesto por David Cox en 1972, es una de las herramientas más utilizadas en el análisis de supervivencia. Este modelo permite evaluar el efecto de varias covariables en el tiempo hasta el evento, sin asumir una forma espec\'ifica para la distribuci\'on de los tiempos de supervivencia.

\section{Definici\'on del Modelo de Cox}
El modelo de Cox se define como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta^T X)
\end{eqnarray*}
donde:
\begin{itemize}
    \item $\lambda(t \mid X)$ es la funci\'on de riesgo en el tiempo $t$ dado el vector de covariables $X$.
    \item $\lambda_0(t)$ es la funci\'on de riesgo basal en el tiempo $t$.
    \item $\beta$ es el vector de coeficientes del modelo.
    \item $X$ es el vector de covariables.
\end{itemize}

\section{Supuesto de Proporcionalidad de Riesgos}
El modelo de Cox asume que las razones de riesgo entre dos individuos son constantes a lo largo del tiempo. Matemáticamente, si $X_i$ y $X_j$ son las covariables de dos individuos, la raz\'on de riesgos se expresa como:
\begin{eqnarray*}
\frac{\lambda(t \mid X_i)}{\lambda(t \mid X_j)} = \frac{\lambda_0(t) \exp(\beta^T X_i)}{\lambda_0(t) \exp(\beta^T X_j)} = \exp(\beta^T (X_i - X_j))
\end{eqnarray*}

\section{Estimaci\'on de los Par\'ametros}
Los par\'ametros $\beta$ se estiman utilizando el m\'etodo de m\'axima verosimilitud parcial. La funci\'on de verosimilitud parcial se define como:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^k \frac{\exp(\beta^T X_i)}{\sum_{j \in R(t_i)} \exp(\beta^T X_j)}
\end{eqnarray*}
donde $R(t_i)$ es el conjunto de individuos en riesgo en el tiempo $t_i$.

\subsection{Funci\'on de Log-Verosimilitud Parcial}
La funci\'on de log-verosimilitud parcial es:
\begin{eqnarray*}
\log L(\beta) = \sum_{i=1}^k \left(\beta^T X_i - \log \sum_{j \in R(t_i)} \exp(\beta^T X_j)\right)
\end{eqnarray*}

\subsection{Derivadas Parciales y Maximizaci\'on}
Para encontrar los estimadores de m\'axima verosimilitud, resolvemos el sistema de ecuaciones obtenido al igualar a cero las derivadas parciales de $\log L(\beta)$ con respecto a $\beta$:
\begin{eqnarray*}
\frac{\partial \log L(\beta)}{\partial \beta} = \sum_{i=1}^k \left(X_i - \frac{\sum_{j \in R(t_i)} X_j \exp(\beta^T X_j)}{\sum_{j \in R(t_i)} \exp(\beta^T X_j)}\right) = 0
\end{eqnarray*}

\section{Interpretaci\'on de los Coeficientes}
Cada coeficiente $\beta_i$ representa el logaritmo de la raz\'on de riesgos asociado con un incremento unitario en la covariable $X_i$. Un valor positivo de $\beta_i$ indica que un aumento en $X_i$ incrementa el riesgo del evento, mientras que un valor negativo indica una reducci\'on del riesgo.

\section{Evaluaci\'on del Modelo}
El modelo de Cox se eval\'ua utilizando varias t\'ecnicas, como el an\'alisis de residuos de Schoenfeld para verificar el supuesto de proporcionalidad de riesgos, y el uso de curvas de supervivencia estimadas para evaluar la bondad de ajuste.

\subsection{Residuos de Schoenfeld}
Los residuos de Schoenfeld se utilizan para evaluar la proporcionalidad de riesgos. Para cada evento en el tiempo $t_i$, el residuo de Schoenfeld para la covariable $X_j$ se define como:
\begin{eqnarray*}
r_{ij} = X_{ij} - \hat{X}_{ij}
\end{eqnarray*}
donde $\hat{X}_{ij}$ es la covariable ajustada.

\subsection{Curvas de Supervivencia Ajustadas}
Las curvas de supervivencia ajustadas se obtienen utilizando la funci\'on de riesgo basal estimada y los coeficientes del modelo. La funci\'on de supervivencia ajustada se define como:
\begin{eqnarray*}
\hat{S}(t \mid X) = \hat{S}_0(t)^{\exp(\beta^T X)}
\end{eqnarray*}
donde $\hat{S}_0(t)$ es la funci\'on de supervivencia basal estimada.

\section{Ejemplo de Aplicaci\'on del Modelo de Cox}
Consideremos un ejemplo con tres covariables: edad, sexo y tratamiento. Supongamos que los datos se ajustan a un modelo de Cox y obtenemos los siguientes coeficientes:
\begin{eqnarray*}
\hat{\beta}_{edad} = 0.02, \quad \hat{\beta}_{sexo} = -0.5, \quad \hat{\beta}_{tratamiento} = 1.2
\end{eqnarray*}

La funci\'on de riesgo ajustada se expresa como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(0.02 \cdot \text{edad} - 0.5 \cdot \text{sexo} + 1.2 \cdot \text{tratamiento})
\end{eqnarray*}

\section{Conclusi\'on}
El modelo de riesgos proporcionales de Cox es una herramienta poderosa para analizar datos de supervivencia con m\'ultiples covariables. Su flexibilidad y la falta de suposiciones fuertes sobre la distribuci\'on de los tiempos de supervivencia lo hacen ampliamente aplicable en diversas disciplinas.



\chapter{Diagnóstico y Validación de Modelos de Cox}

\section{Introducci\'on}
Una vez ajustado un modelo de Cox, es crucial realizar diagn\'osticos y validaciones para asegurar que el modelo es apropiado y que los supuestos subyacentes son válidos. Esto incluye la verificaci\'on del supuesto de proporcionalidad de riesgos y la evaluaci\'on del ajuste del modelo.

\section{Supuesto de Proporcionalidad de Riesgos}
El supuesto de proporcionalidad de riesgos implica que la raz\'on de riesgos entre dos individuos es constante a lo largo del tiempo. Si este supuesto no se cumple, las inferencias hechas a partir del modelo pueden ser incorrectas.

\subsection{Residuos de Schoenfeld}
Los residuos de Schoenfeld se utilizan para evaluar la proporcionalidad de riesgos. Para cada evento en el tiempo $t_i$, el residuo de Schoenfeld para la covariable $X_j$ se define como:
\begin{eqnarray*}
r_{ij} = X_{ij} - \hat{X}_{ij}
\end{eqnarray*}
donde $\hat{X}_{ij}$ es la covariable ajustada. Si los residuos de Schoenfeld no muestran una tendencia sistemática cuando se trazan contra el tiempo, el supuesto de proporcionalidad de riesgos es razonable.

\section{Bondad de Ajuste}
La bondad de ajuste del modelo de Cox se eval\'ua comparando las curvas de supervivencia observadas y ajustadas, y utilizando estad\'isticas de ajuste global.

\subsection{Curvas de Supervivencia Ajustadas}
Las curvas de supervivencia aaustadas se obtienen utilizando la funci\'on de riesgo basal estimada y los coeficientes del modelo. La funci\'on de supervivencia ajustada se define como:
\begin{eqnarray*}
\hat{S}(t \mid X) = \hat{S}_0(t)^{\exp(\beta^T X)}
\end{eqnarray*}
donde $\hat{S}_0(t)$ es la funci\'on de supervivencia basal estimada. Comparar estas curvas con las curvas de Kaplan-Meier para diferentes niveles de las covariables puede proporcionar una validaci\'on visual del ajuste del modelo.

\subsection{Estad\'isticas de Ajuste Global}
Las estad\'isticas de ajuste global, como el test de la desviaci\'on y el test de la bondad de ajuste de Grambsch y Therneau, se utilizan para evaluar el ajuste global del modelo de Cox.

\section{Diagn\'ostico de Influencia}
El diagn\'ostico de influencia identifica observaciones individuales que tienen un gran impacto en los estimados del modelo. Los residuos de devianza y los residuos de martingala se utilizan com\'unmente para este prop\'osito.

\subsection{Residuos de Deviance}
Los residuos de deviance se definen como:
\begin{eqnarray*}
D_i = \text{sign}(O_i - E_i) \sqrt{-2 \left(O_i \log \frac{O_i}{E_i} - (O_i - E_i)\right)}
\end{eqnarray*}
donde $O_i$ es el n\'umero observado de eventos y $E_i$ es el n\'umero esperado de eventos. Observaciones con residuos de deviance grandes en valor absoluto pueden ser influyentes.

\subsection{Residuos de Martingala}
Los residuos de martingala se definen como:
\begin{eqnarray*}
M_i = O_i - E_i
\end{eqnarray*}
donde $O_i$ es el n\'umero observado de eventos y $E_i$ es el n\'umero esperado de eventos. Los residuos de martingala se utilizan para detectar observaciones que no se ajustan bien al modelo.

\section{Ejemplo de Diagn\'ostico}
Consideremos un modelo de Cox ajustado con las covariables edad, sexo y tratamiento. Para diagnosticar la influencia de observaciones individuales, calculamos los residuos de deviance y martingala para cada observaci\'on.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Observaci\'on & Edad & Sexo & Tratamiento & Residuo de Deviance \\
\hline
1 & 50 & 0 & 1 & 1.2 \\
2 & 60 & 1 & 0 & -0.5 \\
3 & 45 & 0 & 1 & -1.8 \\
4 & 70 & 1 & 0 & 0.3 \\
\hline
\end{tabular}
\caption{Residuos de deviance para observaciones individuales}
\end{table}

Observaciones con residuos de deviance grandes en valor absoluto (como la observaci\'on 3) pueden ser influyentes y requieren una revisi\'on adicional.

\section{Conclusi\'on}
El diagn\'ostico y la validaci\'on son pasos cr\'iticos en el an\'slisis de modelos de Cox. Evaluar el supuesto de proporcionalidad de riesgos, la bondad de ajuste y la influencia de observaciones individuales asegura que las inferencias y conclusiones derivadas del modelo sean v\'slidas y fiables.



\chapter{Modelos Acelerados de Fallos}
\section{Introducci\'on}
Los modelos acelerados de fallos (AFT) son una alternativa a los modelos de riesgos proporcionales de Cox. En lugar de asumir que las covariables afectan la tasa de riesgo, los modelos AFT asumen que las covariables multiplican el tiempo de supervivencia por una constante.

\section{Definici\'on del Modelo AFT}
Un modelo AFT se expresa como:
\begin{eqnarray*}
T = T_0 \exp(\beta^T X)
\end{eqnarray*}
donde:
\begin{itemize}
    \item $T$ es el tiempo de supervivencia observado.
    \item $T_0$ es el tiempo de supervivencia bajo condiciones basales.
    \item $\beta$ es el vector de coeficientes del modelo.
    \item $X$ es el vector de covariables.
\end{itemize}

\subsection{Transformaci\'on Logar\'itmica}
El modelo AFT se puede transformar logar\'itmicamente para obtener una forma lineal:
\begin{eqnarray*}
\log(T) = \log(T_0) + \beta^T X
\end{eqnarray*}

\section{Estimaci\'on de los Parámetros}
Los parámetros del modelo AFT se estiman utilizando el m\'etodo de máxima verosimilitud. La funci\'on de verosimilitud se define como:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^n f(t_i \mid X_i; \beta)
\end{eqnarray*}
donde $f(t_i \mid X_i; \beta)$ es la funci\'on de densidad de probabilidad del tiempo de supervivencia $t_i$ dado el vector de covariables $X_i$ y los par\'ametros $\beta$.

\subsection{Funci\'on de Log-Verosimilitud}
La funci\'on de log-verosimilitud es:
\begin{eqnarray*}
\log L(\beta) = \sum_{i=1}^n \log f(t_i \mid X_i; \beta)
\end{eqnarray*}

\subsection{Maximizaci\'on de la Verosimilitud}
Los estimadores de m\'axima verosimilitud se obtienen resolviendo el sistema de ecuaciones obtenido al igualar a cero las derivadas parciales de $\log L(\beta)$ con respecto a $\beta$:
\begin{eqnarray*}
\frac{\partial \log L(\beta)}{\partial \beta} = 0
\end{eqnarray*}

\section{Distribuciones Comunes en Modelos AFT}
En los modelos AFT, el tiempo de supervivencia $T$ puede seguir varias distribuciones comunes, como la exponencial, Weibull, log-normal y log-log\'istica. Cada una de estas distribuciones tiene diferentes propiedades y aplicaciones.

\subsection{Modelo Exponencial AFT}
En un modelo exponencial AFT, el tiempo de supervivencia $T$ sigue una distribuci\'on exponencial con par\'ametro $\lambda$:
\begin{eqnarray*}
f(t) = \lambda \exp(-\lambda t)
\end{eqnarray*}
La funci\'on de supervivencia es:
\begin{eqnarray*}
S(t) = \exp(-\lambda t)
\end{eqnarray*}
La transformaci\'on logar\'itmica del tiempo de supervivencia es:
\begin{eqnarray*}
\log(T) = \log\left(\frac{1}{\lambda}\right) + \beta^T X
\end{eqnarray*}

\subsection{Modelo Weibull AFT}
En un modelo Weibull AFT, el tiempo de supervivencia $T$ sigue una distribuci\'on Weibull con par\'ametros $\lambda$ y $k$:
\begin{eqnarray*}
f(t) = \lambda k t^{k-1} \exp(-\lambda t^k)
\end{eqnarray*}
La funci\'on de supervivencia es:
\begin{eqnarray*}
S(t) = \exp(-\lambda t^k)
\end{eqnarray*}
La transformaci\'on logar\'itmica del tiempo de supervivencia es:
\begin{eqnarray*}
\log(T) = \log\left(\left(\frac{1}{\lambda}\right)^{1/k}\right) + \frac{\beta^T X}{k}
\end{eqnarray*}

\section{Interpretaci\'on de los Coeficientes}
En los modelos AFT, los coeficientes $\beta_i$ se interpretan como factores multiplicativos del tiempo de supervivencia. Un valor positivo de $\beta_i$ indica que un aumento en la covariable $X_i$ incrementa el tiempo de supervivencia, mientras que un valor negativo indica una reducci\'on del tiempo de supervivencia.

\section{Ejemplo de Aplicaci\'on del Modelo AFT}
Consideremos un ejemplo con tres covariables: edad, sexo y tratamiento. Supongamos que los datos se ajustan a un modelo Weibull AFT y obtenemos los siguientes coeficientes:
\begin{eqnarray*}
\hat{\beta}_{edad} = -0.02, \quad \hat{\beta}_{sexo} = 0.5, \quad \hat{\beta}_{tratamiento} = -1.2
\end{eqnarray*}

La funci\'on de supervivencia ajustada se expresa como:
\begin{eqnarray*}
S(t \mid X) = \exp\left(-\left(\frac{t \exp(-0.02 \cdot \text{edad} + 0.5 \cdot \text{sexo} - 1.2 \cdot \text{tratamiento})}{\lambda}\right)^k\right)
\end{eqnarray*}

\section{Conclusi\'on}
Los modelos AFT proporcionan una alternativa flexible a los modelos de riesgos proporcionales de Cox. Su enfoque en la multiplicaci\'on del tiempo de supervivencia por una constante permite una interpretaci\'on intuitiva y aplicaciones en diversas \'areas.



\chapter{Análisis Multivariado de Supervivencia}

\section{Introducci\'on}
El análisis multivariado de supervivencia extiende los modelos de supervivencia para incluir m\'ultiples covariables, permitiendo evaluar su efecto simultáneo sobre el tiempo hasta el evento. Los modelos de Cox y AFT son com\'unmente utilizados en este contexto.

\section{Modelo de Cox Multivariado}
El modelo de Cox multivariado se define como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta^T X)
\end{eqnarray*}
donde $X$ es un vector de covariables.

\subsection{Estimaci\'on de los Parámetros}
Los parámetros $\beta$ se estiman utilizando el m\'etodo de máxima verosimilitud parcial, como se discuti\'o anteriormente. La funci\'on de verosimilitud parcial se maximiza para obtener los estimadores de los coeficientes.

\section{Modelo AFT Multivariado}
El modelo AFT multivariado se expresa como:
\begin{eqnarray*}
T = T_0 \exp(\beta^T X)
\end{eqnarray*}

\subsection{Estimaci\'on de los Par\'ametros}
Los par\'ametros $\beta$ se estiman utilizando el m\'etodo de m\'axima verosimilitud, similar al caso univariado. La funci\'on de verosimilitud se maximiza para obtener los estimadores de los coeficientes.

\section{Interacci\'on y Efectos No Lineales}
En el an\'alisis multivariado, es importante considerar la posibilidad de interacciones entre covariables y efectos no lineales. Estos se pueden incluir en los modelos extendiendo las funciones de riesgo o supervivencia.

\subsection{Interacciones}
Las interacciones entre covariables se pueden modelar a\~nadiendo t\'erminos de interacci\'on en el modelo:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2)
\end{eqnarray*}
donde $X_1 X_2$ es el t\'ermino de interacci\'on.

\subsection{Efectos No Lineales}
Los efectos no lineales se pueden modelar utilizando funciones no lineales de las covariables, como polinomios o splines:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta_1 X + \beta_2 X^2)
\end{eqnarray*}

\section{Selecci\'on de Variables}
La selecci\'on de variables es crucial en el an\'alisis multivariado para evitar el sobreajuste y mejorar la interpretabilidad del modelo. M\'etodos como la regresi\'on hacia atr\'as, la regresi\'on hacia adelante y la selecci\'on por criterios de informaci\'on (AIC, BIC) son com\'unmente utilizados.

\subsection{Regresi\'on Hacia Atr\'as}
La regresi\'on hacia atr\'as comienza con todas las covariables en el modelo y elimina iterativamente la covariable menos significativa hasta que todas las covariables restantes sean significativas.

\subsection{Regresi\'on Hacia Adelante}
La regresi\'on hacia adelante comienza con un modelo vac\'io y a\~nade iterativamente la covariable m\'as significativa hasta que no se pueda a\~nadir ninguna covariable adicional significativa.

\subsection{Criterios de Informaci\'on}
Los criterios de informaci\'on, como el AIC (Akaike Information Criterion) y el BIC (Bayesian Information Criterion), se utilizan para seleccionar el modelo que mejor se ajusta a los datos con la menor complejidad posible:
\begin{eqnarray*}
AIC &=& -2 \log L + 2k \\
BIC &=& -2 \log L + k \log n
\end{eqnarray*}
donde $L$ es la funci\'on de verosimilitud del modelo, $k$ es el n\'umero de par\'ametros en el modelo y $n$ es el tama\~no de la muestra.

\section{Ejemplo de An\'alisis Multivariado}
Consideremos un ejemplo con tres covariables: edad, sexo y tratamiento. Ajustamos un modelo de Cox multivariado y obtenemos los siguientes coeficientes:
\begin{eqnarray*}
\hat{\beta}_{edad} = 0.03, \quad \hat{\beta}_{sexo} = -0.6, \quad \hat{\beta}_{tratamiento} = 1.5
\end{eqnarray*}

La funci\'on de riesgo ajustada se expresa como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(0.03 \cdot \text{edad} - 0.6 \cdot \text{sexo} + 1.5 \cdot \text{tratamiento})
\end{eqnarray*}

\section{Conclusi\'on}
El an\'alisis multivariado de supervivencia permite evaluar el efecto conjunto de m\'ultiples covariables sobre el tiempo hasta el evento. La inclusi\'on de interacciones y efectos no lineales, junto con la selecci\'on adecuada de variables, mejora la precisi\'on y la interpretabilidad de los modelos de supervivencia.



\chapter{Supervivencia en Datos Complicados}

\section{Introducci\'on}
El análisis de supervivencia en datos complicados se refiere a la evaluaci\'on de datos de supervivencia que presentan desaf\'ios adicionales, como la censura por intervalo, datos truncados y datos con m\'ultiples tipos de eventos. Estos escenarios requieren m\'etodos avanzados para un análisis adecuado.

\section{Censura por Intervalo}
La censura por intervalo ocurre cuando el evento de inter\'es se sabe que ocurri\'o dentro de un intervalo de tiempo, pero no se conoce el momento exacto. Esto es com\'un en estudios donde las observaciones se realizan en puntos de tiempo discretos.

\subsection{Modelo para Datos Censurados por Intervalo}
Para datos censurados por intervalo, la funci\'on de verosimilitud se modifica para incluir la probabilidad de que el evento ocurra dentro de un intervalo:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^n P(T_i \in [L_i, U_i] \mid X_i; \beta)
\end{eqnarray*}
donde $[L_i, U_i]$ es el intervalo de tiempo durante el cual se sabe que ocurri\'o el evento para el individuo $i$.

\section{Datos Truncados}
Los datos truncados ocurren cuando los tiempos de supervivencia est\'an sujetos a un umbral, y solo se observan los individuos cuyos tiempos de supervivencia superan (o est\'an por debajo de) ese umbral. Existen dos tipos principales de truncamiento: truncamiento a la izquierda y truncamiento a la derecha.

\subsection{Modelo para Datos Truncados}
Para datos truncados a la izquierda, la funci\'on de verosimilitud se ajusta para considerar solo los individuos que superan el umbral de truncamiento:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^n \frac{f(t_i \mid X_i; \beta)}{1 - F(L_i \mid X_i; \beta)}
\end{eqnarray*}
donde $L_i$ es el umbral de truncamiento para el individuo $i$.

\section{An\'alisis de Competing Risks}
En estudios donde pueden ocurrir m\'ultiples tipos de eventos (competing risks), es crucial modelar adecuadamente el riesgo asociado con cada tipo de evento. La probabilidad de ocurrencia de cada evento compite con las probabilidades de ocurrencia de otros eventos.

\subsection{Modelo de Competing Risks}
Para un an\'alisis de competing risks, la funci\'on de riesgo se descompone en funciones de riesgo espec\'ificas para cada tipo de evento:
\begin{eqnarray*}
\lambda(t) = \sum_{j=1}^m \lambda_j(t)
\end{eqnarray*}
donde $\lambda_j(t)$ es la funci\'on de riesgo para el evento $j$.

\section{M\'etodos de Imputaci\'on}
Los m\'etodos de imputaci\'on se utilizan para manejar datos faltantes o censurados en estudios de supervivencia. La imputaci\'on m\'ultiple es un enfoque com\'un que crea m\'ultiples conjuntos de datos completos imputando valores faltantes varias veces y luego combina los resultados.

\subsection{Imputaci\'on M\'ultiple}
La imputaci\'on m\'ultiple para datos de supervivencia se realiza en tres pasos:
\begin{enumerate}
    \item Imputar los valores faltantes m\'ultiples veces para crear varios conjuntos de datos completos.
    \item Analizar cada conjunto de datos completo por separado utilizando m\'etodos de supervivencia est\'andar.
    \item Combinar los resultados de los an\'alisis separados para obtener estimaciones y varianzas combinadas.
\end{enumerate}

\section{Ejemplo de An\'alisis con Datos Complicados}
Consideremos un estudio con datos censurados por intervalo y competing risks. Ajustamos un modelo para los datos censurados por intervalo y obtenemos los siguientes coeficientes para las covariables edad y tratamiento:
\begin{eqnarray*}
\hat{\beta}_{edad} = 0.04, \quad \hat{\beta}_{tratamiento} = -0.8
\end{eqnarray*}

La funci\'on de supervivencia ajustada se expresa como:
\begin{eqnarray*}
S(t \mid X) = \exp\left(-\left(\frac{t \exp(0.04 \cdot \text{edad} - 0.8 \cdot \text{tratamiento})}{\lambda}\right)^k\right)
\end{eqnarray*}

\section{Conclusi\'on}
El an\'alisis de supervivencia en datos complicados requiere m\'etodos avanzados para manejar censura por intervalo, datos truncados y competing risks. La aplicaci\'on de modelos adecuados y m\'etodos de imputaci\'on asegura un an\'alisis preciso y completo de estos datos complejos.



\chapter{Proyecto Final y Revisión}

\section{Introducci\'on}
El proyecto final proporciona una oportunidad para aplicar los conceptos y t\'ecnicas aprendidas en el curso de análisis de supervivencia. Este cap\'itulo incluye una gu\'ia para desarrollar un proyecto de análisis de supervivencia y una revisi\'on de los conceptos clave.

\section{Desarrollo del Proyecto}
El proyecto final debe incluir los siguientes componentes:
\begin{enumerate}
    \item Definici\'on del problema: Identificar la pregunta de investigaci\'on y los objetivos del análisis de supervivencia.
    \item Descripci\'on de los datos: Presentar los datos utilizados, incluyendo las covariables y la estructura de los datos.
    \item Análisis exploratorio: Realizar un análisis descriptivo de los datos, incluyendo la censura y la distribuci\'on de los tiempos de supervivencia.
    \item Ajuste del modelo: Ajustar modelos de supervivencia adecuados (Kaplan-Meier, Cox, AFT) y evaluar su bondad de ajuste.
    \item Diagn\'ostico del modelo: Realizar diagn\'osticos para evaluar los supuestos del modelo y la influencia de observaciones individuales.
    \item Interpretaci\'on de resultados: Interpretar los coeficientes del modelo y las curvas de supervivencia ajustadas.
    \item Conclusiones: Resumir los hallazgos del análisis y proporcionar recomendaciones basadas en los resultados.
\end{enumerate}

\section{Revisi\'on de Conceptos Clave}
Una revisi\'on de los conceptos clave del an\'alisis de supervivencia incluye:
\begin{itemize}
    \item \textbf{Funci\'on de Supervivencia:} Define la probabilidad de sobrevivir m\'as all\'a de un tiempo espec\'ifico.
    \item \textbf{Funci\'on de Riesgo:} Define la tasa instant\'anea de ocurrencia del evento.
    \item \textbf{Estimador de Kaplan-Meier:} Proporciona una estimaci\'on no param\'etrica de la funci\'on de supervivencia.
    \item \textbf{Test de Log-rank:} Compara curvas de supervivencia entre diferentes grupos.
    \item \textbf{Modelo de Cox:} Eval\'ua el efecto de m\'ultiples covariables sobre el tiempo hasta el evento, asumiendo proporcionalidad de riesgos.
    \item \textbf{Modelos AFT:} Modelan el efecto de las covariables multiplicando el tiempo de supervivencia por una constante.
    \item \textbf{An\'alisis Multivariado:} Considera interacciones y efectos no lineales entre m\'ultiples covariables.
    \item \textbf{Supervivencia en Datos Complicados:} Maneja censura por intervalo, datos truncados y competing risks.
\end{itemize}

\section{Ejemplo de Proyecto Final}
A continuaci\'on se presenta un ejemplo de estructura de un proyecto final de an\'alisis de supervivencia:

\subsection{Definici\'on del Problema}
Analizar el efecto del tratamiento y la edad sobre la supervivencia de pacientes con una enfermedad espec\'ifica.

\subsection{Descripci\'on de los Datos}
Datos de supervivencia de 100 pacientes, con covariables: edad, sexo y tipo de tratamiento. Los tiempos de supervivencia est\'an censurados a la derecha.

\subsection{An\'alisis Exploratorio}
Realizar histogramas y curvas de Kaplan-Meier para explorar la distribuci\'on de los tiempos de supervivencia y la censura.

\subsection{Ajuste del Modelo}
Ajustar un modelo de Cox y un modelo AFT con las covariables edad y tratamiento.

\subsection{Diagn\'ostico del Modelo}
Evaluar la proporcionalidad de riesgos y realizar an\'alisis de residuos para identificar observaciones influyentes.

\subsection{Interpretaci\'on de Resultados}
Interpretar los coeficientes del modelo y las curvas de supervivencia ajustadas para diferentes niveles de las covariables.

\subsection{Conclusiones}
Resumir los hallazgos y proporcionar recomendaciones sobre el efecto del tratamiento y la edad en la supervivencia de los pacientes.

\section{Conclusi\'on}
El proyecto final es una oportunidad para aplicar los conocimientos adquiridos en un contexto pr\'actico. La revisi\'on de los conceptos clave y la aplicaci\'on de t\'ecnicas adecuadas de an\'alisis de supervivencia aseguran un an\'alisis riguroso y significativo.

\chapter{Fundamentos}


\section{2. Pruebas de Hipótesis}

\subsection{2.1 Tipos de errores}

\begin{itemize}
    \item Una hipótesis estadística es una afirmación acerca de la distribución de probabilidad de una variable aleatoria, a menudo involucran uno o más parámetros de la distribución.
    \item Las hipótesis son afirmaciones respecto a la población o distribución bajo estudio, no en torno a la muestra.
    \item La mayoría de las veces, la prueba de hipótesis consiste en determinar si la situación experimental ha cambiado.
    \item El interés principal es decidir sobre la veracidad o falsedad de una hipótesis, a este procedimiento se le llama \textit{prueba de hipótesis}.
    \item Si la información es consistente con la hipótesis, se concluye que esta es verdadera, de lo contrario que con base en la información, es falsa.
\end{itemize}

Una prueba de hipótesis está formada por cinco partes:
\begin{itemize}
    \item La hipótesis nula, denotada por $H_{0}$.
    \item La hipótesis alternativa, denotada por $H_{1}$.
    \item El estadístico de prueba y su valor $p$.
    \item La región de rechazo.
    \item La conclusión.
\end{itemize}

\begin{Def}
Las dos hipótesis en competencia son la \textbf{hipótesis alternativa $H_{1}$}, usualmente la que se desea apoyar, y la \textbf{hipótesis nula $H_{0}$}, opuesta a $H_{1}$.
\end{Def}
En general, es más fácil presentar evidencia de que $H_{1}$ es cierta, que demostrar que $H_{0}$ es falsa, es por eso que por lo regular se comienza suponiendo que $H_{0}$ es cierta, luego se utilizan los datos de la muestra para decidir si existe evidencia a favor de $H_{1}$, más que a favor de $H_{0}$, así se tienen dos conclusiones:
\begin{itemize}
    \item Rechazar $H_{0}$ y concluir que $H_{1}$ es verdadera.
    \item Aceptar, no rechazar, $H_{0}$ como verdadera.
\end{itemize}

\begin{Ejem}
Se desea demostrar que el salario promedio por hora en cierto lugar es distinto de $19$ usd, que es el promedio nacional. Entonces $H_{1}:\mu \neq 19$, y $H_{0}:\mu = 19$.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hipótesis de dos colas}.

\begin{Ejem}
Un determinado proceso produce un promedio de $5\%$ de piezas defectuosas. Se está interesado en demostrar que un simple ajuste en una máquina reducirá $p$, la proporción de piezas defectuosas producidas en este proceso. Entonces se tiene $H_{0}: p < 0.3$ y $H_{1}: p = 0.03$. Si se puede rechazar $H_{0}$, se concluye que el proceso ajustado produce menos del $5\%$ de piezas defectuosas.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hipótesis de una cola}.

La decisión de rechazar o aceptar la hipótesis nula está basada en la información contenida en una muestra proveniente de la población de interés. Esta información tiene estas formas:
\begin{itemize}
    \item \textbf{Estadístico de prueba:} un sólo número calculado a partir de la muestra.
    \item \textbf{$p$-value:} probabilidad calculada a partir del estadístico de prueba.
\end{itemize}

\begin{Def}
El $p$-value es la probabilidad de observar un estadístico de prueba tanto o más alejado del valor observado, si en realidad $H_{0}$ es verdadera.\medskip
Valores grandes del estadístico de prueba y valores pequeños de $p$ significan que se ha observado un evento muy poco probable, si $H_{0}$ en realidad es verdadera.
\end{Def}
Todo el conjunto de valores que puede tomar el estadístico de prueba se divide en dos regiones. Un conjunto, formado de valores que apoyan la hipótesis alternativa y llevan a rechazar $H_{0}$, se denomina \textbf{región de rechazo}. El otro, conformado por los valores que sustentan la hipótesis nula, se le denomina \textbf{región de aceptación}.\medskip

Cuando la región de rechazo está en la cola izquierda de la distribución, la prueba se denomina \textbf{prueba lateral izquierda}. Una prueba con región de rechazo en la cola derecha se le llama \textbf{prueba lateral derecha}.\medskip

Si el estadístico de prueba cae en la región de rechazo, entonces se rechaza $H_{0}$. Si el estadístico de prueba cae en la región de aceptación, entonces la hipótesis nula se acepta o la prueba se juzga como no concluyente.\medskip

Dependiendo del nivel de confianza que se desea agregar a las conclusiones de la prueba, y el \textbf{nivel de significancia $\alpha$}, el riesgo que está dispuesto a correr si se toma una decisión incorrecta.

\begin{Def}
Un \textbf{error de tipo I} para una prueba estadística es el error que se tiene al rechazar la hipótesis nula cuando es verdadera. El \textbf{nivel de significancia} para una prueba estadística de hipótesis es
\begin{eqnarray*}
\alpha &=& P\left\{\textrm{error tipo I}\right\} = P\left\{\textrm{rechazar equivocadamente } H_{0}\right\} \\
&=& P\left\{\textrm{rechazar } H_{0} \textrm{ cuando } H_{0} \textrm{ es verdadera}\right\}
\end{eqnarray*}
\end{Def}
Este valor $\alpha$ representa el valor máximo de riesgo tolerable de rechazar incorrectamente $H_{0}$. Una vez establecido el nivel de significancia, la región de rechazo se define para poder determinar si se rechaza $H_{0}$ con un cierto nivel de confianza.


\section{2.2 Muestras grandes: una media poblacional}
\subsection{2.2.1 Cálculo de valor $p$}


\begin{Def}
El \textbf{valor de $p$} (\textbf{$p$-value}) o nivel de significancia observado de un estadístico de prueba es el valor más pequeño de $\alpha$ para el cual $H_{0}$ se puede rechazar. El riesgo de cometer un error tipo $I$, si $H_{0}$ es rechazada con base en la información que proporciona la muestra.
\end{Def}

\begin{Note}
Valores pequeños de $p$ indican que el valor observado del estadístico de prueba se encuentra alejado del valor hipotético de $\mu$, es decir se tiene evidencia de que $H_{0}$ es falsa y por tanto debe de rechazarse.
\end{Note}

\begin{Note}
Valores grandes de $p$ indican que el estadístico de prueba observado no está alejado de la media hipotética y no apoya el rechazo de $H_{0}$.
\end{Note}

\begin{Def}
Si el valor de $p$ es menor o igual que el nivel de significancia $\alpha$, determinado previamente, entonces $H_{0}$ es rechazada y se puede concluir que los resultados son estadísticamente significativos con un nivel de confianza del $100 (1-\alpha)\%$.
\end{Def}
Es usual utilizar la siguiente clasificación de resultados:

\begin{tabular}{|c||c|l|}\hline
$p$ & $H_{0}$ & Significativa \\ \hline
$\leq 0.01$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. altamente significativos \\ y en contra de $H_{0}$\end{tabular} \\ \hline
$\leq 0.05$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. significativos \\ y en contra de $H_{0}$\end{tabular} \\ \hline
$\leq 0.10$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. posiblemente \\ significativos \\ y en contra de $H_{0}$\end{tabular} \\ \hline
$> 0.10$ & no rechazada & \begin{tabular}[c]{@{}l@{}}Result. no significativos \\ y no rechazar $H_{0}$\end{tabular} \\ \hline
\end{tabular}

\chapter{Elementos}

%---------------------------------------------------------
\section{Pruebas de Hip\'otesis}
%---------------------------------------------------------
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection*{Tipos de errores}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

\begin{itemize}
\item Una hip\'otesis estad\'istica es una afirmaci\'on  acerca de la distribuci\'on de probabilidad de una variable aleatoria, a menudo involucran uno o m\'as par\'ametros de la distribuci\'on.

\item Las hip\'otesis son afirmaciones respecto a la poblaci\'on o distribuci\'on bajo estudio, no en torno a la muestra.

\item La mayor\'ia de las veces, la prueba de hip\'otesis consiste en determinar si la situaci \'on experimental ha cambiado

\item el inter\'es principal es decidir sobre la veracidad o falsedad de una hip\'otesis, a este procedimiento se le llama \textit{prueba de hip\'otesis}.

\item Si la informaci\'on es consistente con la hip\'otesis, se concluye que esta es verdadera, de lo contrario que con base en la informaci\'on, es falsa.

\end{itemize}

Una prueba de hip\'otesis est\'a formada por cinco partes
\begin{itemize}
\item La hip\'otesis nula, denotada por $H_{0}$.
\item La hip\'otesis alterativa, denorada por $H_{1}$.
\item El estad\'sitico de prueba y su valor $p$.
\item La regi\'on de rechazo.
\item La conclusi\'on.
\end{itemize}

\begin{Def}
Las dos hip\'otesis en competencias son la \textbf{hip\'otesis alternativa $H_{1}$}, usualmente la que se desea apoyar, y la \textbf{hip\'otesis nula $H_{0}$}, opuesta a $H_{1}$.
\end{Def}

En general, es m\'as f\'acil presentar evidencia de que $H_{1}$ es cierta, que demostrar 	que $H_{0}$ es falsa, es por eso que por lo regular se comienza suponiendo que $H_{0}$ es cierta, luego se utilizan los datos de la muestra para decidir si existe evidencia a favor de $H_{1}$, m\'as que a favor de $H_{0}$, as\'i se tienen dos conclusiones:
\begin{itemize}
\item Rechazar $H_{0}$ y concluir que $H_{1}$ es verdadera.
\item Aceptar, no rechazar, $H_{0}$ como verdadera.
\end{itemize}

\begin{Ejem}
Se desea demostrar que el salario promedio  por hora en cierto lugar es distinto de $19$usd, que es el promedio nacional. Entonces $H_{1}:\mu\neq19$, y $H_{0}:\mu=19$.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de dos colas}.

\begin{Ejem}
Un determinado proceso produce un promedio de $5\%$ de piezas defectuosas. Se est\'a interesado en demostrar que un simple ajuste en una m\'aquina reducir\'a $p$, la proporci\'on de piezas defectuosas producidas en este proceso. Entonces se tiene $H_{0}:p<0.3$ y $H_{1}:p=0.03$. Si se puede rechazar $H_{0}$, se concluye que el proceso ajustado produce menos del $5\%$ de piezas defectuosas.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de una cola}.

La decisi\'on de rechazar o aceptar la hip\'otesis nula est\'a basada en la informaci\'on contenida en una muestra proveniente de la poblaci\'on de inter\'es. Esta informaci\'on tiene estas formas

\begin{itemize}
\item \textbf{Estad\'sitico de prueba:} un s\'olo n\'umero calculado a partir de la muestra.

\item \textbf{$p$-value:} probabilidad calculada a partir del estad\'stico de prueba.
\end{itemize}

\begin{Def}
El $p$-value es la probabilidad de observar un estad\'istico de prueba tanto o m\'as alejado del valor obervado, si en realidad $H_{0}$ es verdadera.\medskip
Valores grandes del estad\'stica de prueba  y valores peque\~nos de $p$ significan que se ha observado un evento muy poco probable, si $H_{0}$ en realidad es verdadera.
\end{Def}

Todo el conjunto de valores que puede tomar el estad\'istico de prueba se divide en dos regiones. Un conjunto, formado de valores que apoyan la hip\'otesis alternativa y llevan a rechazar $H_{0}$, se denomina \textbf{regi\'on de rechazo}. El otro, conformado por los valores que sustentatn la hip\'otesis nula, se le denomina \textbf{regi\'on de aceptaci\'on}.\medskip

Cuando la regi\'on de rechazo est\'a en la cola izquierda de la distribuci\'on, la  prueba se denomina \textbf{prueba lateral izquierda}. Una prueba con regi\'on de rechazo en la cola derecha se le llama \textbf{prueba lateral derecha}.\medskip

Si el estad\'stico de prueba cae en la regi\'on de rechazo, entonces se rechaza $H_{0}$. Si el estad\'stico de prueba cae en la regi\'on de aceptaci\'on, entonces la hip\'otesis nula se acepta o la prueba se juzga como no concluyente.\medskip

Dependiendo del nivel de confianza que se desea agregar a las conclusiones de la prueba, y el \textbf{nivel de significancia $\alpha$}, el riesgo que est\'a dispuesto a correr si se toma una decisi\'on incorrecta.

\begin{Def}
Un \textbf{error de tipo I} para una prueba estad\'istica es el error que se tiene al rechazar la hip\'otesis nula cuando es verdadera. El \textbf{nivel de significancia} para una prueba estad\'istica de hip\'otesis es
\begin{eqnarray*}
\alpha&=&P\left\{\textrm{error tipo I}\right\}=P\left\{\textrm{rechazar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es verdadera}\right\}
\end{eqnarray*}

\end{Def}

Este valor $\alpha$ representa el valor m\'aximo de riesgo tolerable de rechazar incorrectamente $H_{0}$. Una vez establecido el nivel de significancia, la regi\'on de rechazo se define para poder determinar si se rechaza $H_{0}$ con un cierto nivel de confianza.
%---------------------------------------------------------
\section*{Muestras grandes: una media poblacional}
%---------------------------------------------------------
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection*{C\'alculo de valor $p$}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\begin{Def}
El \textbf{valor de $p$} (\textbf{$p$-value}) o nivel de significancia observado de un estad\'istico de prueba es el valor m\'as peque\~ no de $\alpha$ para el cual $H_{0}$ se puede rechazar. El riesgo de cometer un error tipo $I$, si $H_{0}$ es rechazada con base en la informaci\'on que proporciona la muestra.
\end{Def}

\begin{Note}
Valores peque\~ nos de $p$ indican 	que el valor observado del estad\'stico de prueba se encuentra alejado del valor hipot\'etico de $\mu$, es decir se tiene evidencia de que $H_{0}$ es falsa y por tanto debe de rechazarse.
\end{Note}

\begin{Note}
Valores grandes de $p$ indican que el estad\'istico de prueba observado no est\'a alejado de la medi hipot\'etica y no apoya el rechazo de $H_{0}$.
\end{Note}

\begin{Def}
Si el valor de $p$ es menor o igual que el nivel de significancia $\alpha$, determinado previamente, entonces $H_{0}$ es rechazada y se puede concluir que los resultados son estad\'isticamente significativos con un nivel de confianza del $100\left(1-\alpha\right)\%$.
\end{Def}
Es usual utilizar la siguiente clasificaci\'on de resultados

\begin{tabular}{|c||c|l|}\hline
$p$& $H_{0}$&Significativa\\\hline\hline
$p<0.01$&Rechazar &Altamente\\\hline
$0.01\leq p<0.05$ & Rechazar&Estad\'isticamente\\\hline
$0.05\leq p <0.1$ & No rechazar & Tendencia estad\'istica\\\hline
$0.01\leq p$ & No rechazar & No son estad\'isticamente\\\hline
\end{tabular}

\begin{Note}
Para determinar el valor de $p$, encontrar el \'area en la cola despu\'es del estad\'istico de prueba. Si la prueba es de una cola, este es el valor de $p$. Si es de dos colas, \'este valor encontrado es la mitad del valor de $p$. Rechazar $H_{0}$ cuando el valor de $p<\alpha$.
\end{Note}

Hay dos tipos de errores al realizar una prueba de hip\'otesis
\begin{center}
\begin{tabular}{c|cc}
& $H_{0}$ es Verdadera & $H_{0}$ es Falsa\\\hline\hline
Rechazar $H_{0}$ & Error tipo I & $\surd$\\
Aceptar $H_{0}$ & $\surd$ & Error tipo II
\end{tabular}
\end{center}
\begin{Def}
La probabilidad de cometer el error tipo II se define por $\beta$ donde
\begin{eqnarray*}
\beta&=&P\left\{\textrm{error tipo II}\right\}=P\left\{\textrm{Aceptar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{Aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}
\end{eqnarray*}
\end{Def}

\begin{Note}
Cuando $H_{0}$ es falsa y $H_{1}$ es verdadera, no siempre es posible especificar un valor exacto de $\mu$, sino m\'as bien un rango de posibles valores.\medskip
En lugar de arriesgarse a tomar una decisi\'on incorrecta, es mejor conlcuir que \textit{no hay evidencia suficiente para rechazar $H_{0}$}, es decir en lugar de aceptar $H_{0}$, \textit{no rechazar $H_{0}$}.

\end{Note}
La bondad de una prueba estad\'istica se mide por el tama\~ no de $\alpha$ y $\beta$, ambas deben de ser peque\~ nas. Una manera muy efectiva de medir la potencia de la prueba es calculando el complemento del error tipo $II$:
\begin{eqnarray*}
1-\beta&= &P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{1}\textrm{ es verdadera}\right\}
\end{eqnarray*}
\begin{Def}
La \textbf{potencia de la prueba}, $1-\beta$, mide la capacidad de que la prueba funciona como se necesita.
\end{Def}

\begin{Ejem}
La producci\'on diariade una planta qu\'imica local ha promediado 880 toneladas en los \'ultimos a\~nos. A la gerente de control de calidad le gustar\'ia saber si este promedio ha cambiado en meses recientes. Ella selecciona al azar 50 d\'ias de la base de datos computarizada y calcula el promedio y la desviaci\'on est\'andar de las $n=50$  producciones como $\overline{x}=871$ toneladas y $s=21$ toneladas, respectivamente. Pruebe la hip\'otesis  apropiada usando $\alpha=0.05$.

La hip\'otesis nula apropiada es:
\begin{eqnarray*}
H_{0}&:& \mu=880\\
&&\textrm{ y la hip\'otesis alternativa }H_{1}\textrm{ es }\\
H_{1}&:& \mu\neq880
\end{eqnarray*}
el estimador puntual para $\mu$ es $\overline{x}$, entonces el estad\'istico de prueba es\medskip
\begin{eqnarray*}
z&=&\frac{\overline{x}-\mu_{0}}{s/\sqrt{n}}\\
&=&\frac{871-880}{21/\sqrt{50}}=-3.03
\end{eqnarray*}

Para esta prueba de  dos colas, hay que determinar los dos valores de $z_{\alpha/2}$, es decir,  $z_{\alpha/2}=\pm1.96$, como $z>z_{\alpha/2}$, $z$ cae en la zona de rechazo, por lo tanto  la gerente puede rechazar la hip\'otesis nula y concluir que el promedio efectivamente ha cambiado.\medskip
La probabilidad de rechazar $H_{0}$ cuando esta es verdadera es de  $0.05$.

Recordemos que el valor observado del estad\'istico de prueba es $z=-3.03$, la regi\'on de rechazo m\'as peque\~na que puede usarse y todav\'ia seguir rechazando $H_{0}$ es $|z|>3.03$, \\
entonces $p=2(0.012)=0.0024$, que a su vez es menor que el nivel de significancia $\alpha$ asignado inicialmente, y adem\'as los resultados son  \textbf{altamente significativos}.

Finalmente determinemos la potencia de la prueba cuando $\mu$ en realidad es igual a $870$ toneladas.

Recordar que la regi\'on de aceptaci\'on est\'a entre $-1.96$ y $1.96$, para $\mu=880$, equivalentemente $$874.18<\overline{x}<885.82$$
$\beta$ es la probabilidad de aceptar $H_{0}$ cuando $\mu=870$, calculemos los valores de $z$ correspondientes a $874.18$ y $885.82$ \medskip
Entonces
\begin{eqnarray*}
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{874.18-870}{21/\sqrt{50}}=1.41\\
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{885.82-870}{21/\sqrt{50}}=5.33
\end{eqnarray*}
por lo tanto
\begin{eqnarray*}
\beta&=&P\left\{\textrm{aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{874.18<\mu<885.82\textrm{ cuando }\mu=870\right\}\\
&=&P\left\{1.41<z<5.33\right\}=P\left\{1.41<z\right\}\\
&=&1-0.9207=0.0793
\end{eqnarray*}
entonces, la potencia de la prueba es
$$1-\beta=1-0.0793=0.9207$$ que es la probabilidad de rechazar correctamente $H_{0}$ cuando $H_{0}$ es falsa.
\end{Ejem}

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection*{Prueba de hip\'otesis para la diferencia entre dos medias poblacionales}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

El estad\'istico que resume la informaci\'on muestral respecto a la diferencia en medias poblacionales $\left(\mu_{1}-\mu_{2}\right)$ es la diferencia de las medias muestrales $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, por tanto al probar la difencia entre las medias muestrales se verifica que la diferencia real entre las medias poblacionales difiere de un valor especificado, $\left(\mu_{1}-\mu_{2}\right)=D_{0}$, se puede usar el error est\'andar de $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, es decir
$$\sqrt{\frac{\sigma^{2}_{1}}{n_{1}}+\frac{\sigma^{2}_{2}}{n_{2}}}$$
cuyo estimador est\'a dado por
$$SE=\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}$$
El procedimiento para muestras grandes es:

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,
donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}

\item[3) ] Estad\'istico de prueba:
$$z=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}$$
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}

\begin{Ejem}
Para determinar si ser propietario de un autom\'ovil afecta el rendimiento acad\'emico de un estudiante, se tomaron dos muestras aleatorias de 100 estudiantes varones. El promedio de calificaciones para los $n_{1}=100$ no propietarios de un auto tuvieron un promedio y varianza de $\overline{x}_{1}=2.7$ y $s_{1}^{2}=0.36$, respectivamente, mientras que para para la segunda muestra con $n_{2}=100$ propietarios de un auto, se tiene $\overline{x}_{2}=2.54$ y $s_{2}^{2}=0.4$. Los datos presentan suficiente evidencia para indicar una diferencia en la media en el rendimiento acad\'emico entre propietarios y no propietarios de un autom\'ovil? Hacer pruebas para $\alpha=0.01,0.05$ y $\alpha=0.1$.

\begin{itemize}
\item Soluci\'on utilizando la t\'ecnica de regiones de rechazo:\medskip
realizando las operaciones
$z=1.84$, determinar si excede los valores de $z_{\alpha/2}$.
\item Soluci\'on utilizando el $p$-value:\medskip
Calcular el valor de $p$, la probabilidad de que $z$ sea mayor que $z=1.84$ o menor que $z=-1.84$, se tiene que $p=0.0658$. Concluir.

\item Si el intervalo de confianza que se construye contiene el valor del par\'ametro especificado por $H_{0}$, entonces ese valor es uno de los posibles valores del par\'ametro y $H_{0}$ no debe ser rechazada.

\item Si el valor hipot\'etico se encuentra fuera de los l\'imites de confianza, la hip\'otesis nula es rechazada al nivel de significancia $\alpha$.
\end{itemize}
\end{Ejem}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection*{Prueba de Hip\'otesis para una Proporci\'on Binomial}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
Para una muestra aleatoria de $n$ intentos id\'enticos, de una poblaci\'on binomial, la proporci\'on muesrtal $\hat{p}$ tiene una distribuci\'on aproximadamente normal cuando $n$ es grande, con media $p$ y error est\'andar
$$SE=\sqrt{\frac{pq}{n}}.$$
La prueba de hip\'otesis de la forma
\begin{eqnarray*}
H_{0}&:&p=p_{0}\\
H_{1}&:&p>p_{0}\textrm{, o }p<p_{0}\textrm{ o }p\neq p_{0}
\end{eqnarray*}
El estad\'istico de prueba se construye con el mejor estimador de la proporci\'on verdadera, $\hat{p}$, con el estad\'istico de prueba $z$, que se distribuye normal est\'andar.

El procedimiento es
\begin{itemize}
\item[1) ] Hip\'otesis nula: $H_{0}:p=p_{0}$
\item[2) ] Hip\'otesis alternativa
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:p>p_{0}$ & $p\neq p_{0}$\\
$H_{1}:p<p_{0}$ & \\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
z=\frac{\hat{p}-p_{0}}{\sqrt{\frac{pq}{n}}},\hat{p}=\frac{x}{n}
\end{eqnarray*}
donde $x$ es el n\'umero de \'exitos en $n$ intentos binomiales.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection*{Prueba de Hip\'otesis diferencia entre dos Proporciones Binomiales}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

Cuando se tienen dos muestras aleatorias independientes de dos poblaciones binomiales, el objetivo del experimento puede ser la diferencia $\left(p_{1}-p_{2}\right)$ en las proporciones de individuos u objetos que poseen una caracter\'istica especifica en las dos poblaciones. En este caso se pueden utilizar los estimadores de las dos proporciones $\left(\hat{p}_{1}-\hat{p}_{2}\right)$ con error est\'andar dado por
$$SE=\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}$$
considerando el estad\'istico $z$ con un nivel de significancia $\left(1-\alpha\right)100\%$

La hip\'otesis nula a probarse es de la forma
\begin{itemize}
\item[$H_{0}$: ] $p_{1}=p_{2}$ o equivalentemente $\left(p_{1}-p_{2}\right)=0$, contra una hip\'otesis alternativa $H_{1}$ de una o dos colas.
\end{itemize}

Para estimar el error est\'andar del estad\'istico $z$, se debe de utilizar el hecho de que suponiendo que $H_{0}$ es verdadera, las dos proporciones son iguales a alg\'un valor com\'un, $p$. Para obtener el mejor estimador de $p$ es
$$p=\frac{\textrm{n\'umero total de \'exitos}}{\textrm{N\'umero total de pruebas}}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$$

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\left(p_{1}-p_{2}\right)=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(p_{1}-p_{2}\right)>0$ & $H_{1}:\left(p_{1}-p_{2}\right)\neq 0$\\ 
$H_{1}:\left(p_{1}-p_{2}\right)<0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}}=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{pq}{n_{1}}+\frac{pq}{n_{2}}}}
\end{eqnarray*}
donde $\hat{p_{1}}=x_{1}/n_{1}$ y $\hat{p_{2}}=x_{2}/n_{2}$ , dado que el valor com\'un para $p_{1}$ y $p_{2}$ es $p$, entonces $\hat{p}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$ y por tanto el estad\'istico de prueba es
\begin{eqnarray*}
z=\frac{\hat{p}_{1}-\hat{p}_{2}}{\sqrt{\hat{p}\hat{q}}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}
\end{eqnarray*}
\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{\alpha}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}



%----------------------------------------------------------------
\section{Muestras Peque\~nas}
%----------------------------------------------------------------
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection*{Una media poblacional}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu=\mu_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu>\mu_{0}$ & $H_{1}:\mu\neq \mu_{0}$\\ 
$H_{1}:\mu<\mu0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
t=\frac{\overline{x}-\mu_{0}}{\sqrt{\frac{s^{2}}{n}}}
\end{eqnarray*}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection*{Diferencia entre dos medias poblacionales: Muestras Aleatorias Independientes}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
Cuando los tama\ ~nos de muestra son peque\ ~nos, no se puede asegurar que las medias muestrales sean normales, pero si las poblaciones originales son normales, entonces la distribuci\'on muestral de la diferencia de las medias muestales, $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, ser\'a normal con media $\left(\mu_{1}-\mu_{2}\right)$ y error est\'andar $$ES=\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}$$


\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}

\item[3) ] Estad\'istico de prueba:
$$t=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}$$


donde $$s^{2}=\frac{\left(n_{1}-1\right)s_{1}^{2}+\left(n_{2}-1\right)s_{2}^{2}}{n_{1}+n_{2}-2}$$

\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.
\end{itemize}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection*{Diferencia entre dos medias poblacionales: Diferencias Pareadas}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu_{d}=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:\mu_{d}$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu_{d}>0$ & $H_{1}:\mu_{d}\neq 0$\\ 
$H_{1}:\mu_{d}<0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
t=\frac{\overline{d}}{\sqrt{\frac{s_{d}^{2}}{n}}}
\end{eqnarray*}
donde $n$ es el n\'umero de diferencias pareadas, $\overline{d}$ es la media de las diferencias muestrales, y $s_{d}$ es la desviaci\'on est\'andar de las diferencias muestrales.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}

Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.
\end{itemize}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection*{Inferencias con respecto a la Varianza Poblacional}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\sigma^{2}=\sigma^{2}_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\sigma^{2}>\sigma^{2}_{0}$ & $H_{1}:\sigma^{2}\neq \sigma^{2}_{0}$\\ 
$H_{1}:\sigma^{2}<\sigma^{2}_{0}$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
\chi^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}_{0}}
\end{eqnarray*}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$\chi^{2}>\chi^{2}_{\alpha}$ & \\
$\chi^{2}<\chi^{2}_{\left(1-\alpha\right)}$ cuando $H_{1}:\chi^{2}<\chi^{2}_{0}$&$\chi^{2}>\chi^{2}_{\alpha/2}$ o $\chi^{2}<\chi^{2}_{\left(1-\alpha/2\right)}$\\
 cuando $p<\alpha$&\\
\end{tabular}

Los valores cr\'iticos de $\chi^{2}$,est\'an basados en $\left(n_{1}+\right)$ grados de libertad.

\end{itemize}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection*{Comparaci\'on de dos varianzas poblacionales}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)>D_{0}$ & $H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)<D_{0}$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
$$F=\frac{s_{1}^{2}}{s_{2}^{2}}$$
donde $s_{1}^{2}$ es la varianza muestral m\'as grande.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$F>F_{\alpha}$ & $F>F_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}


\section{Introducci\'on}

La regresi\'on log\'istica es una t\'ecnica de modelado estad\'istico utilizada para predecir la probabilidad de un evento binario en funci\'on de una o m\'as variables independientes. Este cap\'itulo profundiza en las matem\'aticas subyacentes a la regresi\'on log\'istica, incluyendo la funci\'on log\'istica, la funci\'on de verosimilitud, y los m\'etodos para estimar los coeficientes del modelo.

\section{Funci\'on Log\'istica}

La funci\'on log\'istica es la base de la regresi\'on log\'istica. Esta funci\'on transforma una combinaci\'on lineal de variables independientes en una probabilidad.

\subsection{Definici\'on}

La funci\'on log\'istica se define como:
\begin{eqnarray*}
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}
\end{eqnarray*}
donde $p$ es la probabilidad de que el evento ocurra, $\beta_0, \beta_1, \ldots, \beta_n$ son los coeficientes del modelo, y $X_1, X_2, \ldots, X_n$ son las variables independientes.

\subsection{Propiedades}

La funci\'on log\'istica tiene varias propiedades importantes:
\begin{itemize}
    \item \textbf{Rango}: La funci\'on log\'istica siempre produce un valor entre 0 y 1, lo que la hace adecuada para modelar probabilidades.
    \item \textbf{Monoton\'ia}: La funci\'on es mon\'otona creciente, lo que significa que a medida que la combinaci\'on lineal de variables independientes aumenta, la probabilidad tambi\'en aumenta.
    \item \textbf{Simetr\'ia}: La funci\'on log\'istica es sim\'etrica en torno a $p = 0.5$.
\end{itemize}

\section{Funci\'on de Verosimilitud}

La funci\'on de verosimilitud se utiliza para estimar los coeficientes del modelo de regresi\'on log\'istica. Esta funci\'on mide la probabilidad de observar los datos dados los coeficientes del modelo.

\subsection{Definici\'on}

Para un conjunto de $n$ observaciones, la funci\'on de verosimilitud $L$ se define como el producto de las probabilidades individuales de observar cada dato:
\begin{eqnarray*}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{eqnarray*}
donde $y_i$ es el valor observado de la variable dependiente para la $i$-\'esima observaci\'on y $p_i$ es la probabilidad predicha de que $Y_i = 1$.

\subsection{Funci\'on de Log-Verosimilitud}

Para simplificar los c\'alculos, trabajamos con el logaritmo de la funci\'on de verosimilitud, conocido como la funci\'on de log-verosimilitud. Tomar el logaritmo convierte el producto en una suma:
\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{eqnarray*}

Sustituyendo $p_i$:
\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{eqnarray*}

\section{Estimaci\'on de Coeficientes}

Los coeficientes del modelo de regresi\'on log\'istica se estiman maximizando la funci\'on de log-verosimilitud. Este proceso generalmente se realiza mediante m\'etodos iterativos como el algoritmo de Newton-Raphson.

\subsection{Gradiente y Hessiana}

Para maximizar la funci\'on de log-verosimilitud, necesitamos calcular su gradiente y su matriz Hessiana.

\subsubsection{Gradiente}

El gradiente de la funci\'on de log-verosimilitud con respecto a los coeficientes $\beta$ es:
\begin{eqnarray*}
\mathbf{g}(\beta) = \frac{\partial \log L}{\partial \beta} = \sum_{i=1}^{n} \mathbf{X}_i (y_i - p_i)
\end{eqnarray*}
donde $\mathbf{X}_i$ es el vector de valores de las variables independientes para la $i$-\'esima observaci\'on.

\subsubsection{Hessiana}

La matriz Hessiana de la funci\'on de log-verosimilitud con respecto a los coeficientes $\beta$ es:
\begin{eqnarray*}
\mathbf{H}(\beta) = \frac{\partial^2 \log L}{\partial \beta \partial \beta^T} = -\sum_{i=1}^{n} p_i (1 - p_i) \mathbf{X}_i \mathbf{X}_i^T
\end{eqnarray*}

\subsection{Algoritmo Newton-Raphson}

El algoritmo Newton-Raphson se utiliza para encontrar los valores de los coeficientes que maximizan la funci\'on de log-verosimilitud. El algoritmo se puede resumir en los siguientes pasos:
\begin{enumerate}
    \item Inicializar el vector de coeficientes $\beta^{(0)}$ (por ejemplo, con ceros o valores peque\~nos aleatorios).
    \item Calcular el gradiente $\mathbf{g}(\beta^{(k)})$ y la matriz Hessiana $\mathbf{H}(\beta^{(k)})$ en la iteraci\'on $k$.
    \item Actualizar los coeficientes utilizando la f\'ormula:
    \begin{eqnarray*}
    \beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
    \end{eqnarray*}
    \item Repetir los pasos 2 y 3 hasta que la diferencia entre $\beta^{(k+1)}$ y $\beta^{(k)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}

\section{Validaci\'on del Modelo}

Una vez que se han estimado los coeficientes del modelo de regresi\'on log\'istica, es importante validar el modelo para asegurarse de que proporciona predicciones precisas.

\subsection{Curva ROC y AUC}

La curva ROC (Receiver Operating Characteristic) es una herramienta gr\'afica utilizada para evaluar el rendimiento de un modelo de clasificaci\'on binaria. El \'area bajo la curva (AUC) mide la capacidad del modelo para distinguir entre las clases.

\subsection{Matriz de Confusi\'on}

La matriz de confusi\'on es una tabla que resume el rendimiento de un modelo de clasificaci\'on al comparar las predicciones del modelo con los valores reales. Los t\'erminos en la matriz de confusi\'on incluyen verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos.





\section{Conceptos Básicos}

La regresión logística es una técnica de modelado estadístico utilizada para predecir la probabilidad de un evento binario (es decir, un evento que tiene dos posibles resultados) en función de una o más variables independientes. Es ampliamente utilizada en diversas disciplinas, como medicina, economía, biología, y ciencias sociales, para analizar y predecir resultados binarios.  Un modelo de regresión logística describe cómo una variable dependiente binaria $Y$ (que puede tomar los valores $0$ o $1$) está relacionada con una o más variables independientes $X_1, X_2, \ldots, X_n$. A diferencia de la regresión lineal, que predice un valor continuo, la regresión logística predice una probabilidad que puede ser interpretada como la probabilidad de que $Y=1$ dado un conjunto de valores para $X_1, X_2, \ldots, X_n$.

\section{Regresión Lineal}

La regresión lineal es utilizada para predecir el valor de una variable dependiente continua en función de una o más variables independientes. El modelo de regresión lineal tiene la forma:
\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
\end{equation}
donde:
\begin{itemize}
    \item $Y$ es la variable dependiente.
    \item $\beta_0$ es la intersección con el eje $Y$ o término constante.
    \item $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes que representan la relación entre las variables independientes y la variable dependiente.
    \item $X_1, X_2, \ldots, X_n$ son las variables independientes.
    \item $\epsilon$ es el término de error, que representa la desviación de los datos observados de los valores predichos por el modelo.
\end{itemize}

%\subsection*{Mínimos Cuadrados Ordinarios (OLS)}

El objetivo de la regresión lineal es encontrar los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ que minimicen la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos. Este método se conoce como mínimos cuadrados ordinarios (OLS, por sus siglas en inglés). La función de costo a minimizar es:
\begin{equation}
J\left(\beta_0, \beta_1, \ldots, \beta_n\right) = \sum_{i=1}^{n}\left(y_i - \hat{y}_i\right)^2
\end{equation}
donde:
\begin{itemize}
    \item $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación.
    \item $\hat{y}_i$ es el valor predicho por el modelo para la $i$-ésima observación, dado por:
    \begin{equation}
    \hat{y}_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_n x_{in}
    \end{equation}
\end{itemize}

Para encontrar los valores óptimos de los coeficientes, se toman las derivadas parciales de la función de costo con respecto a cada coeficiente y se igualan a cero:
\begin{equation}
\frac{\partial J}{\partial \beta_j} = 0 \quad \text{para } j = 0, 1, \ldots, n
\end{equation}

Resolviendo este sistema de ecuaciones, se obtienen los valores de los coeficientes que minimizan la función de costo.

\section{Regresión Logística}

La deducción de la fórmula de la regresión logística comienza con la necesidad de modelar la probabilidad de un evento binario. Queremos encontrar una función que relacione las variables independientes con la probabilidad de que la variable dependiente tome el valor $1$. La probabilidad de que el evento ocurra, $P(Y=1)$, se denota como $p$. La probabilidad de que el evento no ocurra, $P(Y=0)$, es $1-p$. Los \textit{odds} (chances) de que ocurra el evento se definen como:
\begin{equation}
\text{odds} = \frac{p}{1-p}
\end{equation}
Los \textit{odds} nos indican cuántas veces más probable es que ocurra el evento frente a que no ocurra. Para simplificar el modelado de los \textit{odds}, aplicamos el logaritmo natural, obteniendo la función logit:
\begin{equation}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right)
\end{equation}
La transformación logit es útil porque convierte el rango de la probabilidad (0, 1) al rango de números reales $\left(-\infty, \infty\right)$. La idea clave de la regresión logística es modelar la transformación logit de la probabilidad como una combinación lineal de las variables independientes:
\begin{equation}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{equation}
Aquí, $\beta_0$ es el t\'ermino constante y $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes asociados con las variables independientes $X_1, X_2, \ldots, X_n$. Para expresar $p$ en función de una combinación lineal de las variables independientes, invertimos la transformación logit. Partimos de la ecuación:
\begin{eqnarray*}
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{eqnarray*}
Aplicamos la exponenciación a ambos lados:
\begin{eqnarray*}
\frac{p}{1-p} = e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}
\end{eqnarray*}
Despejamos $p$:
\begin{eqnarray*}
p = \frac{e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}{1 + e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}
\end{eqnarray*}

%\subsection*{Función Logística}

La expresión final que obtenemos es conocida como la función logística:
\begin{equation}\label{Eq.Logit1}
p = \frac{1}{1 + e^{-\left(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n\right)}}
\end{equation}
Esta función describe cómo las variables independientes se relacionan con la probabilidad de que el evento de interés ocurra. Los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ se estiman a partir de los datos utilizando el método de máxima verosimilitud.

\section{Método de Máxima Verosimilitud}

Para estimar los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ en la regresión logística, utilizamos el método de máxima verosimilitud. La idea es encontrar los valores de los coeficientes que maximicen la probabilidad de observar los datos dados. Esta probabilidad se expresa mediante la función de verosimilitud $L$. La función de verosimilitud $L(\beta_0, \beta_1, \ldots, \beta_n)$ para un conjunto de $n$ observaciones se define como el producto de las probabilidades de las observaciones dadas las variables independientes:

\begin{equation}\label{Eq.Verosimilitud}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{equation}

donde:
\begin{itemize}
    \item $p_i$ es la probabilidad predicha de que $Y_i = 1$,
    \item $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación.
\end{itemize}

%\subsection{Función de Log-Verosimilitud}

Trabajar directamente con esta función de verosimilitud puede ser complicado debido al producto de muchas probabilidades, especialmente si $n$ es grande. Para simplificar los cálculos, se utiliza el logaritmo de la función de verosimilitud, conocido como la función de log-verosimilitud. El uso del logaritmo simplifica significativamente la diferenciación y maximización de la función. La función de log-verosimilitud se define como:

\begin{equation}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}

Aquí, $\log$ representa el logaritmo natural. Esta transformación es válida porque el logaritmo es una función monótona creciente, lo que significa que maximizar la log-verosimilitud es equivalente a maximizar la verosimilitud original. En la regresión logística, la probabilidad $p_i$ está dada por la función logística:

\begin{equation}
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}}
\end{equation}

Sustituyendo esta expresión en la función de log-verosimilitud, obtenemos:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &= \sum_{i=1}^{n} \left[ y_i \log \left( \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}} \right) + \right. \nonumber \\
& \quad \left. (1 - y_i) \log \left( 1 - \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}} \right) \right]
\end{eqnarray*}

Simplificando esta expresión, notamos que:

\begin{eqnarray*}
\log \left( \frac{1}{1 + e^{-z}} \right) = -\log(1 + e^{-z})
\end{eqnarray*}

y

\begin{eqnarray*}
\log \left( 1 - \frac{1}{1 + e^{-z}} \right) = \log \left( \frac{e^{-z}}{1 + e^{-z}} \right) = -z - \log(1 + e^{-z})
\end{eqnarray*}

Aplicando estas identidades, la función de log-verosimilitud se convierte en:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &=& \sum_{i=1}^{n} \left[ y_i (-\log(1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})})) + \right. \nonumber \\
&& \quad \left. (1 - y_i) \left( -(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}) \right) \right]
\end{eqnarray*}

Simplificando aún más, obtenemos:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &=& \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})\right.\\
& -&\left. \log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{eqnarray*}


Para simplificar aún más la notación, podemos utilizar notación matricial. Definimos la matriz $\mathbf{X}$ de tamaño $n \times (k+1)$ y el vector de coeficientes $\boldsymbol{\beta}$ de tamaño $(k+1) \times 1$ como sigue:

\begin{equation}\label{Eq.Matricial1}
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1k} \\
1 & x_{21} & x_{22} & \ldots & x_{2k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nk}
\end{bmatrix}, \quad
\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix}
\end{equation}

Entonces, la expresión para la función de log-verosimilitud es:

\begin{equation}\label{Eq.LogLikelihood1}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i (\mathbf{X}_i \boldsymbol{\beta}) - \log(1 + e^{\mathbf{X}_i \boldsymbol{\beta}}) \right]
\end{equation}

donde $\mathbf{X}_i$ es la $i$-ésima fila de la matriz $\mathbf{X}$.  Esta notación matricial simplifica la implementación y la derivación de los estimadores de los coeficientes en la regresión logística. Utilizando métodos numéricos, como el algoritmo de Newton-Raphson, se pueden encontrar los coeficientes que maximizan la función de log-verosimilitud. Para maximizar la función de log-verosimilitud, derivamos esta función con respecto a cada uno de los coeficientes $\beta_j$ y encontramos los puntos críticos. La derivada parcial de la función de log-verosimilitud con respecto a $\beta_j$ es:

\begin{eqnarray}\label{Eq.1.14}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} \left[ y_i X_{ij} - \frac{X_{ij} e^{\mathbf{X}_i \boldsymbol{\beta}}}{1 + e^{\mathbf{X}_i \boldsymbol{\beta}}} \right]
\end{eqnarray}

Simplificando, esta derivada se puede expresar como:

\begin{eqnarray}\label{Eq.PrimeraDerivada}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} X_{ij} (y_i - p_i),\textrm{ donde }p_i = \frac{1}{1 + e^{-\mathbf{X}_i \boldsymbol{\beta}}}
\end{eqnarray}

Para encontrar los coeficientes que maximizan la log-verosimilitud, resolvemos el sistema de ecuaciones 
\begin{eqnarray*}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = 0 \textrm{ para todos los }j = 0, 1, \ldots, k. 
\end{eqnarray*}
Este sistema de ecuaciones no tiene una solución analítica cerrada, por lo que se resuelve numéricamente utilizando métodos iterativos como el algoritmo de Newton-Raphson.

\section{Método de Newton-Raphson}

El método de Newton-Raphson es un algoritmo iterativo que se utiliza para encontrar las raíces de una función. En el contexto de la regresión logística, se utiliza para maximizar la función de log-verosimilitud encontrando los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$. Este m\'etodo se basa en una aproximación de segundo orden de la función objetivo. Dado un valor inicial de los coeficientes $\boldsymbol{\beta}^{(0)}$, se actualiza iterativamente el valor de los coeficientes utilizando la fórmula:

\begin{equation}\label{Eq.Criterio0}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \mathbf{H}(\boldsymbol{\beta}^{(t)}) \right]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
\end{equation}

donde:
\begin{itemize}
    \item $\boldsymbol{\beta}^{(t)}$ es el vector de coeficientes en la $t$-ésima iteración.
    \item $\nabla \log L(\boldsymbol{\beta}^{(t)})$ es el gradiente de la función de log-verosimilitud con respecto a los coeficientes $\boldsymbol{\beta}$:

\begin{equation}\label{Eq.Gradiente1}
\nabla \log L(\boldsymbol{\beta}) = \mathbf{X}^T (\mathbf{y} - \mathbf{p})
\end{equation}

donde $\mathbf{y}$ es el vector de valores observados y $\mathbf{p}$ es el vector de probabilidades.
    \item $\mathbf{H}(\boldsymbol{\beta}^{(t)})$ es la matriz Hessiana (matriz de segundas derivadas) evaluada en $\boldsymbol{\beta}^{(t)}$:
\begin{equation}\label{Eq.Hessiana1}
\mathbf{H}(\boldsymbol{\beta}) = -\mathbf{X}^T \mathbf{W} \mathbf{X}
\end{equation}

donde $\mathbf{W}$ es una matriz diagonal de pesos con elementos $w_i = p_i (1 - p_i)$.

\end{itemize}

En resumen:

\begin{Algthm}\label{Algoritmo1}
El algoritmo Newton-Raphson para la regresión logística se puede resumir en los siguientes pasos:
\begin{enumerate}
    \item Inicializar el vector de coeficientes $\boldsymbol{\beta}^{(0)}$ (por ejemplo, con ceros o valores pequeños aleatorios).
    \item Calcular el gradiente $\nabla \log L(\boldsymbol{\beta}^{(t)})$ y la matriz Hessiana $\mathbf{H}(\boldsymbol{\beta}^{(t)})$ en la iteración $t$.
    \item Actualizar los coeficientes utilizando la fórmula:
    \begin{equation}\label{Eq.Criterio1}
    \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \mathbf{H}(\boldsymbol{\beta}^{(t)}) \right]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
    \end{equation}
    \item Repetir los pasos 2 y 3 hasta que la diferencia entre $\boldsymbol{\beta}^{(t+1)}$ y $\boldsymbol{\beta}^{(t)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}
\end{Algthm}

En resumen, el método de Newton-Raphson permite encontrar los coeficientes que maximizan la función de log-verosimilitud de manera eficiente. 

\section{Espec\'ificando}
En espec\'ifico para un conjunto de $n$ observaciones, la función de verosimilitud $L$ se define como el producto de las probabilidades individuales de observar cada dato:
\begin{equation}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{equation}
donde $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación y $p_i$ es la probabilidad predicha de que $Y_i = 1$. Aquí, $p_i$ es dado por la función logística:
\begin{equation}
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}}
\end{equation}

Tomando el logaritmo:
\begin{equation}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}

Sustituyendo $p_i$:
\begin{equation}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{equation}

Dado que el objetivo es encontrar los valores de $\beta_0, \beta_1, \ldots, \beta_n$ que maximicen la función de log-verosimilitud.  Para $\beta_j$, la derivada parcial de la función de log-verosimilitud es:
\begin{equation}
\frac{\partial \log L}{\partial \beta_j} = \sum_{i=1}^{n} \left[ y_i X_{ij} - \frac{X_{ij} e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}}{1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}} \right]
\end{equation}

Esto se simplifica a (comparar con la ecuaci\'on \ref{Eq.1.14}):
\begin{eqnarray}\label{Eq.1.25}
\frac{\partial \log L}{\partial \beta_j} = \sum_{i=1}^{n} X_{ij} (y_i - p_i)
\end{eqnarray}


Para maximizar la log-verosimilitud, resolvemos el sistema de ecuaciones $\frac{\partial \log L}{\partial \beta_j} = 0$ para todos los $j$ de 0 a $n$., mismo que se resuelve numéricamente utilizando métodos el algoritmo de Newton-Raphson. El método de Newton-Raphson se basa en una aproximación de segundo orden de la función objetivo. Dado un valor inicial de los coeficientes $\beta^{(0)}$, se iterativamente actualiza el valor de los coeficientes utilizando la fórmula:
\begin{equation}\label{Eq.Criterio1.5}
\beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
\end{equation}
donde:
\begin{itemize}
    \item $\beta^{(k)}$ es el vector de coeficientes en la $k$-ésima iteración.
    \item $\mathbf{g}(\beta^{(k)})$ es el gradiente (vector de primeras derivadas) evaluado en $\beta^{(k)}$:
\begin{equation}\label{Eq.Gradiente2}
\mathbf{g}(\beta) = \frac{\partial \log L}{\partial \beta} = \sum_{i=1}^{n} \mathbf{X}_i (y_i - p_i)
\end{equation}
donde $\mathbf{X}_i$ es el vector de valores de las variables independientes para la $i$-ésima observación (comparar con ecuaci\'on \ref{Eq.Gradiente1}).

    \item $\mathbf{H}(\beta^{(k)})$ es la matriz Hessiana (matriz de segundas derivadas) evaluada en $\beta^{(k)}$:
\begin{equation}\label{Eq.Hessiana2}
\mathbf{H}(\beta) = \frac{\partial^2 \log L}{\partial \beta \partial \beta^T} = -\sum_{i=1}^{n} p_i (1 - p_i) \mathbf{X}_i \mathbf{X}_i^T,
\end{equation}
comparar con ecuaci\'on \ref{Eq.Hessiana1}
\end{itemize}

\begin{Algthm} \label{Algoritmo2}
Los pasos del algoritmo Newton-Raphson para la regresión logística son:
\begin{enumerate}
    \item Inicializar el vector de coeficientes $\beta^{(0)}$ (por ejemplo, con ceros o valores pequeños aleatorios).
    \item Calcular el gradiente $\mathbf{g}(\beta^{(k)})$ y la matriz Hessiana $\mathbf{H}(\beta^{(k)})$ en la iteración $k$.
    \item Actualizar los coeficientes utilizando la fórmula:
    \begin{equation}\label{Eq.Criterio2}
    \beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
    \end{equation}
    \item Repetir los pasos 2 y 3 hasta que la diferencia entre $\beta^{(k+1)}$ y $\beta^{(k)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}
\end{Algthm}
Como se puede observar la diferencia entre el Algoritmo \ref{Algoritmo1} y el Algoritmo \ref{Algoritmo2} son m\'inimas

\section*{Notas finales}

En el contexto de la regresión logística, los vectores $X_1, X_2, \ldots, X_n$ representan las variables independientes. Cada $X_j$ es un vector columna que contiene los valores de la variable independiente $j$ para cada una de las $n$ observaciones. Es decir,

\begin{equation}
X_j = \begin{bmatrix}
x_{1j} \\
x_{2j} \\
\vdots \\
x_{nj}
\end{bmatrix}
\end{equation}

Para simplificar la notación y los cálculos, a menudo combinamos todos los vectores de variables independientes en una única matriz de diseño $\mathbf{X}$ de tamaño $n \times (k+1)$, donde $n$ es el número de observaciones y $k+1$ es el número de variables independientes más el término de intercepto. La primera columna de $\mathbf{X}$ corresponde a un vector de unos para el término de intercepto, y las demás columnas corresponden a los valores de las variables independientes:

\begin{equation}
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1k} \\
1 & x_{21} & x_{22} & \ldots & x_{2k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nk}
\end{bmatrix}
\end{equation}
revisar la ecuaci\'on \ref{Eq.Matricial1}. De esta forma, el modelo logit puede ser escrito de manera compacta utilizando la notación matricial:

\begin{equation}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \mathbf{X} \boldsymbol{\beta}
\end{equation}

donde $\boldsymbol{\beta}$ es el vector de coeficientes:

\begin{equation}
\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix}
\end{equation}

Así, la probabilidad $p$ se puede expresar como:

\begin{equation}\label{Eq.Logit2}
p = \frac{1}{1 + e^{-\mathbf{X} \boldsymbol{\beta}}}
\end{equation}

Comparar la ecuaci\'on anterior con la ecuaci\'on \ref{Eq.Logit1}. Esta notación matricial simplifica la implementación y la derivación de los estimadores de los coeficientes en la regresión logística. Para estimar los coeficientes $\boldsymbol{\beta}$ en la regresión logística, se utiliza el método de máxima verosimilitud. La función de verosimilitud $L(\boldsymbol{\beta})$ se define como el producto de las probabilidades de las observaciones dadas las variables independientes, recordemos la ecuaci\'on \ref{Eq.Verosimilitud}:

\begin{eqnarray}
L(\boldsymbol{\beta}) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{eqnarray}


donde $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación, y $p_i$ es la probabilidad predicha de que $Y_i = 1$.  La función de log-verosimilitud, que es más fácil de maximizar, se obtiene tomando el logaritmo natural de la función de verosimilitud (\ref{Eq.LogLikelihood1}):

\begin{equation}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}

Sustituyendo $p_i = \frac{1}{1 + e^{-\mathbf{X}_i \boldsymbol{\beta}}}$, donde $\mathbf{X}_i$ es la $i$-ésima fila de la matriz de diseño $\mathbf{X}$, obtenemos:

\begin{equation}\label{Eq.LogLikelihood2}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i (\mathbf{X}_i \boldsymbol{\beta}) - \log(1 + e^{\mathbf{X}_i \boldsymbol{\beta}}) \right]
\end{equation}

Para encontrar los valores de $\boldsymbol{\beta}$ que maximizan la función de log-verosimilitud, se utiliza un algoritmo iterativo como el método de Newton-Raphson. Este método requiere calcular el gradiente y la matriz Hessiana de la función de log-verosimilitud.


El gradiente de la función de log-verosimilitud con respecto a $\boldsymbol{\beta}$ es (\ref{Eq.Gradiente1} y \ref{Eq.Gradiente2}):

\begin{equation}
\nabla \log L(\boldsymbol{\beta}) = \mathbf{X}^T (\mathbf{y} - \mathbf{p})
\end{equation}

donde $\mathbf{y}$ es el vector de valores observados y $\mathbf{p}$ es el vector de probabilidades predichas.

La matriz Hessiana de la función de log-verosimilitud es (\ref{Eq.Hessiana1} y \ref{Eq.Hessiana2}):

\begin{equation}
\mathbf{H}(\boldsymbol{\beta}) = -\mathbf{X}^T \mathbf{W} \mathbf{X}
\end{equation}

donde $\mathbf{W}$ es una matriz diagonal de pesos con elementos $w_i = p_i (1 - p_i)$.

El método de Newton-Raphson actualiza los coeficientes $\boldsymbol{\beta}$ de la siguiente manera:

\begin{equation}\label{Eq.Criterio3}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - [\mathbf{H}(\boldsymbol{\beta}^{(t)})]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
\end{equation}

Iterando este proceso hasta que la diferencia entre $\boldsymbol{\beta}^{(t+1)}$ y $\boldsymbol{\beta}^{(t)}$ sea menor que un umbral predefinido (\ref{Eq.Criterio0}, \ref{Eq.Criterio1}, \ref{Eq.Criterio1.5} y \ref{Eq.Criterio2}), se obtienen los estimadores de máxima verosimilitud para los coeficientes de la regresión logística.


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Introducci\'on}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Introducci\'on}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

La Estad\'istica es una ciencia formal que estudia la recolecci\'on, an\'alisis e interpretaci\'on de datos de una muestra representativa, ya sea para ayudar en la toma de decisiones o para explicar condiciones regulares o irregulares de alg\'un fen\'omeno o estudio aplicado, de ocurrencia en forma aleatoria o condicional. Sin embargo, la estad\'istica es m\'as que eso, es decir, es transversal a una amplia variedad de disciplinas, desde la f\'isica hasta las ciencias sociales, desde las ciencias de la salud hasta el control de calidad. Se usa para la toma de decisiones en \'areas de negocios o instituciones gubernamentales. Ahora bien, las t\'ecnicas estad\'isticas se aplican de manera amplia en mercadotecnia, contabilidad, control de calidad y en otras actividades; estudios de consumidores; an\'alisis de resultados en deportes; administradores de instituciones; en la educaci\'on; organismos pol\'iticos; m\'edicos; y por otras personas que intervienen en la toma de decisiones.

\begin{Def}
    La Estad\'istica es la ciencia cuyo objetivo es reunir una informaci\'on cuantitativa concerniente a individuos, grupos, series de hechos, etc. y deducir de ello gracias al an\'alisis de estos datos unos significados precisos o unas previsiones para el futuro.
\end{Def}

La estad\'istica, en general, es la ciencia que trata de la recopilaci\'on, organizaci\'on presentaci\'on, an\'alisis e interpretaci\'on de datos num\'ericos con el fin de realizar una toma de decisi\'on m\'as efectiva. Los m\'etodos estad\'isticos tradicionalmente se utilizan para prop\'ositos descriptivos, para organizar y resumir datos num\'ericos. La estad\'istica descriptiva, por ejemplo trata de la tabulaci\'on de datos, su presentaci\'on en forma gr\'afica o ilustrativa y el c\'alculo de medidas descriptivas.

%---------------------------------------------------------
\subsection{Historia de la Estad\'istica}
%---------------------------------------------------------
Es dif\'icil conocer los or\'igenes de la Estad\'istica. Desde los comienzos de la civilizaci\'on han existido formas sencillas de estad\'istica, pues ya se utilizaban representaciones gr\'aficas y otros s\'imbolos en pieles, rocas, palos de madera y paredes de cuevas para contar el n\'umero de personas, animales o ciertas cosas. Su origen empieza posiblemente en la isla de Cerde\~na, donde existen monumentos prehist\'oricos pertenecientes a los Nuragas, las primeros habitantes de la isla; estos monumentos constan de bloques de basalto superpuestos sin mortero y en cuyas paredes de encontraban grabados toscos signos que han sido interpretados con mucha verosimilidad como muescas que serv\'ian para llevar la cuenta del ganado y la caza. Los babilonios usaban ya peque\~nas tablillas de arcilla para recopilar datos en tablas sobre la producci\'on agr\'icola y los g\'eneros vendidos o cambiados mediante trueque. Otros vestigios pueden ser hallados en el antiguo Egipto, cuyos faraones lograron recopilar, hacia el a\~no 3050 antes de Cristo, prolijos datos relativos a la poblaci\'on y la riqueza del pa\'is. De acuerdo al historiador griego Her\'odoto, dicho registro de riqueza y poblaci\'on se hizo con el objetivo de preparar la construcci\'on de las pir\'amides. En el mismo Egipto, Rams\'es II hizo un censo de las tierras con el objeto de verificar un nuevo reparto. En el antiguo Israel la Biblia da referencias, en el libro de los N\'umeros, de los datos estad\'isticos obtenidos en dos recuentos de la poblaci\'on hebrea. El rey David por otra parte, orden\'o a Joab, general del ej\'ercito hacer un censo de Israel con la finalidad de conocer el n\'umero de la poblaci\'on. Tambi\'en los chinos efectuaron censos hace m\'as de cuarenta siglos. Los griegos efectuaron censos peri\'odicamente con fines tributarios, sociales (divisi\'on de tierras) y militares (c\'alculo de recursos y hombres disponibles). La investigaci\'on hist\'orica revela que se realizaron 69 censos para calcular los impuestos, determinar los derechos de voto y ponderar la potencia guerrera. \medskip

Fueron los romanos, maestros de la organizaci\'on pol\'itica, quienes mejor supieron emplear los recursos de la estad\'istica. Cada cinco a\~nos realizaban un censo de la poblaci\'on y sus funcionarios p\'ublicos ten\'ian la obligaci\'on de anotar nacimientos, defunciones y matrimonios, sin olvidar los recuentos peri\'odicos del ganado y de las riquezas contenidas en las tierras conquistadas. Para el nacimiento de Cristo suced\'ia uno de estos empadronamientos de la poblaci\'on bajo la autoridad del imperio. Durante los mil a\~nos siguientes a la ca\'ida del imperio Romano se realizaron muy pocas operaciones Estad\'isticas, con la notable excepci\'on de las relaciones de tierras pertenecientes a la Iglesia, compiladas por Pipino el Breve en el 758 y por Carlomagno en el 762 DC. Durante el siglo IX se realizaron en Francia algunos censos parciales de siervos. En Inglaterra, Guillermo el Conquistador recopil\'o el Domesday Book o libro del Gran Catastro para el a\~no 1086, un documento de la propiedad, extensi\'on y valor de las tierras de Inglaterra. Esa obra fue el primer compendio estad\'istico de Inglaterra.  Aunque Carlomagno, en Francia; y Guillermo el Conquistador, en Inglaterra, trataron de revivir la t\'ecnica romana, los m\'etodos estad\'isticos permanecieron casi olvidados durante la Edad Media.


Durante los siglos XV, XVI, y XVII, hombres como Leonardo de Vinci, Nicol\'as Cop\'ernico, Galileo, Neper, William Harvey, Sir Francis Bacon y Ren\'e Descartes, hicieron grandes operaciones al m\'etodo cient\'ifico, de tal forma que cuando se crearon los Estados Nacionales y surgi\'o como fuerza el comercio internacional exist\'ia ya un m\'etodo capaz de aplicarse a los datos econ\'omicos. Para el a\~no 1532 empezaron a registrarse en Inglaterra las defunciones debido al temor que Enrique VII ten\'ia por la peste.  M\'as o menos por la misma \'epoca, en Francia la ley exigi\'o a los cl\'erigos registrar los bautismos, fallecimientos y matrimonios. Durante un brote de peste que apareci\'o a fines de la d\'ecada de 1500, el gobierno ingl\'es comenz\'o a publicar estad\'istica semanales de los decesos. Esa costumbre continu\'o muchos a\~nos, y en 1632 estos Bills of Mortality (Cuentas de Mortalidad) conten\'ian los nacimientos y fallecimientos por sexo. En 1662, el capit\'an John Graunt us\'o documentos que abarcaban treinta a\~nos y efectu\'o predicciones sobre el n\'umero de personas que morir\'ian de varias enfermedades y sobre las proporciones de nacimientos de varones y mujeres que cabr\'ia esperar. El trabajo de Graunt, condensado en su obra \textit{Natural and Political Observations...Made upon the Bills of Mortality}, fue un esfuerzo innovador en el an\'alisis estad\'istico. Por el a\~no 1540 el alem\'an Sebasti\'an Muster realiz\'o una compilaci\'on estad\'istica de los recursos nacionales, comprensiva de datos sobre organizaci\'on pol\'itica, instrucciones sociales, comercio y poder\'io militar. 

Los eruditos del siglo XVII demostraron especial inter\'es por la Estad\'istica Demogr\'afica como resultado de la especulaci\'on sobre si la poblaci\'on aumentaba, decrec\'ia o permanec\'ia est\'atica. En los tiempos modernos tales m\'etodos fueron resucitados por algunos reyes que necesitaban conocer las riquezas monetarias y el potencial humano de sus respectivos pa\'ises. El primer empleo de los datos estad\'isticos para fines ajenos a la pol\'itica tuvo lugar en 1691 y estuvo a cargo de Gaspar Neumann, un profesor alem\'an que viv\'ia en Breslau. Este investigador se propuso destruir la antigua creencia popular de que en los a\~nos terminados en siete mor\'ia m\'as gente que en los restantes, y para lograrlo hurg\'o pacientemente en los archivos parroquiales de la ciudad. Despu\'es de revisar miles de partidas de defunci\'on pudo demostrar que en tales a\~nos no fallec\'ian m\'as personas que en los dem\'as. Los procedimientos de Neumann fueron conocidos por el astr\'onomo ingl\'es Halley, descubridor del cometa que lleva su nombre, quien los aplic\'o al estudio de la vida humana. 

Durante el siglo XVII y principios del XVIII, matem\'aticos como Bernoulli, Francis Maseres, Lagrange y Laplace desarrollaron la teor\'ia de probabilidades. No obstante durante cierto tiempo, la teor\'ia de las probabilidades limit\'o su aplicaci\'on a los juegos de azar y hasta el siglo XVIII no comenz\'o a aplicarse a los grandes problemas cient\'ificos. Godofredo Achenwall, profesor de la Universidad de Gotinga, acu\~n\'o en 1760 la palabra estad\'istica, que extrajo del t\'ermino italiano statista (estadista). Cre\'ia, y con sobrada raz\'on, que los datos de la nueva ciencia ser\'ian el aliado m\'as eficaz del gobernante consciente. La ra\'iz remota de la palabra se halla, por otra parte, en el t\'ermino latino status, que significa estado o situaci\'on; Esta etimolog\'ia aumenta el valor intr\'inseco de la palabra, por cuanto la estad\'istica revela el sentido cuantitativo de las m\'as variadas situaciones. Jacques Qu\'etelect es quien aplica las Estad\'isticas a las ciencias sociales. Este interpret\'o la teor\'ia de la probabilidad para su uso en las ciencias sociales y resolver la aplicaci\'on del principio de promedios y de la variabilidad a los fen\'omenos sociales. Qu\'etelect fue el primero en realizar la aplicaci\'on pr\'actica de todo el m\'etodo Estad\'istico, entonces conocido, a las diversas ramas de la ciencia. Entretanto, en el per\'iodo del 1800 al 1820 se desarrollaron dos conceptos matem\'aticos fundamentales para la teor\'ia Estad\'istica; la teor\'ia de los errores de observaci\'on, aportada por Laplace y Gauss; y la teor\'ia de los m\'inimos cuadrados desarrollada por Laplace, Gauss y Legendre. A finales del siglo XIX, Sir Francis Gaston ide\'o el m\'etodo conocido por Correlaci\'on, que ten\'ia por objeto medir la influencia relativa de los factores sobre las variables. De aqu\'i parti\'o el desarrollo del coeficiente de correlaci\'on creado por Karl Pearson y otros cultivadores de la ciencia biom\'etrica como J. Pease Norton, R. H. Hooker y G. Udny Yule, que efectuaron amplios estudios sobre la medida de las relaciones.

La historia de la estad\'istica est\'a resumida en tres grandes etapas o fases.

\begin{itemize}
    \item \textbf{Fase 1: Los Censos:} Desde el momento en que se constituye una autoridad pol\'itica, la idea de inventariar de una forma m\'as o menos regular la poblaci\'on y las riquezas existentes en el territorio est\'a ligada a la conciencia de soberan\'ia y a los primeros esfuerzos administrativos.
    \item \textbf{Fase 2: De la Descripci\'on de los Conjuntos a la Aritm\'etica Pol\'itica:} Las ideas mercantilistas extra\~nan una intensificaci\'on de este tipo de investigaci\'on. Colbert multiplica las encuestas sobre art\'iculos manufacturados, el comercio y la poblaci\'on: los intendentes del Reino env\'ian a Par\'is sus memorias. Vauban, m\'as conocido por sus fortificaciones o su Dime Royale, que es la primera propuesta de un impuesto sobre los ingresos, se se\~nala como el verdadero precursor de los sondeos. M\'as tarde, Buf\'on se preocupa de esos problemas antes de dedicarse a la historia natural. La escuela inglesa proporciona un nuevo progreso al superar la fase puramente descriptiva.

Sus tres principales representantes son Graunt, Petty y Halley. El pen\'ultimo es autor de la famosa Aritm\'etica Pol\'itica. Chaptal, ministro del interior franc\'es, publica en 1801 el primer censo general de poblaci\'on, desarrolla los estudios industriales, de las producciones y los cambios, haci\'endose sistem\'aticos durantes las dos terceras partes del siglo XIX.

\item \textbf{Fase 3: Estad\'istica y C\'alculo de Probabilidades:} El c\'alculo de probabilidades se incorpora r\'apidamente como un instrumento de an\'alisis extremadamente poderoso para el estudio de los fen\'omenos econ\'omicos y sociales y en general para el estudio de fen\'omenos cuyas causas son demasiados complejas para conocerlos totalmente y hacer posible su an\'alisis.

\end{itemize}


La Estad\'istica para su mejor estudio se ha dividido en dos grandes ramas: \textbf{la Estad\'istica Descriptiva y la Estad\'istica Inferencial}.

\begin{itemize}
    \item \textbf{Descriptiva:} consiste sobre todo en la presentaci\'on de datos en forma de tablas y gr\'aficas. Esta comprende cualquier actividad relacionada con los datos y est\'a dise\~nada para resumir o describir los mismos sin factores pertinentes adicionales; esto es, sin intentar inferir nada que vaya m\'as all\'a de los datos, como tales.
    \item \textbf{Inferencial:} se deriva de muestras, de observaciones hechas s\'olo acerca de una parte de un conjunto numeroso de elementos y esto implica que su an\'alisis requiere de generalizaciones que van m\'as all\'a de los datos. Como consecuencia, la caracter\'istica m\'as importante del reciente crecimiento de la estad\'istica ha sido un cambio en el \'enfasis de los m\'etodos que describen a m\'etodos que sirven para hacer generalizaciones. La Estad\'istica Inferencial investiga o analiza una poblaci\'on partiendo de una muestra tomada.
\end{itemize}

%---------------------------------------------------------
\subsection*{Estad\'istica Inferencial}
%---------------------------------------------------------

Los m\'etodos b\'asicos de la estad\'istica inferencial son la estimaci\'on y el contraste de hip\'otesis, que juegan un papel fundamental en la investigaci\'on. Por tanto, algunos de los objetivos que se persiguen son:

\begin{itemize}
    \item Calcular los par\'ametros de la distribuci\'on de medias o proporciones muestrales de tama\~no $n$, extra\'idas de una poblaci\'on de media y varianza conocidas.
    \item Estimar la media o la proporci\'on de una poblaci\'on a partir de la media o proporci\'on muestral.
    \item Utilizar distintos tama\~nos muestrales para controlar la confianza y el error admitido.
    \item Contrastar los resultados obtenidos a partir de muestras.
    \item Visualizar gr\'aficamente, mediante las respectivas curvas normales, las estimaciones realizadas.
\end{itemize}

En definitiva, la idea es, a partir de una poblaci\'on se extrae una muestra por algunos de los m\'etodos existentes, con la que se generan datos num\'ericos que se van a utilizar para generar estad\'isticos con los que realizar estimaciones o contrastes poblacionales. Existen dos formas de estimar par\'ametros: la \textit{estimaci\'on puntual} y la \textit{estimaci\'on por intervalo de confianza}. En la primera se busca, con base en los datos muestrales, un \'unico valor estimado para el par\'ametro. Para la segunda, se determina un intervalo dentro del cual se encuentra el valor del par\'ametro, con una probabilidad determinada.

Si el objetivo del tratamiento estad\'istico inferencial, es efectuar generalizaciones acerca de la estructura, composici\'on o comportamiento de las poblaciones no observadas, a partir de una parte de la poblaci\'on, ser\'a necesario que la proporci\'on de poblaci\'on examinada sea representativa del total. Por ello, la selecci\'on de la muestra requiere unos requisitos que lo garanticen, debe ser representativa y aleatoria. 

Adem\'as, la cantidad de elementos que integran la muestra (el tama\~no de la muestra) depende de m\'ultiples factores, como el dinero y el tiempo disponibles para el estudio, la importancia del tema analizado, la confiabilidad que se espera de los resultados, las caracter\'isticas propias del fen\'omeno analizado, etc\'etera. 

As\'i, a partir de la muestra seleccionada se realizan algunos c\'alculos y se estima el valor de los par\'ametros de la poblaci\'on tales como la media, la varianza, la desviaci\'on est\'andar, o la forma de la distribuci\'on, etc.


El conjunto de los m\'etodos que se utilizan para medir las caracter\'isticas de la informaci\'on, para resumir los valores individuales, y para analizar los datos a fin de extraerles el m\'aximo de informaci\'on, es lo que se llama \textit{m\'etodos estad\'isticos}. Los m\'etodos de an\'alisis para la informaci\'on cuantitativa se pueden dividir en los siguientes seis pasos:

\begin{itemize}
    \item Definici\'on del problema.
    \item Recopilaci\'on de la informaci\'on existente.
    \item Obtenci\'on de informaci\'on original.
    \item Clasificaci\'on.
    \item Presentaci\'on.
    \item An\'alisis.
\end{itemize}

El centro de gravedad de la metodolog\'ia estad\'istica se empieza a desplazar t\'ecnicas de computaci\'on intensiva aplicadas a grandes masas de datos, y se empieza a considerar el m\'etodo estad\'istico como un proceso iterativo de b\'usqueda del modelo ideal. Las aplicaciones en este periodo de la Estad\'istica a la Econom\'ia conducen a una disciplina con contenido propio: la Econometr\'ia. La investigaci\'on estad\'istica en problemas militares durante la segunda guerra mundial y los nuevos m\'etodos de programaci\'on matem\'atica, dan lugar a la Investigaci\'on Operativa. El tratamiento de los datos de la investigaci\'on cient\'ifica tiene varias etapas:

\begin{itemize}
    \item En la etapa de recolecci\'on de datos del m\'etodo cient\'ifico, se define a la poblaci\'on de inter\'es y se selecciona una muestra o conjunto de personas representativas de la misma, se realizan experimentos o se emplean instrumentos ya existentes o de nueva creaci\'on, para medir los atributos de inter\'es necesarios para responder a las preguntas de investigaci\'on. Durante lo que es llamado trabajo de campo se obtienen los datos en crudo, es decir las respuestas directas de los sujetos uno por uno, se codifican (se les asignan valores a las respuestas), se capturan y se verifican para ser utilizados en las siguientes etapas.
    \item En la etapa de recuento, se organizan y ordenan los datos obtenidos de la muestra. Esta ser\'a descrita en la siguiente etapa utilizando la estad\'istica descriptiva, todas las investigaciones utilizan estad\'istica descriptiva, para conocer de manera organizada y resumida las caracter\'isticas de la muestra.
    \item En la etapa de an\'alisis se utilizan las pruebas estad\'isticas (estad\'istica inferencial) y en la interpretaci\'on se acepta o rechaza la hip\'otesis nula.
\end{itemize}
%---------------------------------------------------------
\subsection*{Niveles de medici\'on y tipos de variables}
%---------------------------------------------------------
Para poder emplear el m\'etodo estad\'istico en un estudio es necesario medir las variables. 

\begin{itemize}
    \item Medir: es asignar valores a las propiedades de los objetos bajo ciertas reglas, esas reglas son los niveles de medici\'on.
    \item Cuantificar: es asignar valores a algo tomando un patr\'on de referencia. Por ejemplo, cuantificar es ver cu\'antos hombres y cu\'antas mujeres hay.
\end{itemize}

\textbf{Variable:} es una caracter\'istica o propiedad que asume diferentes valores dentro de una poblaci\'on de inter\'es y cuya variaci\'on es susceptible de medirse.

Las variables pueden clasificarse de acuerdo al tipo de valores que puede tomar como:

\begin{itemize}
\item \textbf{Discretas o categ\'oricas} en las que los valores se relacionan a nombres, etiquetas o categor\'ias, no existe un significado num\'erico directo.
\item \textbf{Continuas} los valores tienen un correlato num\'erico directo, son continuos y susceptibles de fraccionarse y de poder utilizarse en operaciones aritm\'eticas.
\item \textbf{Dicot\'omica} s\'olo tienen dos valores posibles, la caracter\'istica est\'a ausente o presente.
\end{itemize}

En cuanto a una clasificaci\'on estad\'istica, las varibles pueden ser:

\begin{itemize}
\item \textbf{Aleatoria} Aquella en la cual desconocemos el valor porque fluct\'ua de acuerdo a un evento debido al azar.
\item \textbf{Determin\'istica} Aquella variable de la que se conoce el valor.
\item \textbf{Independiente} aquellas variables que son manipuladas por el investigador. Define los grupos.
\item \textbf{Dependiente} son mediciones que ocurren durante el experimento o tratamiento (resultado de la independiente), es la que se mide y compara entre los grupos.
\end{itemize}

En lo que tiene que ver con los \textbf{Niveles de Medici\'on} tenemoss distintos tipos de variable

\begin{itemize}
\item \textbf{Nominal:} Las propiedades de la medici\'on nominal son:
\begin{itemize}
\item Exhaustiva: implica a todas las opciones.
\item A los sujetos se les asignan categor\'ias, por lo que son mutuamente excluyentes. Es decir, la variable est\'a presente o no; tiene o no una caracter\'istica.
\end{itemize}
\item \textbf{Ordinal:} Las propiedades de la medici\'on ordinal son:
\begin{itemize}
\item El nivel ordinal posee transitividad, por lo que se tiene la capacidad de identificar que es mejor o mayor que otra, en ese sentido se pueden establecer jerarqu\'ias.
\item Las distancias entre un valor y otro no son iguales.
\end{itemize}
\item \textbf{Intervalo:} 
\begin{itemize}
\item El nivel de medici\'on intervalar requiere distancias iguales entre cada valor. Por lo general utiliza datos cuantitativos. Por ejemplo: temperatura, atributos psicol\'ogicos (CI, nivel de autoestima, pruebas de conocimientos, etc.)
\item Las unidades de calificaci\'on son equivalentes en todos los puntos de la escala. Una escala de intervalos implica: clasificaci\'on, magnitud y unidades de tama\~nos iguales (Brown, 2000).
\item Se pueden hacer operaciones aritm\'eticas.
\item Cuando se le pide al sujeto que califique una situaci\'on del 0 al 10 puede tomarse como un nivel de medici\'on de intervalo, siempre y cuando se incluya el 0.
\end{itemize}
\item \textbf{Raz\'on:} 
\begin{itemize}
\item La escala empieza a partir del 0 absoluto, por lo tanto incluye s\'olo los n\'umeros por su valor en s\'i, por lo que no pueden existir los n\'umeros con signo negativo. Por ejemplo: Peso corporal en kg., edad en a\~nos, estatura en cm.
\end{itemize}
\end{itemize}
%---------------------------------------------------------
\subsection*{Definiciones adicionales}
%---------------------------------------------------------
\begin{itemize}
    \item \textbf{Variable:} Consideraciones que una variable son una caracter\'istica o fen\'omeno que puede tomar distintos valores.
    \item \textbf{Dato:} Mediciones o cualidades que han sido recopiladas como resultado de observaciones.
    \item \textbf{Poblaci\'on:} Se considera el \'area de la cual son extra\'idos los datos. Es decir, es el conjunto de elementos o individuos que poseen una caracter\'istica com\'un y medible acerca de lo cual se desea informaci\'on. Es tambi\'en llamado Universo.
    \item \textbf{Muestra:} Es un subconjunto de la poblaci\'on, seleccionado de acuerdo a una regla o alg\'un plan de muestreo.
    \item \textbf{Censo:} Recopilaci\'on de todos los datos (de inter\'es para la investigaci\'on) de la poblaci\'on.
    \item \textbf{Estad\'istica:} Es una funci\'on o f\'ormula que depende de los datos de la muestra (es variable).
    \item \textbf{Par\'ametro:} Caracter\'istica medible de la poblaci\'on. Es un resumen num\'erico de alguna variable observada de la poblaci\'on. Los par\'ametros normales que se estudian son: \textit{La media poblacional, Proporci\'on.}
    \item \textbf{Estimador:} Un estimador de un par\'ametro es un estad\'istico que se emplea para conocer el par\'ametro desconocido.
    \item \textbf{Estad\'istico:} Es una funci\'on de los valores de la muestra. Es una variable aleatoria, cuyos valores dependen de la muestra seleccionada. Su distribuci\'on de probabilidad, se conoce como \textit{Distribuci\'on muestral del estad\'istico}.
    \item \textbf{Estimaci\'on:} Este t\'ermino indica que a partir de lo observado en una muestra (un resumen estad\'istico con las medidas que conocemos de Descriptiva) se extrapola o generaliza dicho resultado muestral a la poblaci\'on total, de modo que lo estimado es el valor generalizado a la poblaci\'on. Consiste en la b\'usqueda del valor de los par\'ametros poblacionales objeto de estudio. Puede ser puntual o por intervalo de confianza:
    \begin{itemize}
        \item \textbf{\textit{Puntual:}} cuando buscamos un valor concreto. Un estimador de un par\'ametro poblacional es una funci\'on de los datos muestrales. En pocas palabras, es una f\'ormula que depende de los valores obtenidos de una muestra, para realizar estimaciones. Lo que se pretende obtener es el valor exacto de un par\'ametro.
    \item \textbf{\textit{Intervalo de confianza:}} cuando determinamos un intervalo, dentro del cual se supone que va a estar el valor del par\'ametro que se busca con una cierta probabilidad. El intervalo de confianza est\'a determinado por dos valores dentro de los cuales afirmamos que est\'a el verdadero par\'ametro con cierta probabilidad. Son unos l\'imites o margen de variabilidad que damos al valor estimado, para poder afirmar, bajo un criterio de probabilidad, que el verdadero valor no los rebasar\'a.

Este intervalo contiene al par\'ametro estimado con una determinada certeza o nivel de confianza. 
\end{itemize}

En la estimaci\'on por intervalos se usan los siguientes conceptos:

\item \textbf{Variabilidad del par\'ametro:} Si no se conoce, puede obtenerse una aproximaci\'on en los datos o en un estudio piloto. Tambi\'en hay m\'etodos para calcular el tama\~no de la muestra que prescinden de este aspecto. Habitualmente se usa como medida de esta variabilidad la desviaci\'on t\'ipica poblacional.
\item \textbf{Error de la estimaci\'on:} Es una medida de su precisi\'on que se corresponde con la amplitud del intervalo de confianza. Cuanta m\'as precisi\'on se desee en la estimaci\'on de un par\'ametro, m\'as estrecho deber\'a ser el intervalo de confianza y, por tanto, menor el error, y m\'as sujetos deber\'an incluirse en la muestra estudiada. 
\item \textbf{Nivel de confianza:} Es la probabilidad de que el verdadero valor del par\'ametro estimado en la poblaci\'on se sit\'ue en el intervalo de confianza obtenido. El nivel de confianza se denota por $1-\alpha$
\item \textbf{$p$-value:} Tambi\'en llamado nivel de significaci\'on. Es la probabilidad (en tanto por uno) de fallar en nuestra estimaci\'on, esto es, la diferencia entre la certeza (1) y el nivel de confianza $1-\alpha$. 
\item \textbf{Valor cr\'itico:} Se representa por $Z_{\alpha/2}$. Es el valor de la abscisa en una determinada distribuci\'on que deja a su derecha un \'area igual a 1/2, siendo $1-\alpha$ el nivel de confianza. Normalmente los valores cr\'iticos est\'an tabulados o pueden calcularse en funci\'on de la distribuci\'on de la poblaci\'on.

\end{itemize}

Para un tama\~no fijo de la muestra, los conceptos de error y nivel de confianza van relacionados. Si admitimos un error mayor, esto es, aumentamos el tama\~no del intervalo de confianza, tenemos tambi\'en una mayor probabilidad de \'exito en nuestra estimaci\'on, es decir, un mayor nivel de confianza. Por tanto, un aspecto que debe de tenerse en cuenta es el tama\~no muestral, ya que para disminuir el error que se comente habr\'a que aumentar el tama\~no muestral. Esto se resolver\'a, para un intervalo de confianza cualquiera, despejando el tama\~no de la muestra en cualquiera de las formulas de los intervalos de confianza que veremos a continuaci\'on, a partir del error m\'aximo permitido. Los intervalos de confianza pueden ser unilaterales o bilaterales:

\begin{itemize}
    \item \textbf{Contraste de Hip\'otesis:} Consiste en determinar si es aceptable, partiendo de datos muestrales, que la caracter\'istica o el par\'ametro poblacional estudiado tome un determinado valor o est\'e dentro de unos determinados valores.
    \item \textbf{Nivel de Confianza:} Indica la proporci\'on de veces que acertar\'iamos al afirmar que el par\'ametro est\'a dentro del intervalo al seleccionar muchas muestras.
\end{itemize}
%---------------------------------------------------------
\subsection{Muestreo:} 
%---------------------------------------------------------
\textbf{Muestreo:} Una muestra es representativa en la medida que es imagen de la poblaci\'on. En general, podemos decir que el tama\~no de una muestra depender\'a principalmente de: \textit{Nivel de precisi\'on deseado, Recursos disponibles, Tiempo involucrado en la investigaci\'on.} Adem\'as el plan de muestreo debe considerar \textit{La poblaci\'on, Par\'ametros a medir}. Existe una gran cantidad de tipos de muestreo, en la pr\'actica los m\'as utilizados son los siguientes:


\begin{itemize}
    \item \textbf{MUESTREO ALEATORIO SIMPLE:} Es un m\'etodo de selecci\'on de $n$ unidades extra\'idas de $N$, de tal manera que cada una de las posibles muestras tiene la misma probabilidad de ser escogida. (En la pr\'actica, se enumeran las unidades de 1 a $N$, y a continuaci\'on se seleccionan $n$ n\'umeros aleatorios entre 1 y $N$, ya sea de tablas o de alguna urna con fichas numeradas).
    \item \textbf{MUESTREO ESTRATIFICADO ALEATORIO:} Se usa cuando la poblaci\'on est\'a agrupada en pocos estratos, cada uno de ellos son muchas entidades. Este muestreo consiste en sacar una muestra aleatoria simple de cada uno de los estratos. (Generalmente, de tama\~no proporcional al estrato).
    \item \textbf{MUESTREO SISTEM\'ATICO:} Se utiliza cuando las unidades de la poblaci\'on est\'an de alguna manera totalmente ordenadas. Para seleccionar una muestra de $n$ unidades, se divide la poblaci\'on en $n$ subpoblaciones de tama\~no $K = N/n$ y se toma al azar una unidad de la $K$ primeras y de ah\'i en adelante cada $K$-\'esima unidad.
    \item \textbf{MUESTREO POR CONGLOMERADO:} Se emplea cuando la poblaci\'on est\'a dividida en grupos o conglomerados peque\~nos. Consiste en obtener una muestra aleatoria simple de conglomerados y luego CENSAR cada uno de \'estos.
    \item \textbf{MUESTREO EN DOS ETAPAS (Biet\'apico):} En este caso la muestra se toma en dos pasos:
    \begin{itemize}
        \item Seleccionar una muestra de unidades primarias, y 
        \item Seleccionar una muestra de elementos a partir de cada unidad primaria escogida.
        \item \textit{Observaci\'on:} En la realidad es posible encontrarse con situaciones en las cuales no es posible aplicar libremente un tipo de muestreo, incluso estaremos obligados a mezclarlas en ocasiones.
    \end{itemize}
\end{itemize}

%---------------------------------------------------------
\subsection{Errores Estad\'isticos Comunes}
%---------------------------------------------------------

El prop\'osito de esta secci\'on es solamente indicar los malos usos comunes de datos estad\'isticos, sin incluir el uso de m\'etodos estad\'isticos complicados. Un estudiante deber\'ia estar alerta en relaci\'on con estos malos usos y deber\'ia hacer un gran esfuerzo para evitarlos a fin de ser un verdadero estad\'istico.

\textbf{Datos estad\'isticos inadecuados:} Los datos estad\'isticos son usados como la materia prima para un estudio estad\'istico. Cuando los datos son inadecuados, la conclusi\'on extra\'ida del estudio de los datos se vuelve obviamente inv\'alida. Por ejemplo, supongamos que deseamos encontrar el ingreso familiar t\'ipico del a\~no pasado en la ciudad Y de 50,000 familias y tenemos una muestra consistente del ingreso de solamente tres familias: 1 mill\'on, 2 millones y no ingreso. Si sumamos el ingreso de las tres familias y dividimos el total por 3, obtenemos un promedio de 1 mill\'on. Entonces, extraemos una conclusi\'on basada en la muestra de que el ingreso familiar promedio durante el a\~no pasado en la ciudad fue de 1 mill\'on. Es obvio que la conclusi\'on es falsa, puesto que las cifras son extremas y el tama\~no de la muestra es demasiado peque\~no; por lo tanto la muestra no es representativa. 

Hay muchas otras clases de datos inadecuados. Por ejemplo, algunos datos son respuestas inexactas de una encuesta, porque las preguntas usadas en la misma son vagas o enga\~nosas, algunos datos son toscas estimaciones porque no hay disponibles datos exactos o es demasiado costosa su obtenci\'on, y algunos datos son irrelevantes en un problema dado, porque el estudio estad\'istico no est\'a bien planeado. Al momento de recopilar los datos que ser\'an procesados se es susceptible de cometer errores as\'i como durante los c\'omputos de los mismos. No obstante, hay otros errores que no tienen nada que ver con la digitaci\'on y que no son tan f\'acilmente identificables. Algunos de \'estos errores son:

\begin{itemize}
    \item \textbf{Sesgo:} Es imposible ser completamente objetivo o no tener ideas preconcebidas antes de comenzar a estudiar un problema, y existen muchas maneras en que una perspectiva o estado mental pueda influir en la recopilaci\'on y en el an\'alisis de la informaci\'on. En estos casos se dice que hay un sesgo cuando el individuo da mayor peso a los datos que apoyan su opini\'on que a aquellos que la contradicen. Un caso extremo de sesgo ser\'ia la situaci\'on donde primero se toma una decisi\'on y despu\'es se utiliza el an\'alisis estad\'istico para justificar la decisi\'on ya tomada.
    \item \textbf{Datos No Comparables:} el establecer comparaciones es una de las partes m\'as importantes del an\'alisis estad\'istico, pero es extremadamente importante que tales comparaciones se hagan entre datos que sean comparables.
    \item \textbf{Proyecci\'on descuidada de tendencias:} la proyecci\'on simplista de tendencias pasadas hacia el futuro es uno de los errores que m\'as ha desacreditado el uso del an\'alisis estad\'istico.
    \item \textbf{Muestreo Incorrecto:} en la mayor\'ia de los estudios sucede que el volumen de informaci\'on disponible es tan inmenso que se hace necesario estudiar muestras, para derivar conclusiones acerca de la poblaci\'on a que pertenece la muestra. Si la muestra se selecciona correctamente, tendr\'a b\'asicamente las mismas propiedades que la poblaci\'on de la cual fue extra\'ida; pero si el muestreo se realiza incorrectamente, entonces puede suceder que los resultados no signifiquen nada.
\end{itemize}

\textbf{Sesgo} significa que un usuario d\'e los datos perjudicialmente de m\'as \'enfasis a los hechos, los cuales son empleados para mantener su predeterminada posici\'on u opini\'on. Los estad\'isticos son frecuentemente degradados por lemas tales como: \textit{Hay tres clases de mentiras: mentiras, mentiras reprobables y estad\'istica, y Las cifras no mienten, pero los mentirosos piensan}. Hay dos clases de sesgos: conscientes e inconscientes. Ambos son comunes en el an\'alisis estad\'istico. Hay numerosos ejemplos de sesgos conscientes. Un anunciante frecuentemente usa la estad\'istica para probar que su producto es muy superior al producto de su competidor. Un pol\'itico prefiere usar la estad\'istica para sostener su punto de vista. Gerentes y l\'ideres de trabajadores pueden simult\'aneamente situar sus respectivas cifras estad\'isticas sobre la misma tabla de trato para mostrar que sus rechazos o peticiones son justificadas. Es casi imposible que un sesgo inconsciente est\'e completamente ausente en un trabajo estad\'istico. En lo que respecta al ser humano, es dif\'icil obtener una actitud completamente objetiva al abordar un problema, aun cuando un cient\'ifico deber\'ia tener una mente abierta. Un estad\'istico deber\'ia estar enterado del hecho de que su interpretaci\'on de los resultados del an\'alisis estad\'istico est\'a influenciado por su propia experiencia, conocimiento y antecedentes con relaci\'on al problema dado.

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Fundamentos}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

%---------------------------------------------------------
\subsection{Pruebas de Hipótesis}
%---------------------------------------------------------

\begin{itemize}
    \item Una hipótesis estadística es una afirmación acerca de la distribución de probabilidad de una variable aleatoria, a menudo involucran uno o más parámetros de la distribución.
    \item Las hipótesis son afirmaciones respecto a la población o distribución bajo estudio, no en torno a la muestra.
    \item La mayoría de las veces, la prueba de hipótesis consiste en determinar si la situación experimental ha cambiado.
    \item El interés principal es decidir sobre la veracidad o falsedad de una hipótesis, a este procedimiento se le llama \textit{prueba de hipótesis}.
    \item Si la información es consistente con la hipótesis, se concluye que esta es verdadera, de lo contrario que con base en la información, es falsa.
\end{itemize}

Una prueba de hipótesis está formada por cinco partes:
\begin{itemize}
    \item La hipótesis nula, denotada por $H_{0}$.
    \item La hipótesis alternativa, denotada por $H_{1}$.
    \item El estadístico de prueba y su valor $p$.
    \item La región de rechazo.
    \item La conclusión.
\end{itemize}

\begin{Def}
Las dos hipótesis en competencia son la \textbf{hipótesis alternativa $H_{1}$}, usualmente la que se desea apoyar, y la \textbf{hipótesis nula $H_{0}$}, opuesta a $H_{1}$.
\end{Def}
En general, es más fácil presentar evidencia de que $H_{1}$ es cierta, que demostrar que $H_{0}$ es falsa, es por eso que por lo regular se comienza suponiendo que $H_{0}$ es cierta, luego se utilizan los datos de la muestra para decidir si existe evidencia a favor de $H_{1}$, más que a favor de $H_{0}$, así se tienen dos conclusiones:
\begin{itemize}
    \item Rechazar $H_{0}$ y concluir que $H_{1}$ es verdadera.
    \item Aceptar, no rechazar, $H_{0}$ como verdadera.
\end{itemize}

\begin{Ejem}
Se desea demostrar que el salario promedio por hora en cierto lugar es distinto de $19$ usd, que es el promedio nacional. Entonces $H_{1}:\mu \neq 19$, y $H_{0}:\mu = 19$.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hipótesis de dos colas}.

\begin{Ejem}
Un determinado proceso produce un promedio de $5\%$ de piezas defectuosas. Se está interesado en demostrar que un simple ajuste en una máquina reducirá $p$, la proporción de piezas defectuosas producidas en este proceso. Entonces se tiene $H_{0}: p < 0.3$ y $H_{1}: p = 0.03$. Si se puede rechazar $H_{0}$, se concluye que el proceso ajustado produce menos del $5\%$ de piezas defectuosas.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hipótesis de una cola}.

La decisión de rechazar o aceptar la hipótesis nula está basada en la información contenida en una muestra proveniente de la población de interés. Esta información tiene estas formas:
\begin{itemize}
    \item \textbf{Estadístico de prueba:} un sólo número calculado a partir de la muestra.
    \item \textbf{$p$-value:} probabilidad calculada a partir del estadístico de prueba.
\end{itemize}

\begin{Def}
El $p$-value es la probabilidad de observar un estadístico de prueba tanto o más alejado del valor observado, si en realidad $H_{0}$ es verdadera. Valores grandes del estadístico de prueba y valores pequeños de $p$ significan que se ha observado un evento muy poco probable, si $H_{0}$ en realidad es verdadera.
\end{Def}

Todo el conjunto de valores que puede tomar el estadístico de prueba se divide en dos regiones. Un conjunto, formado de valores que apoyan la hipótesis alternativa y llevan a rechazar $H_{0}$, se denomina \textbf{región de rechazo}. El otro, conformado por los valores que sustentan la hipótesis nula, se le denomina \textbf{región de aceptación}. Cuando la región de rechazo está en la cola izquierda de la distribución, la prueba se denomina \textbf{prueba lateral izquierda}. Una prueba con región de rechazo en la cola derecha se le llama \textbf{prueba lateral derecha}. Si el estadístico de prueba cae en la región de rechazo, entonces se rechaza $H_{0}$. Si el estadístico de prueba cae en la región de aceptación, entonces la hipótesis nula se acepta o la prueba se juzga como no concluyente. Dependiendo del nivel de confianza que se desea agregar a las conclusiones de la prueba, y el \textbf{nivel de significancia $\alpha$}, el riesgo que está dispuesto a correr si se toma una decisión incorrecta.

\begin{Def}
Un \textbf{error de tipo I} para una prueba estadística es el error que se tiene al rechazar la hipótesis nula cuando es verdadera. El \textbf{nivel de significancia} para una prueba estadística de hipótesis es
\begin{eqnarray*}
\alpha &=& P\left\{\textrm{error tipo I}\right\} = P\left\{\textrm{rechazar equivocadamente } H_{0}\right\} \\
&=& P\left\{\textrm{rechazar } H_{0} \textrm{ cuando } H_{0} \textrm{ es verdadera}\right\}
\end{eqnarray*}
\end{Def}
Este valor $\alpha$ representa el valor máximo de riesgo tolerable de rechazar incorrectamente $H_{0}$. Una vez establecido el nivel de significancia, la región de rechazo se define para poder determinar si se rechaza $H_{0}$ con un cierto nivel de confianza.

%---------------------------------------------------------
\subsection{Muestras grandes: una media poblacional}
%---------------------------------------------------------


\begin{Def}
El \textbf{valor de $p$} (\textbf{$p$-value}) o nivel de significancia observado de un estadístico de prueba es el valor más pequeño de $\alpha$ para el cual $H_{0}$ se puede rechazar. El riesgo de cometer un error tipo $I$, si $H_{0}$ es rechazada con base en la información que proporciona la muestra.
\end{Def}

\begin{Note}
Valores pequeños de $p$ indican que el valor observado del estadístico de prueba se encuentra alejado del valor hipotético de $\mu$, es decir se tiene evidencia de que $H_{0}$ es falsa y por tanto debe de rechazarse.
\end{Note}

\begin{Note}
Valores grandes de $p$ indican que el estadístico de prueba observado no está alejado de la media hipotética y no apoya el rechazo de $H_{0}$.
\end{Note}

\begin{Def}
Si el valor de $p$ es menor o igual que el nivel de significancia $\alpha$, determinado previamente, entonces $H_{0}$ es rechazada y se puede concluir que los resultados son estadísticamente significativos con un nivel de confianza del $100 (1-\alpha)\%$.
\end{Def}
Es usual utilizar la siguiente clasificación de resultados:


\begin{center}
\begin{tabular}{|c||c|l|}\hline
$p$ & $H_{0}$ & Significativa \\ \hline
$p\leq 0.01$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. altamente significativos  y en contra de $H_{0}$\end{tabular} \\ \hline
$p\leq 0.05$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. Estad\'isticamente significativos  y en contra de $H_{0}$\end{tabular} \\ \hline
$p\leq 0.10$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. posiblemente significativos con Tendencia estad\'istica \\ y en contra de $H_{0}$\end{tabular} \\ \hline
$p> 0.10$ & no rechazada & \begin{tabular}[c]{@{}l@{}}Result.  estad\'isticamente no significativos y no rechazar $H_{0}$\end{tabular} \\ \hline
\end{tabular}
\end{center}

\begin{Note}
Para determinar el valor de $p$, encontrar el \'area en la cola despu\'es del estad\'istico de prueba. Si la prueba es de una cola, este es el valor de $p$. Si es de dos colas, \'este valor encontrado es la mitad del valor de $p$. Rechazar $H_{0}$ cuando el valor de $p<\alpha$.
\end{Note}

Hay dos tipos de errores al realizar una prueba de hip\'otesis
\begin{center}
\begin{tabular}{c|cc}
& $H_{0}$ es Verdadera & $H_{0}$ es Falsa\\\hline\hline
Rechazar $H_{0}$ & Error tipo I & $\surd$\\
Aceptar $H_{0}$ & $\surd$ & Error tipo II
\end{tabular}
\end{center}
\begin{Def}
La probabilidad de cometer el error tipo II se define por $\beta$ donde
\begin{eqnarray*}
\beta&=&P\left\{\textrm{error tipo II}\right\}=P\left\{\textrm{Aceptar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{Aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}
\end{eqnarray*}
\end{Def}

\begin{Note}
Cuando $H_{0}$ es falsa y $H_{1}$ es verdadera, no siempre es posible especificar un valor exacto de $\mu$, sino m\'as bien un rango de posibles valores.\medskip
En lugar de arriesgarse a tomar una decisi\'on incorrecta, es mejor conlcuir que \textit{no hay evidencia suficiente para rechazar $H_{0}$}, es decir en lugar de aceptar $H_{0}$, \textit{no rechazar $H_{0}$}.

\end{Note}
La bondad de una prueba estad\'istica se mide por el tama\~ no de $\alpha$ y $\beta$, ambas deben de ser peque\~ nas. Una manera muy efectiva de medir la potencia de la prueba es calculando el complemento del error tipo $II$:
\begin{eqnarray*}
1-\beta&= &P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{1}\textrm{ es verdadera}\right\}
\end{eqnarray*}
\begin{Def}
La \textbf{potencia de la prueba}, $1-\beta$, mide la capacidad de que la prueba funciona como se necesita.
\end{Def}

\begin{Ejem}
La producci\'on diariade una planta qu\'imica local ha promediado 880 toneladas en los \'ultimos a\~nos. A la gerente de control de calidad le gustar\'ia saber si este promedio ha cambiado en meses recientes. Ella selecciona al azar 50 d\'ias de la base de datos computarizada y calcula el promedio y la desviaci\'on est\'andar de las $n=50$  producciones como $\overline{x}=871$ toneladas y $s=21$ toneladas, respectivamente. Pruebe la hip\'otesis  apropiada usando $\alpha=0.05$.

La hip\'otesis nula apropiada es:
\begin{eqnarray*}
H_{0}&:& \mu=880\\
&&\textrm{ y la hip\'otesis alternativa }H_{1}\textrm{ es }\\
H_{1}&:& \mu\neq880
\end{eqnarray*}
el estimador puntual para $\mu$ es $\overline{x}$, entonces el estad\'istico de prueba es\medskip
\begin{eqnarray*}
z&=&\frac{\overline{x}-\mu_{0}}{s/\sqrt{n}}\\
&=&\frac{871-880}{21/\sqrt{50}}=-3.03
\end{eqnarray*}

Para esta prueba de  dos colas, hay que determinar los dos valores de $z_{\alpha/2}$, es decir,  $z_{\alpha/2}=\pm1.96$, como $z>z_{\alpha/2}$, $z$ cae en la zona de rechazo, por lo tanto  la gerente puede rechazar la hip\'otesis nula y concluir que el promedio efectivamente ha cambiado. La probabilidad de rechazar $H_{0}$ cuando esta es verdadera es de  $0.05$. Recordemos que el valor observado del estad\'istico de prueba es $z=-3.03$, la regi\'on de rechazo m\'as peque\~na que puede usarse y todav\'ia seguir rechazando $H_{0}$ es $|z|>3.03$, entonces $p=2(0.012)=0.0024$, que a su vez es menor que el nivel de significancia $\alpha$ asignado inicialmente, y adem\'as los resultados son  \textbf{altamente significativos}. Finalmente determinemos la potencia de la prueba cuando $\mu$ en realidad es igual a $870$ toneladas.

Recordar que la regi\'on de aceptaci\'on est\'a entre $-1.96$ y $1.96$, para $\mu=880$, equivalentemente $$874.18<\overline{x}<885.82$$
$\beta$ es la probabilidad de aceptar $H_{0}$ cuando $\mu=870$, calculemos los valores de $z$ correspondientes a $874.18$ y $885.82$ \medskip
Entonces
\begin{eqnarray*}
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{874.18-870}{21/\sqrt{50}}=1.41\\
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{885.82-870}{21/\sqrt{50}}=5.33
\end{eqnarray*}
por lo tanto
\begin{eqnarray*}
\beta&=&P\left\{\textrm{aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}=P\left\{874.18<\mu<885.82\textrm{ cuando }\mu=870\right\}\\
&=&P\left\{1.41<z<5.33\right\}=P\left\{1.41<z\right\}=1-0.9207=0.0793
\end{eqnarray*}
entonces, la potencia de la prueba es
$$1-\beta=1-0.0793=0.9207$$ que es la probabilidad de rechazar correctamente $H_{0}$ cuando $H_{0}$ es falsa.
\end{Ejem}

%---------------------------------------------------------
\subsubsection{Prueba de hip\'otesis para la diferencia entre dos medias poblacionales}
%---------------------------------------------------------

El estad\'istico que resume la informaci\'on muestral respecto a la diferencia en medias poblacionales $\left(\mu_{1}-\mu_{2}\right)$ es la diferencia de las medias muestrales $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, por tanto al probar la difencia entre las medias muestrales se verifica que la diferencia real entre las medias poblacionales difiere de un valor especificado, $\left(\mu_{1}-\mu_{2}\right)=D_{0}$, se puede usar el error est\'andar de $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, es decir
$$\sqrt{\frac{\sigma^{2}_{1}}{n_{1}}+\frac{\sigma^{2}_{2}}{n_{2}}}$$
cuyo estimador est\'a dado por
$$SE=\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}$$
El procedimiento para muestras grandes es:

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,
donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
$$z=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}$$
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

\begin{Ejem}
Para determinar si ser propietario de un autom\'ovil afecta el rendimiento acad\'emico de un estudiante, se tomaron dos muestras aleatorias de 100 estudiantes varones. El promedio de calificaciones para los $n_{1}=100$ no propietarios de un auto tuvieron un promedio y varianza de $\overline{x}_{1}=2.7$ y $s_{1}^{2}=0.36$, respectivamente, mientras que para para la segunda muestra con $n_{2}=100$ propietarios de un auto, se tiene $\overline{x}_{2}=2.54$ y $s_{2}^{2}=0.4$. Los datos presentan suficiente evidencia para indicar una diferencia en la media en el rendimiento acad\'emico entre propietarios y no propietarios de un autom\'ovil? Hacer pruebas para $\alpha=0.01,0.05$ y $\alpha=0.1$.

\begin{itemize}
\item Soluci\'on utilizando la t\'ecnica de regiones de rechazo:\medskip
realizando las operaciones
$z=1.84$, determinar si excede los valores de $z_{\alpha/2}$.
\item Soluci\'on utilizando el $p$-value:\medskip
Calcular el valor de $p$, la probabilidad de que $z$ sea mayor que $z=1.84$ o menor que $z=-1.84$, se tiene que $p=0.0658$. 

\item Si el intervalo de confianza que se construye contiene el valor del par\'ametro especificado por $H_{0}$, entonces ese valor es uno de los posibles valores del par\'ametro y $H_{0}$ no debe ser rechazada.

\item Si el valor hipot\'etico se encuentra fuera de los l\'imites de confianza, la hip\'otesis nula es rechazada al nivel de significancia $\alpha$.
\end{itemize}
\end{Ejem}

%---------------------------------------------------------
\subsubsection*{Prueba de Hip\'otesis para una Proporci\'on Binomial}
%---------------------------------------------------------

Para una muestra aleatoria de $n$ intentos id\'enticos, de una poblaci\'on binomial, la proporci\'on muesrtal $\hat{p}$ tiene una distribuci\'on aproximadamente normal cuando $n$ es grande, con media $p$ y error est\'andar
$$SE=\sqrt{\frac{pq}{n}}.$$
La prueba de hip\'otesis de la forma
\begin{eqnarray*}
H_{0}&:&p=p_{0}\\
H_{1}&:&p>p_{0}\textrm{, o }p<p_{0}\textrm{ o }p\neq p_{0}
\end{eqnarray*}
El estad\'istico de prueba se construye con el mejor estimador de la proporci\'on verdadera, $\hat{p}$, con el estad\'istico de prueba $z$, que se distribuye normal est\'andar.

El procedimiento es
\begin{itemize}
\item[1) ] Hip\'otesis nula: $H_{0}:p=p_{0}$
\item[2) ] Hip\'otesis alternativa

\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:p>p_{0}$ & $p\neq p_{0}$\\
$H_{1}:p<p_{0}$ & \\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
z=\frac{\hat{p}-p_{0}}{\sqrt{\frac{pq}{n}}},\hat{p}=\frac{x}{n}
\end{eqnarray*}
donde $x$ es el n\'umero de \'exitos en $n$ intentos binomiales.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

%--------------------------------------------------------------------------------
\subsubsection{Prueba de Hip\'otesis diferencia entre dos Proporciones Binomiales}
%--------------------------------------------------------------------------------

Cuando se tienen dos muestras aleatorias independientes de dos poblaciones binomiales, el objetivo del experimento puede ser la diferencia $\left(p_{1}-p_{2}\right)$ en las proporciones de individuos u objetos que poseen una caracter\'istica especifica en las dos poblaciones. En este caso se pueden utilizar los estimadores de las dos proporciones $\left(\hat{p}_{1}-\hat{p}_{2}\right)$ con error est\'andar dado por
$$SE=\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}},$$
considerando el estad\'istico $z$ con un nivel de significancia $\left(1-\alpha\right)100\%$

La hip\'otesis nula a probarse es de la forma
\begin{itemize}
\item[$H_{0}$: ] $p_{1}=p_{2}$ o equivalentemente $\left(p_{1}-p_{2}\right)=0$, contra una hip\'otesis alternativa $H_{1}$ de una o dos colas.
\end{itemize}

Para estimar el error est\'andar del estad\'istico $z$, se debe de utilizar el hecho de que suponiendo que $H_{0}$ es verdadera, las dos proporciones son iguales a alg\'un valor com\'un, $p$. Para obtener el mejor estimador de $p$ es
$$p=\frac{\textrm{n\'umero total de \'exitos}}{\textrm{N\'umero total de pruebas}}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}.$$

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\left(p_{1}-p_{2}\right)=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(p_{1}-p_{2}\right)>0$ & $H_{1}:\left(p_{1}-p_{2}\right)\neq 0$\\ 
$H_{1}:\left(p_{1}-p_{2}\right)<0$&\\
\end{tabular}
\end{center}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}}=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{pq}{n_{1}}+\frac{pq}{n_{2}}}},
\end{eqnarray*}
donde $\hat{p_{1}}=x_{1}/n_{1}$ y $\hat{p_{2}}=x_{2}/n_{2}$ , dado que el valor com\'un para $p_{1}$ y $p_{2}$ es $p$, entonces $\hat{p}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$ y por tanto el estad\'istico de prueba es
\begin{eqnarray*}
z=\frac{\hat{p}_{1}-\hat{p}_{2}}{\sqrt{\hat{p}\hat{q}}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}.
\end{eqnarray*}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{\alpha}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

%----------------------------------------------------------------
\subsection{Muestras Peque\~nas}
%----------------------------------------------------------------

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu=\mu_{0}$,
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu>\mu_{0}$ & $H_{1}:\mu\neq \mu_{0}$\\ 
$H_{1}:\mu<\mu0$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
t=\frac{\overline{x}-\mu_{0}}{\sqrt{\frac{s^{2}}{n}}},
\end{eqnarray*}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}
%------------------------------------------------------------------------------------
\subsubsection{Diferencia entre dos medias poblacionales: MAI}
%------------------------------------------------------------------------------------

Cuando los tama\~nos de muestra son peque\ ~nos, no se puede asegurar que las medias muestrales sean normales, pero si las poblaciones originales son normales, entonces la distribuci\'on muestral de la diferencia de las medias muestales, $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, ser\'a normal con media $\left(\mu_{1}-\mu_{2}\right)$ y error est\'andar $$ES=\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}.$$


\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
$$t=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}$$


donde $$s^{2}=\frac{\left(n_{1}-1\right)s_{1}^{2}+\left(n_{2}-1\right)s_{2}^{2}}{n_{1}+n_{2}-2}$$

\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}

Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.
\end{itemize}
%------------------------------------------------------------------------------------
\subsubsection{Diferencia entre dos medias poblacionales: Diferencias Pareadas}
%------------------------------------------------------------------------------------

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu_{d}=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:\mu_{d}$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu_{d}>0$ & $H_{1}:\mu_{d}\neq 0$\\ 
$H_{1}:\mu_{d}<0$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
t=\frac{\overline{d}}{\sqrt{\frac{s_{d}^{2}}{n}}}
\end{eqnarray*}
donde $n$ es el n\'umero de diferencias pareadas, $\overline{d}$ es la media de las diferencias muestrales, y $s_{d}$ es la desviaci\'on est\'andar de las diferencias muestrales.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.
\end{itemize}
%------------------------------------------------------------------------------------
\subsubsection{Inferencias con respecto a la Varianza Poblacional}
%------------------------------------------------------------------------------------
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\sigma^{2}=\sigma^{2}_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\sigma^{2}>\sigma^{2}_{0}$ & $H_{1}:\sigma^{2}\neq \sigma^{2}_{0}$\\ 
$H_{1}:\sigma^{2}<\sigma^{2}_{0}$&\\
\end{tabular}
\end{center}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
\chi^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}_{0}},
\end{eqnarray*}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$\chi^{2}>\chi^{2}_{\alpha}$ & \\
$\chi^{2}<\chi^{2}_{\left(1-\alpha\right)}$ cuando $H_{1}:\chi^{2}<\chi^{2}_{0}$&$\chi^{2}>\chi^{2}_{\alpha/2}$ o $\chi^{2}<\chi^{2}_{\left(1-\alpha/2\right)}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}

Los valores cr\'iticos de $\chi^{2}$,est\'an basados en $\left(n_{1}+\right)$ grados de libertad.

\end{itemize}

%------------------------------------------------------------------------------------
\subsubsection{Comparaci\'on de dos varianzas poblacionales}
%------------------------------------------------------------------------------------

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)>D_{0}$ & $H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)<D_{0}$&\\
\end{tabular}
\end{center}
\item[3) ] Estad\'istico de prueba:
$$F=\frac{s_{1}^{2}}{s_{2}^{2}}$$
donde $s_{1}^{2}$ es la varianza muestral m\'as grande.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$F>F_{\alpha}$ & $F>F_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

%---------------------------------------------------------
\subsection{Estimaci\'on por intervalos}
%---------------------------------------------------------

Recordemos que $S^{2}$ es un estimador insesgado de $\sigma^{2}$, entonces se tiene la siguiente definici\'on 
\begin{Def}
Sean $\hat{\theta}_{1}$ y $\hat{\theta}_{2}$ dos estimadores insesgados de $\theta$, par\'ametro poblacional. Si $\sigma_{\hat{\theta}_{1}}^{2}<\sigma_{\hat{\theta}_{2}}^{2}$, decimos que $\hat{\theta}_{1}$ un estimador m\'as eficaz de $\theta$ que $\hat{\theta}_{2}$.
\end{Def}

Algunas observaciones que es preciso realizar
\begin{Note}
\begin{enumerate}
\item[a) ]Para poblaciones normales, $\overline{X}$ y $\tilde{X}$ son estimadores insesgados de $\mu$, pero con $\sigma_{\overline{X}}^{2}<\sigma_{\tilde{X}_{2}}^{2}$.
%\end{Note}

%\begin{Note}
\item[b) ]Para las estimaciones por intervalos de $\theta$, un intervalo de la forma $\hat{\theta}_{L}<\theta<\hat{\theta}_{U}$,  $\hat{\theta}_{L}$ y $\hat{\theta}_{U}$ dependen del valor de $\hat{\theta}$.
\item[c) ]Para $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, si $n\rightarrow\infty$, entonces $\hat{\theta}\rightarrow\mu$.
%\end{Note}
\end{enumerate}
\end{Note}


\begin{Note}
Para $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, si $n\rightarrow\infty$, %entonces $\hat{\theta}\rightarrow\mu$.
\end{Note}


\begin{enumerate}
\item[d) ]Para $\hat{\theta}$ se determinan $\hat{\theta}_{L}$ y $\hat{\theta}_{U}$ de modo tal que 
\begin{eqnarray*}
P\left\{\hat{\theta}_{L}<\hat{\theta}<\hat{\theta}_{U}\right\}=1-\alpha,
\end{eqnarray*}
con $\alpha\in\left(0,1\right)$. Es decir, $\theta\in\left(\hat{\theta}_{L},\hat{\theta}_{U}\right)$ es un intervalo de confianza del $100\left(1-\alpha\right)\%$.

\item[e) ] De acuerdo con el TLC se espera que la distribuci\'on muestral de $\overline{X}$ se distribuye aproximadamente normal con media $\mu_{X}=\mu$ y desviaci\'on est\'andar $\sigma_{\overline{X}}=\frac{\sigma}{\sqrt{n}}$.

\end{enumerate}

Para $Z_{\alpha/2}$ se tiene $P\left\{-Z_{\alpha/2}<Z<Z_{\alpha/2}\right\}=1-\alpha$, donde $Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}$. Entonces
$$P\left\{-Z_{\alpha/2}<\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}<Z_{\alpha/2}\right\}=1-\alpha,$$ es equivalente a 
$$P\left\{\overline{X}-Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}<\mu<\overline{X}+Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right\}=1-\alpha.$$ 

\begin{enumerate}
\item[f) ]Si $\overline{X}$ es la media muestral de una muestra de tama\~no $n$ de una poblaci\'on con varianza conocida $\sigma^{2}$, el intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\mu$ es $$\mu\in\left(\overline{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}},\overline{x}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right)$$.

\item[g) ] Para muestras peque\~nas de poblaciones no normales, no se puede esperar que el grado de confianza sea preciso.
\item[h) ] Para $n\geq30$, con distribuci\'on de forma no muy sesgada, se pueden tener buenos resultados.
\end{enumerate}

\begin{Teo}
Si $\overline{X}$ es un estimador de $\mu$, podemos tener $100\left(1-\alpha\right)\%$  de confianza en que el error no exceder\'a a $z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$, error entre $\overline{X}$ y $\mu$.
\end{Teo}

\begin{Teo}
Si $\overline{X}$ es un estimador de $\mu$, podemos tener $100\left(1-\alpha\right)\%$  de confianza en que el error no exceder\'a una cantidad $e$ cuando el tama\~no de la muestra es $$n=\left(\frac{z_{\alpha/2}\sigma}{e}\right)^{2}.$$
\end{Teo}
\begin{Note}
Para intervalos unilaterales
$$P\left\{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}<Z_{\alpha}\right\}=1-\alpha$$
\end{Note}

equivalentemente
$$P\left\{\mu<\overline{X}+Z_{\alpha}\frac{\sigma}{\sqrt{n}}\right\}=1-\alpha.$$
Si $\overline{X}$ es la media de una muestra aleatoria de tama\~no $n$  a partir de una poblaci\'on con varianza $\sigma^{2}$, los l\'imites de confianza unilaterales del   $100\left(1-\alpha\right)\%$  de confianza para $\mu$ est\'an dados por
\begin{itemize}
\item[a) ] L\'imite unilateral superior: $\overline{x}+z_{\alpha}\frac{\sigma}{\sqrt{n}}$
\item[b) ] L\'imite unilateral inferior: $\overline{x}-z_{\alpha}\frac{\sigma}{\sqrt{n}}$
\item[c) ] Para $\sigma$ desconocida recordar que $T=\frac{\overline{x}-\mu}{s/\sqrt{n}}\sim t_{n-1}$, donde $s$ es la desviaci\'on est\'andar de la muestra. Entonces
\begin{eqnarray*}
P\left\{-t_{\alpha/2}<T<t_{\alpha/2}\right\}=1-\alpha,\textrm{equivalentemente}
P\left\{\overline{X}-t_{\alpha/2}\frac{s}{\sqrt{n}}<\mu<\overline{X}+t_{\alpha/2}\frac{s}{\sqrt{n}}\right\}=1-\alpha.
\end{eqnarray*}

\item[d) ] Un intervalo de confianza del $100\left(1-\alpha\right)\%$  de confianza para $\mu$, $\sigma^{2}$ desconocida y poblaci\'on normal es $\mu\in\left(\overline{x}-t_{\alpha/2}\frac{s}{\sqrt{n}},\overline{x}+t_{\alpha/2}\frac{s}{\sqrt{n}}\right)$, donde $t_{\alpha/2}$ es una $t$-student con $\nu=n-1$ grados de libertad.
\item[e) ] Los l\'imites unilaterales para $\mu$ con $\sigma$ desconocida son $\overline{X}-t_{\alpha/2}\frac{s}{\sqrt{n}}$ y $\overline{X}+t_{\alpha/2}\frac{s}{\sqrt{n}}$.

\item[f) ] Cuando la poblaci\'on no es normal, $\sigma$ desconocida y $n\geq30$, $\sigma$ se puede reemplazar por $s$ para obtener el intervalo de confianza para muestras grandes:
$$\overline{X}\pm t_{\alpha/2}\frac{s}{\sqrt{n}}.$$

\item[g) ] El estimador de $\overline{X}$ de $\mu$,  $\sigma$ desconocida, la varianza de $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, el error est\'andar de $\overline{X}$ es $\sigma/\sqrt{n}$.

\item[h) ] Si $\sigma$ es desconocida y la poblaci\'on es normal, $s\rightarrow\sigma$ y se incluye el error est\'andar $s/\sqrt{n}$, entonces $$\overline{x}\pm t_{\alpha/2}\frac{s}{\sqrt{n}}.$$
\end{itemize}

%---------------------------------------------------------
\subsubsection{Intervalos de confianza sobre la varianza}
%---------------------------------------------------------

Supongamos que  $X$ se distribuye normal $\left(\mu,\sigma^{2}\right)$, desconocidas. Sea $X_{1},X_{2},\ldots,X_{n}$ muestra aleatoria de tama\~no $n$ , $s^{2}$ la varianza muestral.

Se sabe que $X^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}}$ se distribuye $\chi^{2}_{n-1}$ grados de libertad. Su intervalo de confianza es
\begin{eqnarray*}
\begin{array}{l}
P\left\{\chi^{2}_{1-\frac{\alpha}{2},n-1}\leq\chi^{2}\leq\chi^{2}_{\frac{\alpha}{2},n-1}\right\}=1-\alpha\\
P\left\{\chi^{2}_{1-\frac{\alpha}{2},n-1}\leq\frac{\left(n-1\right)s^{2}}{\sigma^{2}}\leq\chi^{2}_{\frac{\alpha}{2},n-1}\right\}=1-\alpha\\
P\left\{\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}}\leq\sigma^{2}\leq\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right\}=1-\alpha,
\end{array}
\end{eqnarray*}
es decir

\begin{eqnarray*}
\sigma^{2}\in\left[\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}},\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right],
\end{eqnarray*}
los intervalos unilaterales son
\begin{eqnarray*}
\sigma^{2}\in\left[\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}},\infty\right],
\end{eqnarray*}
y
\begin{eqnarray*}
\sigma^{2}\in\left[-\infty,\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right].
\end{eqnarray*}

%---------------------------------------------------------
\subsubsection{Intervalos de confianza para proporciones}
%---------------------------------------------------------

Supongamos que se tienen una muestra de tama\~no $n$ de una poblaci\'on grande pero finita, y supongamos que $X$, $X\leq n$, pertenecen a la clase de inter\'es, entonces $$\hat{p}=\frac{\overline{X}}{n},$$ es el estimador puntual de la proporci\'on de la poblaci\'on que pertenece a dicha clase. $n$ y $p$ son los par\'ametros de la distribuci\'on binomial, entonces $\hat{p}\sim N\left(p,\frac{p\left(1-p\right)}{n}\right)$ aproximadamente si $p$ es distinto de $0$ y $1$; o si $n$ es suficientemente grande. Entonces
\begin{eqnarray*}
Z=\frac{\hat{p}-p}{\sqrt{\frac{p\left(1-p\right)}{n}}}\sim N\left(0,1\right),\textrm{aproximadamente.}
\end{eqnarray*}
 
Entonces
\begin{eqnarray*}
1-\alpha&=&P\left\{-z_{\alpha/2}\leq\frac{\hat{p}-p}{\sqrt{\frac{p\left(1-p\right)}{n}}}\leq z_{\alpha/2}\right\}=P\left\{\hat{p}-z_{\alpha/2}\sqrt{\frac{p\left(1-p\right)}{n}}\leq p\leq \hat{p}+z_{\alpha/2}\sqrt{\frac{p\left(1-p\right)}{n}}\right\}
\end{eqnarray*}
con $\sqrt{\frac{p\left(1-p\right)}{n}}$ error est\'andar del estimador puntual $p$. Una soluci\'on para determinar el intervalo de confianza del par\'ametro $p$ (desconocido) es

\begin{eqnarray*}
1-\alpha=P\left\{\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\leq p\leq \hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right\}
\end{eqnarray*}
entonces los intervalos de confianza, tanto unilaterales como de dos colas son: 
\begin{itemize}
\item[a) ] $p\in \left(\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}},\hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right)$,

\item[b) ] $p\in \left(-\infty,\hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right)$,

\item[c) ] $p\in \left(\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}},\infty\right)$;

\end{itemize}
para minimizar el error est\'andar, se propone que el tama\~no de la muestra sea $$n= \left(\frac{z_{\alpha/2}}{E}\right)^{2}p\left(1-p\right)$$, donde $$E=\mid p-\hat{p}\mid.$$


%---------------------------------------------------------
\subsubsection{Intervalos de confianza para dos muestras: Varianzas conocidas}
%---------------------------------------------------------

Sean $X_{1}$ y $X_{2}$ variables aleatorias independientes. $X_{1}$ con media desconocida $\mu_{1}$ y varianza conocida $\sigma_{1}^{2}$; y $X_{2}$ con media desconocida $\mu_{2}$ y varianza conocida $\sigma_{2}^{2}$. Se busca encontrar un intervalo de confianza de $100\left(1-\alpha\right)\%$ de la diferencia entre medias $\mu_{1}$ y $\mu_{2}$. Sean $X_{11},X_{12},\ldots,X_{1n_{1}}$ muestra aleatoria de $n_{1}$ observaciones de $X_{1}$, y sean $X_{21},X_{22},\ldots,X_{2n_{2}}$ muestra aleatoria de $n_{2}$ observaciones de $X_{2}$.\medskip

Sean $\overline{X}_{1}$ y $\overline{X}_{2}$, medias muestrales, entonces el estad\'sitico 
\begin{eqnarray*}
Z=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}}\sim N\left(0,1\right),\end{eqnarray*}
si $X_{1}$ y $X_{2}$ son normales o aproximadamente normales si se aplican las condiciones del Teorema de L\'imite Central respectivamente. Entonces se tiene
\begin{eqnarray*}
1-\alpha&=& P\left\{-Z_{\alpha/2}\leq Z\leq Z_{\alpha/2}\right\}=P\left\{-Z_{\alpha/2}\leq \frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}}\leq Z_{\alpha/2}\right\}\\
&=&P\left\{\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\leq \mu_{1}-\mu_{2}\leq \left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right\}.
\end{eqnarray*}

Entonces los intervalos de confianza unilaterales y de dos colas al $\left(1-\alpha\right)\%$ de confianza son 

\begin{itemize}
\item[a) ] $\mu_{1}-\mu_{2}\in \left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right]$,

\item[b) ] $\mu_{1}-\mu_{2}\in \left[-\infty,\left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right]$,

\item[c) ] $\mu_{1}-\mu_{2}\in \left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}},\infty\right]$.
\end{itemize}

\begin{Note}
Si $\sigma_{1}$ y $\sigma_{2}$ son conocidas, o por lo menos se conoce una aproximaci\'on, y los tama\~nos de las muestras $n_{1}$ y $n_{2}$ son iguales, $n_{1}=n_{2}=n$, se puede determinar el tama\~no de la muestra para que el error al estimar $\mu_{1}-\mu_{2}$ usando $\overline{X}_{1}-\overline{X}_{2}$ sea menor que $E$ (valor del error deseado) al $\left(1-\alpha\right)\%$ de confianza. El tama\~no $n$ de la muestra requerido para cada muestra es
\begin{eqnarray*}
n=\left(\frac{Z_{\alpha/2}}{E}\right)^{2}\left(\sigma_{1}^{2}+\sigma_{2}^{2}\right).
\end{eqnarray*}
\end{Note}

%------------------------------------------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para dos muestras: Varianzas desconocidas e iguales}
%------------------------------------------------------------------------------------------------------------------

\begin{itemize}
\item[a) ] Si $n_{1},n_{2}\geq30$ se pueden utilizar los intervalos de la distribuci\'on normal para varianza conocida

\item[b) ] Si $n_{1},n_{2}$ son muestras peque\~nas, supongase que las poblaciones para $X_{1}$ y $X_{2}$ son normales con varianzas desconocidas y con base en el intervalo de confianza para distribuciones $t$-student
\end{itemize}


Supongamos que $X_{1}$ es una variable aleatoria con media $\mu_{1}$ y varianza $\sigma_{1}^{2}$, $X_{2}$ es una variable aleatoria con media $\mu_{2}$ y varianza $\sigma_{2}^{2}$. Todos los par\'ametros son desconocidos. Sin embargo sup\'ongase que es razonable considerar que $\sigma_{1}^{2}=\sigma_{2}^{2}=\sigma^{2}$.\medskip

Nuevamente sean $X_{1}$ y $X_{2}$ variables aleatorias independientes. $X_{1}$ con media desconocida $\mu_{1}$ y varianza muestral $S_{1}^{2}$; y $X_{2}$ con media desconocida $\mu_{2}$ y varianza muestral $S_{2}^{2}$. Dado que $S_{1}^{2}$ y $S_{2}^{2}$ son estimadores de $\sigma_{1}^{2}$, se propone el estimador $S$ de $\sigma^{2}$ como 

\begin{eqnarray*}
S_{p}^{2}=\frac{\left(n_{1}-1\right)S_{1}^{2}+\left(n_{2}-1\right)S_{2}^{2}}{n_{1}+n_{2}-2},
\end{eqnarray*}
entonces, el estad\'istico para $\mu_{1}-\mu_{2}$ es

\begin{eqnarray*}
t_{\nu}=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}},
\end{eqnarray*}
donde $t_{\nu}$ es una $t$ de student con $\nu=n_{1}+n_{2}-2$ grados de libertad.\medskip

Por lo tanto

\begin{eqnarray*}
1-\alpha=P\left\{-t_{\alpha/2,\nu}\leq t\leq t_{\alpha/2,\nu}\right\}\\
=P\left\{\left(\overline{X}_{1}-\overline{X}_{2}\right)-t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\leq \right.\\
\left.t\leq\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right\},
\end{eqnarray*}

luego, los intervalos de confianza del $\left(1-\alpha\right)\%$ para $\mu_{1}-|mu_{2}$ son 
\begin{itemize}
\item[a) ] $\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)- t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right]$.


\item[b) ] $\mu_{1}-\mu_{2}\in\left[-\infty,\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right]$.

\item[c) ] $\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)- t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}},\infty\right]$.
\end{itemize}


%------------------------------------------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para dos muestras: Varianzas desconocidas diferentes}
%------------------------------------------------------------------------------------------------------------------

Si no se tiene certeza de que $\sigma_{1}^{2}=\sigma_{2}^{2}$, se propone el estad\'istico
\begin{eqnarray*}
t^{*}=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}}},
\end{eqnarray*}
que se distribuye $t$-student con $\nu$ grados de libertad, donde

\begin{eqnarray*}
\nu=\frac{\left(\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}\right)^{2}}{\frac{S_{1}^{2}/n_{1}}{n_{1}+1}+\frac{S_{2}^{2}/n_{2}}{n_{2}+1}}-2.
\end{eqnarray*}


Entonces el intervalo de confianza de aproximadamente el $100\left(1-\alpha\right)\%$ para $\mu_{1}-\mu_{2}$ con $\sigma_{1}^{2}\neq\sigma_{2}^{2}$ es
\begin{eqnarray*}
\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-t_{\alpha/2,\nu}\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+t_{\alpha/2,\nu}\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}}\right].
\end{eqnarray*}

%------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para raz\'on de Varianzas}
%------------------------------------------------------------------------------

Supongamos que se toman dos muestras aleatorias independientes de las dos poblaciones de inter\'es. Sean $X_{1}$ y $X_{2}$ variables normales independientes con medias desconocidas $\mu_{1}$ y $\mu_{2}$ y varianzas desconocidas $\sigma_{1}^{2}$ y $\sigma_{2}^{2}$ respectivamente. Se busca un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\sigma_{1}^{2}/\sigma_{2}^{2}$. Supongamos $n_{1}$ y $n_{2}$ muestras aleatorias de $X_{1}$ y $X_{2}$ y sean $S_{1}^{2}$ y $S_{2}^{2}$ varianzas muestralres. Se sabe que 
$$F=\frac{S_{2}^{2}/\sigma_{2}^{2}}{S_{1}^{2}/\sigma_{1}^{2}},$$
se distribuye $F$ con $n_{2}-1$ y $n_{1}-1$ grados de libertad.


Por lo tanto
\begin{eqnarray*}
P\left\{F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq F\leq F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha,\\
P\left\{F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq \frac{S_{2}^{2}/\sigma_{2}^{2}}{S_{1}^{2}/\sigma_{1}^{2}}\leq F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha,
\end{eqnarray*}
luego entonces
\begin{eqnarray*}
P\left\{\frac{S_{1}^{2}}{S_{2}^{2}}F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq \frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\leq \frac{S_{1}^{2}}{S_{2}^{2}}F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha.
\end{eqnarray*}
en consecuencia

\begin{eqnarray*}
\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\in \left[\frac{S_{1}^{2}}{S_{2}^{2}}F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}, \frac{S_{1}^{2}}{S_{2}^{2}}F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right],
\end{eqnarray*}
donde
\begin{eqnarray*}
F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}=\frac{1}{F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}}.
\end{eqnarray*}

%------------------------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para diferencia de proporciones}
%------------------------------------------------------------------------------------------------


Sean dos proporciones de inter\'es $p_{1}$ y $p_{2}$. Se busca un intervalo para $p_{1}-p_{2}$ al $100\left(1-\alpha\right)\%$. Sean dos muestras independientes de tama\~no $n_{1}$ y $n_{2}$ de poblaciones infinitas de modo que $X_{1}$ y $X_{2}$ variables aleatorias binomiales independientes con par\'ametros $\left(n_{1},p_{1}\right)$ y $\left(n_{2},p_{2}\right)$.  $X_{1}$ y $X_{2}$ son  el n\'umero de observaciones que pertenecen a la clase de inter\'es correspondientes. Entonces $\hat{p}_{1}=\frac{X_{1}}{n_{1}}$ y $\hat{p}_{2}=\frac{X_{2}}{n_{2}}$ son estimadores de $p_{1}$ y $p_{2}$ respectivamente. Supongamos que se cumple la aproximaci\'on  normal a la binomial, entonces

\begin{eqnarray*}
Z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)-\left(p_{1}-p_{2}\right)}{\sqrt{\frac{p_{1}\left(1-p_{1}\right)}{n_{1}}-\frac{p_{2}\left(1-p_{2}\right)}{n_{2}}}}\sim N\left(0,1\right)\textrm{aproximadamente}
\end{eqnarray*}
por tanto

\begin{eqnarray*}
\left(\hat{p}_{1}-\hat{p}_{2}\right)-Z_{\frac{\alpha}{2}}\sqrt{\frac{\hat{p}_{1}\left(1-\hat{p}_{1}\right)}{n_{1}}+\frac{\hat{p}_{2}\left(1-\hat{p}_{2}\right)}{n_{2}}}\leq p_{1}-p_{2}\leq\left(\hat{p}_{1}-\hat{p}_{2}\right)+Z_{\frac{\alpha}{2}}\sqrt{\frac{\hat{p}_{1}\left(1-\hat{p}_{1}\right)}{n_{1}}-\frac{\hat{p}_{2}\left(1-\hat{p}_{2}\right)}{n_{2}}}
\end{eqnarray*}



%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Bases}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

%---------------------------------------------------------
\subsection{An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------



\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

\end{Note}



%---------------------------------------------------------
\subsection{An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------


\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

\end{Note}





\subsubsection{Regresi\'on Lineal Simple (RLS)}




Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y  $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}

donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.






Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$,  estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.

Entonces la ecuaci\'on (\ref{Modelo.Regresion}) se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on 
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}





Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$,  $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$,  igualar a cero y  resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene 

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}





Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}

Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto





\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}




\subsubsection{Regresi\'on Lineal Simple (RLS)}





Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y  $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}





donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$,  estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.


Entonces la ecuaci\'on (\ref{Modelo.Regresion}) se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on 
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}





Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$,  igualar a cero y  resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}





Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}

Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}




%---------------------------------------------------------
\subsection{3. An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------




\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\end{itemize}

\end{Note}




\subsubsection{3.1 Regresi\'on Lineal Simple (RLS)}





\begin{itemize}

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 





\subsubsection{3.2 M\'etodo de M\'inimos Cuadrados}




Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}
donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$, estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.







Entonces la ecuaci\'on \ref{Modelo.Regresion} se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}






Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$, igualar a cero y resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ecs.Estimadores.Regresion}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}





entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}
Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}




\subsubsection{3.3 Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}


\begin{Note}
\begin{itemize}
\item Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo.

\item Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.
\end{itemize}
\end{Note}
A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)
\end{eqnarray*}
\begin{eqnarray*}
&=&\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)\right.\\
&+&\left.E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]=\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}
\end{eqnarray*}






por lo tanto 
\begin{equation}\label{Esperanza.Beta.1}
E\left(\hat{\beta}_{1}\right)=\beta_{1}
\end{equation}
\begin{Note}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado.
\end{Note}
Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}\\
&=&\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}}
\end{eqnarray*}






por lo tanto
\begin{equation}\label{Varianza.Beta.1}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}
\end{equation}
\begin{Prop}
\begin{eqnarray*}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray*}
\end{Prop}
Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray}






sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=MC_{E}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray*}



\subsubsection{3.4 Prueba de Hip\'otesis en RLS}




\begin{itemize}
\item Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza.

\item Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$.

\end{itemize}






Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:
\begin{centering}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,

\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.

\end{itemize}
donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. 
\end{centering}





De las ecuaciones (\ref{Ecs.Estimadores.Regresion}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1}) y (\ref{Varianza.Beta.1}).
Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}







Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}






\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}=\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}

\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}






\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
\end{eqnarray*}

\begin{eqnarray*}
&=&\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}






Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}
Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados}) se puede reescribir como 
\begin{equation}\label{Suma.Total.Cuadrados.Dos}
S_{yy}=SC_{R}+SC_{E}
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right)\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.






\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx}
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}
Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$.

El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente\medskip






\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 

La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1}), con $\hat{\beta}_{1,0}=0$, es decir
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}





Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.




\subsubsection{Estimaci\'on de Intervalos en RLS}




\begin{itemize}
\item Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros.

\item El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on.

\end{itemize}






Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por
\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\small{
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}}



%---------------------------------------------------------
\subsection{3. An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------



\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\end{itemize}

\end{Note}





\subsubsection{3.1 Regresi\'on Lineal Simple (RLS)}





\begin{itemize}

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 




\subsubsection{3.2 M\'etodo de M\'inimos Cuadrados}




Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}
donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$, estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.









Entonces la ecuaci\'on \ref{Modelo.Regresion} se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}
Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$, igualar a cero y resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}





evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene
\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ecs.Estimadores.Regresion}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}





entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}
Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}





\subsubsection{3.3 Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}





\begin{Note}
\begin{itemize}
\item Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo.

\item Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.
\end{itemize}
\end{Note}
A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)
\end{eqnarray*}






\begin{eqnarray*}
&=&\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)\right.\\
&+&\left.E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]=\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}
\end{eqnarray*}
por lo tanto 
\begin{equation}\label{Esperanza.Beta.1}
E\left(\hat{\beta}_{1}\right)=\beta_{1}
\end{equation}






\begin{Note}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado.
\end{Note}
Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}\\
&=&\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}}
\end{eqnarray*}







por lo tanto
\begin{equation}\label{Varianza.Beta.1}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}
\end{equation}
\begin{Prop}
\begin{eqnarray*}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray*}
\end{Prop}

Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray}






sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=MC_{E}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray*}






%\end{document}
\subsubsection{3.4 Prueba de Hip\'otesis en RLS}




\begin{itemize}
\item Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza.

\item Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$.

\end{itemize}


Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:
\begin{centering}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,

\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.

\end{itemize}

donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. 
\end{centering}




De las ecuaciones (\ref{Ecs.Estimadores.Regresion}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1}) y (\ref{Varianza.Beta.1}).

Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}







Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}








\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}=\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}






\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}

\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
\end{eqnarray*}





\begin{eqnarray*}
&=&\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}

Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}





Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados}) se puede reescribir como 
\begin{equation}\label{Suma.Total.Cuadrados.Dos}
S_{yy}=SC_{R}+SC_{E}
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right)\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.

\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx}
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}




Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$.
									
El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente\medskip

\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 





La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1}), con $\hat{\beta}_{1,0}=0$, es decir
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.





\subsubsection{Estimaci\'on de Intervalos en RLS}



\begin{itemize}
\item Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros.

\item El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on.

\end{itemize}

Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por




\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\small{
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}}


%\end{document}
\subsubsection{Predicci\'on}


Supongamos que se tiene un valor $x_{0}$ de inter\'es, entonces la estimaci\'on puntual de este nuevo valor
\begin{equation}
\hat{y}_{0}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}
\end{equation}
\begin{Note}
Esta nueva observaci\'on es independiente de las utilizadas para obtener el modelo de regresi\'on, por tanto, el intervalo en torno a la recta de regresi\'on es inapropiado, puesto que se basa \'unicamente en los datos empleados para ajustar el modelo de regresi\'on.\\

El intervalo de confianza en torno a la recta de regresi\'on se refiere a la respuesta media verdadera $x=x_{0}$, no a observaciones futuras.
\end{Note}






Sea $y_{0}$ la observaci\'on futura en $x=x_{0}$, y sea $\hat{y}_{0}$ dada en la ecuaci\'on anterior, el estimador de $y_{0}$. Si se define la variable aleatoria $$w=y_{0}-\hat{y}_{0},$$ esta se distribuye normalmente con media cero y varianza $$V\left(w\right)=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]$$
dado que $y_{0}$ es independiente de $\hat{y}_{0}$, por lo tanto el intervalo de predicci\'on al nivel $\alpha$ para futuras observaciones $x_{0}$ es


\begin{eqnarray*}
\hat{y}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}\leq y_{0}\\
\leq \hat{y}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}.
\end{eqnarray*}





\subsubsection{Prueba de falta de ajuste}
%\frametitle{Falta de ajuste}
Es com\'un encontrar que el modelo ajustado no satisface totalmente el modelo necesario para los datos, en este caso es preciso saber qu\'e tan bueno es el modelo propuesto. Para esto se propone la siguiente prueba de hip\'otesis:
\begin{itemize}
\item[$H_{0}:$ ]El modelo propuesto se ajusta adecuademente a los datos.
\item[$H_{1}:$ ]El modelo NO se ajusta a los datos.
\end{itemize}
La prueba implica dividir la suma de cuadrados del eror o del residuo en las siguientes dos componentes:
\begin{eqnarray*}
SC_{E}=SC_{EP}+SC_{FDA}
\end{eqnarray*}



%\frametitle{Falta de ajuste}
donde $SC_{EP}$ es la suma de cuadrados atribuibles al error puro, y $SC_{FDA}$ es la suma de cuadrados atribuible a la falta de ajuste del modelo.


%%\frametitle{Falta de ajuste}
%


\subsubsection{Coeficiente de Determinaci\'on}



La cantidad
\begin{equation}
R^{2}=\frac{SC_{R}}{S_{yy}}=1-\frac{SC_{E}}{S_{yy}}
\end{equation}
se denomina coeficiente de determinaci\'on y se utiliza para saber si el modelo de regresi\'on es suficiente o no. Se puede demostrar que $0\leq R^{2}\leq1$, una manera de interpretar este valor es que si $R^{2}=k$, entonces el modelo de regresi\'on explica el $k*100\%$ de la variabilidad en los datos.
$R^{2}$ 
\begin{itemize}
\item No mide la magnitud de la pendiente de la recta de regresi\'on
\item Un valor grande de $R^{2}$ no implica una pendiente empinada.
\item No mide la suficiencia del modelo.
\item Valores grandes de $R^{2}$ no implican necesariamente que el modelo de regresi\'on proporcionar\'a predicciones precisas para futuras observaciones.
\end{itemize}



\chapter{Regresi\'on Log\'istica}


\section{Conceptos Básicos de la Regresión Logística}

La regresión logística es una técnica de modelado estadístico utilizada para predecir la probabilidad de un evento binario (es decir, un evento que tiene dos posibles resultados) en función de una o más variables independientes. A diferencia de la regresión lineal, que se utiliza para predecir valores continuos, la regresión logística se usa cuando la variable dependiente es categórica.

\section{Diferencias entre Regresión Lineal y Logística}

\subsection{Regresión Lineal}
La regresión lineal busca modelar la relación entre una variable dependiente continua $Y$ y una o más variables independientes $X_1, X_2, \ldots, X_n$ mediante una ecuación de la forma:
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
\]
donde $\beta_0, \beta_1, \ldots, \beta_n$ son los coeficientes del modelo y $\epsilon$ es el término de error.

\subsection{Regresión Logística}
La regresión logística, en cambio, modela la probabilidad de que un evento ocurra (por ejemplo, éxito vs. fracaso) utilizando la función logística. La variable dependiente $Y$ es binaria, tomando valores de 0 o 1. La ecuación de la regresión logística es:
\[
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\]
donde $p$ es la probabilidad de que $Y=1$. La función logística es:
\[
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}
\]

\section{Casos de Uso de la Regresión Logística}

La regresión logística se utiliza en una variedad de campos para problemas de clasificación binaria, tales como:
\begin{itemize}
    \item \textbf{Medicina}: Predicción de la presencia o ausencia de una enfermedad.
    \item \textbf{Marketing}: Determinación de la probabilidad de que un cliente compre un producto.
    \item \textbf{Finanzas}: Evaluación del riesgo de crédito, es decir, si un cliente va a incumplir o no con un préstamo.
    \item \textbf{Seguridad}: Detección de fraudes o intrusiones.
\end{itemize}

\section{Implementación Básica en R}

Para implementar una regresión logística en R, primero es necesario instalar y cargar los paquetes necesarios. Aquí se muestra un ejemplo básico de implementación:

\subsection{Instalación y Configuración de R y RStudio}
\begin{itemize}
    \item Descargue e instale R desde \texttt{https://cran.r-project.org/}.
    \item Descargue e instale RStudio desde \texttt{https://rstudio.com/products/rstudio/download/}.
\end{itemize}

\subsection{Introducción Básica a R}
\begin{itemize}
    \item Sintaxis básica de R.
    \item Operaciones básicas: asignación, operaciones aritméticas, funciones básicas.
\end{itemize}

\subsection{Ejemplo de Regresión Logística en R}

\begin{verbatim}
# Instalación del paquete necesario
install.packages("stats")

# Carga del paquete
library(stats)

# Ejemplo de conjunto de datos
data <- data.frame(
  outcome = c(1, 0, 1, 0, 1, 1, 0, 1, 0, 0),
  predictor = c(2.3, 1.9, 3.1, 2.8, 3.6, 2.4, 2.1, 3.3, 2.2, 1.7)
)

# Ajuste del modelo de regresión logística
model <- glm(outcome ~ predictor, data = data, family = binomial)

# Resumen del modelo
summary(model)
\end{verbatim}

En este ejemplo, se utiliza el conjunto de datos `data` que contiene una variable de resultado binaria `outcome` y una variable predictora continua `predictor`. El modelo de regresión logística se ajusta utilizando la función \texttt{glm} con la familia binomial.

\section{Conceptos Básicos}

La regresión logística es una técnica de modelado estadístico utilizada para predecir la probabilidad de un evento binario (es decir, un evento que tiene dos posibles resultados) en función de una o más variables independientes. Es ampliamente utilizada en diversas disciplinas, como medicina, economía, biología, y ciencias sociales, para analizar y predecir resultados binarios.  Un modelo de regresión logística describe cómo una variable dependiente binaria $Y$ (que puede tomar los valores $0$ o $1$) está relacionada con una o más variables independientes $X_1, X_2, \ldots, X_n$. A diferencia de la regresión lineal, que predice un valor continuo, la regresión logística predice una probabilidad que puede ser interpretada como la probabilidad de que $Y=1$ dado un conjunto de valores para $X_1, X_2, \ldots, X_n$.

\section{Regresión Lineal}

La regresión lineal es utilizada para predecir el valor de una variable dependiente continua en función de una o más variables independientes. El modelo de regresión lineal tiene la forma:
\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
\end{equation}
donde:
\begin{itemize}
    \item $Y$ es la variable dependiente.
    \item $\beta_0$ es la intersección con el eje $Y$ o término constante.
    \item $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes que representan la relación entre las variables independientes y la variable dependiente.
    \item $X_1, X_2, \ldots, X_n$ son las variables independientes.
    \item $\epsilon$ es el término de error, que representa la desviación de los datos observados de los valores predichos por el modelo.
\end{itemize}

%\subsection*{Mínimos Cuadrados Ordinarios (OLS)}

El objetivo de la regresión lineal es encontrar los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ que minimicen la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos. Este método se conoce como mínimos cuadrados ordinarios (OLS, por sus siglas en inglés). La función de costo a minimizar es:
\begin{equation}
J\left(\beta_0, \beta_1, \ldots, \beta_n\right) = \sum_{i=1}^{n}\left(y_i - \hat{y}_i\right)^2
\end{equation}
donde:
\begin{itemize}
    \item $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación.
    \item $\hat{y}_i$ es el valor predicho por el modelo para la $i$-ésima observación, dado por:
    \begin{equation}
    \hat{y}_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_n x_{in}
    \end{equation}
\end{itemize}

Para encontrar los valores óptimos de los coeficientes, se toman las derivadas parciales de la función de costo con respecto a cada coeficiente y se igualan a cero:
\begin{equation}
\frac{\partial J}{\partial \beta_j} = 0 \quad \text{para } j = 0, 1, \ldots, n
\end{equation}

Resolviendo este sistema de ecuaciones, se obtienen los valores de los coeficientes que minimizan la función de costo.

\section{Regresión Logística}

La deducción de la fórmula de la regresión logística comienza con la necesidad de modelar la probabilidad de un evento binario. Queremos encontrar una función que relacione las variables independientes con la probabilidad de que la variable dependiente tome el valor $1$. La probabilidad de que el evento ocurra, $P(Y=1)$, se denota como $p$. La probabilidad de que el evento no ocurra, $P(Y=0)$, es $1-p$. Los \textit{odds} (chances) de que ocurra el evento se definen como:
\begin{equation}
\text{odds} = \frac{p}{1-p}
\end{equation}
Los \textit{odds} nos indican cuántas veces más probable es que ocurra el evento frente a que no ocurra. Para simplificar el modelado de los \textit{odds}, aplicamos el logaritmo natural, obteniendo la función logit:
\begin{equation}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right)
\end{equation}
La transformación logit es útil porque convierte el rango de la probabilidad (0, 1) al rango de números reales $\left(-\infty, \infty\right)$. La idea clave de la regresión logística es modelar la transformación logit de la probabilidad como una combinación lineal de las variables independientes:
\begin{equation}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{equation}
Aquí, $\beta_0$ es el t\'ermino constante y $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes asociados con las variables independientes $X_1, X_2, \ldots, X_n$. Para expresar $p$ en función de una combinación lineal de las variables independientes, invertimos la transformación logit. Partimos de la ecuación:
\begin{eqnarray*}
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{eqnarray*}
Aplicamos la exponenciación a ambos lados:
\begin{eqnarray*}
\frac{p}{1-p} = e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}
\end{eqnarray*}
Despejamos $p$:
\begin{eqnarray*}
p = \frac{e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}{1 + e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}
\end{eqnarray*}

%\subsection*{Función Logística}

La expresión final que obtenemos es conocida como la función logística:
\begin{equation}\label{Eq.Logit1}
p = \frac{1}{1 + e^{-\left(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n\right)}}
\end{equation}
Esta función describe cómo las variables independientes se relacionan con la probabilidad de que el evento de interés ocurra. Los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ se estiman a partir de los datos utilizando el método de máxima verosimilitud.

\section{Método de Máxima Verosimilitud}

Para estimar los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ en la regresión logística, utilizamos el método de máxima verosimilitud. La idea es encontrar los valores de los coeficientes que maximicen la probabilidad de observar los datos dados. Esta probabilidad se expresa mediante la función de verosimilitud $L$. La función de verosimilitud $L(\beta_0, \beta_1, \ldots, \beta_n)$ para un conjunto de $n$ observaciones se define como el producto de las probabilidades de las observaciones dadas las variables independientes:

\begin{equation}\label{Eq.Verosimilitud}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{equation}

donde:
\begin{itemize}
    \item $p_i$ es la probabilidad predicha de que $Y_i = 1$,
    \item $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación.
\end{itemize}

%\subsection{Función de Log-Verosimilitud}

Trabajar directamente con esta función de verosimilitud puede ser complicado debido al producto de muchas probabilidades, especialmente si $n$ es grande. Para simplificar los cálculos, se utiliza el logaritmo de la función de verosimilitud, conocido como la función de log-verosimilitud. El uso del logaritmo simplifica significativamente la diferenciación y maximización de la función. La función de log-verosimilitud se define como:

\begin{equation}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}

Aquí, $\log$ representa el logaritmo natural. Esta transformación es válida porque el logaritmo es una función monótona creciente, lo que significa que maximizar la log-verosimilitud es equivalente a maximizar la verosimilitud original. En la regresión logística, la probabilidad $p_i$ está dada por la función logística:

\begin{equation}
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}}
\end{equation}

Sustituyendo esta expresión en la función de log-verosimilitud, obtenemos:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &= \sum_{i=1}^{n} \left[ y_i \log \left( \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}} \right) + \right. \nonumber \\
& \quad \left. (1 - y_i) \log \left( 1 - \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}} \right) \right]
\end{eqnarray*}

Simplificando esta expresión, notamos que:

\begin{eqnarray*}
\log \left( \frac{1}{1 + e^{-z}} \right) = -\log(1 + e^{-z})
\end{eqnarray*}

y

\begin{eqnarray*}
\log \left( 1 - \frac{1}{1 + e^{-z}} \right) = \log \left( \frac{e^{-z}}{1 + e^{-z}} \right) = -z - \log(1 + e^{-z})
\end{eqnarray*}

Aplicando estas identidades, la función de log-verosimilitud se convierte en:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &=& \sum_{i=1}^{n} \left[ y_i (-\log(1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})})) + \right. \nonumber \\
&& \quad \left. (1 - y_i) \left( -(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}) \right) \right]
\end{eqnarray*}

Simplificando aún más, obtenemos:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &=& \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})\right.\\
& -&\left. \log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{eqnarray*}


Para simplificar aún más la notación, podemos utilizar notación matricial. Definimos la matriz $\mathbf{X}$ de tamaño $n \times (k+1)$ y el vector de coeficientes $\boldsymbol{\beta}$ de tamaño $(k+1) \times 1$ como sigue:

\begin{equation}\label{Eq.Matricial1}
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1k} \\
1 & x_{21} & x_{22} & \ldots & x_{2k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nk}
\end{bmatrix}, \quad
\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix}
\end{equation}

Entonces, la expresión para la función de log-verosimilitud es:

\begin{equation}\label{Eq.LogLikelihood1}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i (\mathbf{X}_i \boldsymbol{\beta}) - \log(1 + e^{\mathbf{X}_i \boldsymbol{\beta}}) \right]
\end{equation}

donde $\mathbf{X}_i$ es la $i$-ésima fila de la matriz $\mathbf{X}$.  Esta notación matricial simplifica la implementación y la derivación de los estimadores de los coeficientes en la regresión logística. Utilizando métodos numéricos, como el algoritmo de Newton-Raphson, se pueden encontrar los coeficientes que maximizan la función de log-verosimilitud. Para maximizar la función de log-verosimilitud, derivamos esta función con respecto a cada uno de los coeficientes $\beta_j$ y encontramos los puntos críticos. La derivada parcial de la función de log-verosimilitud con respecto a $\beta_j$ es:

\begin{eqnarray}\label{Eq.1.14}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} \left[ y_i X_{ij} - \frac{X_{ij} e^{\mathbf{X}_i \boldsymbol{\beta}}}{1 + e^{\mathbf{X}_i \boldsymbol{\beta}}} \right]
\end{eqnarray}

Simplificando, esta derivada se puede expresar como:

\begin{eqnarray}\label{Eq.PrimeraDerivada}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} X_{ij} (y_i - p_i),\textrm{ donde }p_i = \frac{1}{1 + e^{-\mathbf{X}_i \boldsymbol{\beta}}}
\end{eqnarray}

Para encontrar los coeficientes que maximizan la log-verosimilitud, resolvemos el sistema de ecuaciones 
\begin{eqnarray*}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = 0 \textrm{ para todos los }j = 0, 1, \ldots, k. 
\end{eqnarray*}
Este sistema de ecuaciones no tiene una solución analítica cerrada, por lo que se resuelve numéricamente utilizando métodos iterativos como el algoritmo de Newton-Raphson.

\section{Método de Newton-Raphson}

El método de Newton-Raphson es un algoritmo iterativo que se utiliza para encontrar las raíces de una función. En el contexto de la regresión logística, se utiliza para maximizar la función de log-verosimilitud encontrando los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$. Este m\'etodo se basa en una aproximación de segundo orden de la función objetivo. Dado un valor inicial de los coeficientes $\boldsymbol{\beta}^{(0)}$, se actualiza iterativamente el valor de los coeficientes utilizando la fórmula:

\begin{equation}\label{Eq.Criterio0}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \mathbf{H}(\boldsymbol{\beta}^{(t)}) \right]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
\end{equation}

donde:
\begin{itemize}
    \item $\boldsymbol{\beta}^{(t)}$ es el vector de coeficientes en la $t$-ésima iteración.
    \item $\nabla \log L(\boldsymbol{\beta}^{(t)})$ es el gradiente de la función de log-verosimilitud con respecto a los coeficientes $\boldsymbol{\beta}$:

\begin{equation}\label{Eq.Gradiente1}
\nabla \log L(\boldsymbol{\beta}) = \mathbf{X}^T (\mathbf{y} - \mathbf{p})
\end{equation}

donde $\mathbf{y}$ es el vector de valores observados y $\mathbf{p}$ es el vector de probabilidades.
    \item $\mathbf{H}(\boldsymbol{\beta}^{(t)})$ es la matriz Hessiana (matriz de segundas derivadas) evaluada en $\boldsymbol{\beta}^{(t)}$:
\begin{equation}\label{Eq.Hessiana1}
\mathbf{H}(\boldsymbol{\beta}) = -\mathbf{X}^T \mathbf{W} \mathbf{X}
\end{equation}

donde $\mathbf{W}$ es una matriz diagonal de pesos con elementos $w_i = p_i (1 - p_i)$.

\end{itemize}

En resumen:

\begin{Algthm}\label{Algoritmo1}
El algoritmo Newton-Raphson para la regresión logística se puede resumir en los siguientes pasos:
\begin{enumerate}
    \item Inicializar el vector de coeficientes $\boldsymbol{\beta}^{(0)}$ (por ejemplo, con ceros o valores pequeños aleatorios).
    \item Calcular el gradiente $\nabla \log L(\boldsymbol{\beta}^{(t)})$ y la matriz Hessiana $\mathbf{H}(\boldsymbol{\beta}^{(t)})$ en la iteración $t$.
    \item Actualizar los coeficientes utilizando la fórmula:
    \begin{equation}\label{Eq.Criterio1}
    \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \mathbf{H}(\boldsymbol{\beta}^{(t)}) \right]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
    \end{equation}
    \item Repetir los pasos 2 y 3 hasta que la diferencia entre $\boldsymbol{\beta}^{(t+1)}$ y $\boldsymbol{\beta}^{(t)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}
\end{Algthm}

En resumen, el método de Newton-Raphson permite encontrar los coeficientes que maximizan la función de log-verosimilitud de manera eficiente. 

\section{Espec\'ificando}
En espec\'ifico para un conjunto de $n$ observaciones, la función de verosimilitud $L$ se define como el producto de las probabilidades individuales de observar cada dato:
\begin{equation}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{equation}
donde $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación y $p_i$ es la probabilidad predicha de que $Y_i = 1$. Aquí, $p_i$ es dado por la función logística:
\begin{equation}
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}}
\end{equation}

Tomando el logaritmo:
\begin{equation}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}

Sustituyendo $p_i$:
\begin{equation}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{equation}

Dado que el objetivo es encontrar los valores de $\beta_0, \beta_1, \ldots, \beta_n$ que maximicen la función de log-verosimilitud.  Para $\beta_j$, la derivada parcial de la función de log-verosimilitud es:
\begin{equation}
\frac{\partial \log L}{\partial \beta_j} = \sum_{i=1}^{n} \left[ y_i X_{ij} - \frac{X_{ij} e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}}{1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}} \right]
\end{equation}

Esto se simplifica a (comparar con la ecuaci\'on \ref{Eq.1.14}):
\begin{eqnarray}\label{Eq.1.25}
\frac{\partial \log L}{\partial \beta_j} = \sum_{i=1}^{n} X_{ij} (y_i - p_i)
\end{eqnarray}


Para maximizar la log-verosimilitud, resolvemos el sistema de ecuaciones $\frac{\partial \log L}{\partial \beta_j} = 0$ para todos los $j$ de 0 a $n$., mismo que se resuelve numéricamente utilizando métodos el algoritmo de Newton-Raphson. El método de Newton-Raphson se basa en una aproximación de segundo orden de la función objetivo. Dado un valor inicial de los coeficientes $\beta^{(0)}$, se iterativamente actualiza el valor de los coeficientes utilizando la fórmula:
\begin{equation}\label{Eq.Criterio1.5}
\beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
\end{equation}
donde:
\begin{itemize}
    \item $\beta^{(k)}$ es el vector de coeficientes en la $k$-ésima iteración.
    \item $\mathbf{g}(\beta^{(k)})$ es el gradiente (vector de primeras derivadas) evaluado en $\beta^{(k)}$:
\begin{equation}\label{Eq.Gradiente2}
\mathbf{g}(\beta) = \frac{\partial \log L}{\partial \beta} = \sum_{i=1}^{n} \mathbf{X}_i (y_i - p_i)
\end{equation}
donde $\mathbf{X}_i$ es el vector de valores de las variables independientes para la $i$-ésima observación (comparar con ecuaci\'on \ref{Eq.Gradiente1}).

    \item $\mathbf{H}(\beta^{(k)})$ es la matriz Hessiana (matriz de segundas derivadas) evaluada en $\beta^{(k)}$:
\begin{equation}\label{Eq.Hessiana2}
\mathbf{H}(\beta) = \frac{\partial^2 \log L}{\partial \beta \partial \beta^T} = -\sum_{i=1}^{n} p_i (1 - p_i) \mathbf{X}_i \mathbf{X}_i^T,
\end{equation}
comparar con ecuaci\'on \ref{Eq.Hessiana1}
\end{itemize}

\begin{Algthm} \label{Algoritmo2}
Los pasos del algoritmo Newton-Raphson para la regresión logística son:
\begin{enumerate}
    \item Inicializar el vector de coeficientes $\beta^{(0)}$ (por ejemplo, con ceros o valores pequeños aleatorios).
    \item Calcular el gradiente $\mathbf{g}(\beta^{(k)})$ y la matriz Hessiana $\mathbf{H}(\beta^{(k)})$ en la iteración $k$.
    \item Actualizar los coeficientes utilizando la fórmula:
    \begin{equation}\label{Eq.Criterio2}
    \beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
    \end{equation}
    \item Repetir los pasos 2 y 3 hasta que la diferencia entre $\beta^{(k+1)}$ y $\beta^{(k)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}
\end{Algthm}
Como se puede observar la diferencia entre el Algoritmo \ref{Algoritmo1} y el Algoritmo \ref{Algoritmo2} son m\'inimas

\section*{Notas finales}

En el contexto de la regresión logística, los vectores $X_1, X_2, \ldots, X_n$ representan las variables independientes. Cada $X_j$ es un vector columna que contiene los valores de la variable independiente $j$ para cada una de las $n$ observaciones. Es decir,

\begin{equation}
X_j = \begin{bmatrix}
x_{1j} \\
x_{2j} \\
\vdots \\
x_{nj}
\end{bmatrix}
\end{equation}

Para simplificar la notación y los cálculos, a menudo combinamos todos los vectores de variables independientes en una única matriz de diseño $\mathbf{X}$ de tamaño $n \times (k+1)$, donde $n$ es el número de observaciones y $k+1$ es el número de variables independientes más el término de intercepto. La primera columna de $\mathbf{X}$ corresponde a un vector de unos para el término de intercepto, y las demás columnas corresponden a los valores de las variables independientes:

\begin{equation}
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1k} \\
1 & x_{21} & x_{22} & \ldots & x_{2k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nk}
\end{bmatrix}
\end{equation}
revisar la ecuaci\'on \ref{Eq.Matricial1}. De esta forma, el modelo logit puede ser escrito de manera compacta utilizando la notación matricial:

\begin{equation}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \mathbf{X} \boldsymbol{\beta}
\end{equation}

donde $\boldsymbol{\beta}$ es el vector de coeficientes:

\begin{equation}
\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix}
\end{equation}

Así, la probabilidad $p$ se puede expresar como:

\begin{equation}\label{Eq.Logit2}
p = \frac{1}{1 + e^{-\mathbf{X} \boldsymbol{\beta}}}
\end{equation}

Comparar la ecuaci\'on anterior con la ecuaci\'on \ref{Eq.Logit1}. Esta notación matricial simplifica la implementación y la derivación de los estimadores de los coeficientes en la regresión logística. Para estimar los coeficientes $\boldsymbol{\beta}$ en la regresión logística, se utiliza el método de máxima verosimilitud. La función de verosimilitud $L(\boldsymbol{\beta})$ se define como el producto de las probabilidades de las observaciones dadas las variables independientes, recordemos la ecuaci\'on \ref{Eq.Verosimilitud}:

\begin{eqnarray}
L(\boldsymbol{\beta}) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{eqnarray}


donde $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación, y $p_i$ es la probabilidad predicha de que $Y_i = 1$.  La función de log-verosimilitud, que es más fácil de maximizar, se obtiene tomando el logaritmo natural de la función de verosimilitud (\ref{Eq.LogLikelihood1}):

\begin{equation}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}

Sustituyendo $p_i = \frac{1}{1 + e^{-\mathbf{X}_i \boldsymbol{\beta}}}$, donde $\mathbf{X}_i$ es la $i$-ésima fila de la matriz de diseño $\mathbf{X}$, obtenemos:

\begin{equation}\label{Eq.LogLikelihood2}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i (\mathbf{X}_i \boldsymbol{\beta}) - \log(1 + e^{\mathbf{X}_i \boldsymbol{\beta}}) \right]
\end{equation}

Para encontrar los valores de $\boldsymbol{\beta}$ que maximizan la función de log-verosimilitud, se utiliza un algoritmo iterativo como el método de Newton-Raphson. Este método requiere calcular el gradiente y la matriz Hessiana de la función de log-verosimilitud.


El gradiente de la función de log-verosimilitud con respecto a $\boldsymbol{\beta}$ es (\ref{Eq.Gradiente1} y \ref{Eq.Gradiente2}):

\begin{equation}
\nabla \log L(\boldsymbol{\beta}) = \mathbf{X}^T (\mathbf{y} - \mathbf{p})
\end{equation}

donde $\mathbf{y}$ es el vector de valores observados y $\mathbf{p}$ es el vector de probabilidades predichas.

La matriz Hessiana de la función de log-verosimilitud es (\ref{Eq.Hessiana1} y \ref{Eq.Hessiana2}):

\begin{equation}
\mathbf{H}(\boldsymbol{\beta}) = -\mathbf{X}^T \mathbf{W} \mathbf{X}
\end{equation}

donde $\mathbf{W}$ es una matriz diagonal de pesos con elementos $w_i = p_i (1 - p_i)$.

El método de Newton-Raphson actualiza los coeficientes $\boldsymbol{\beta}$ de la siguiente manera:

\begin{equation}\label{Eq.Criterio3}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - [\mathbf{H}(\boldsymbol{\beta}^{(t)})]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
\end{equation}

Iterando este proceso hasta que la diferencia entre $\boldsymbol{\beta}^{(t+1)}$ y $\boldsymbol{\beta}^{(t)}$ sea menor que un umbral predefinido (\ref{Eq.Criterio0}, \ref{Eq.Criterio1}, \ref{Eq.Criterio1.5} y \ref{Eq.Criterio2}), se obtienen los estimadores de máxima verosimilitud para los coeficientes de la regresión logística.


\chapter{Elementos de Probabilidad}
\section{Introducci\'on}

Los fundamentos de probabilidad y estad\'istica son esenciales para comprender y aplicar t\'ecnicas de an\'alisis de datos y modelado estad\'istico, incluyendo la regresi\'on lineal y log\'istica. Este cap\'itulo proporciona una revisi\'on de los conceptos clave en probabilidad y estad\'istica que son relevantes para estos m\'etodos.

\section{Probabilidad}

La probabilidad es una medida de la incertidumbre o el grado de creencia en la ocurrencia de un evento. Los conceptos fundamentales incluyen:

\subsection{Espacio Muestral y Eventos}

El espacio muestral, denotado como $S$, es el conjunto de todos los posibles resultados de un experimento aleatorio. Un evento es un subconjunto del espacio muestral. Por ejemplo, si lanzamos un dado, el espacio muestral es:
\begin{eqnarray*}
S = \{1, 2, 3, 4, 5, 6\}
\end{eqnarray*}
Un evento podr\'ia ser obtener un n\'umero par:
\begin{eqnarray*}
E = \{2, 4, 6\}
\end{eqnarray*}

\subsection{Definiciones de Probabilidad}

Existen varias definiciones de probabilidad, incluyendo la probabilidad cl\'asica, la probabilidad frecuentista y la probabilidad bayesiana.

\subsubsection{Probabilidad Cl\'asica}

La probabilidad cl\'asica se define como el n\'umero de resultados favorables dividido por el n\'umero total de resultados posibles:
\begin{eqnarray*}
P(E) = \frac{|E|}{|S|}
\end{eqnarray*}
donde $|E|$ es el n\'umero de elementos en el evento $E$ y $|S|$ es el n\'umero de elementos en el espacio muestral $S$.

\subsubsection{Probabilidad Frecuentista}

La probabilidad frecuentista se basa en la frecuencia relativa de ocurrencia de un evento en un gran n\'umero de repeticiones del experimento:
\begin{eqnarray*}
P(E) = \lim_{n \to \infty} \frac{n_E}{n}
\end{eqnarray*}
donde $n_E$ es el n\'umero de veces que ocurre el evento $E$ y $n$ es el n\'umero total de repeticiones del experimento.

\subsubsection{Probabilidad Bayesiana}

La probabilidad bayesiana se interpreta como un grado de creencia actualizado a medida que se dispone de nueva informaci\'on. Se basa en el teorema de Bayes:
\begin{eqnarray*}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{eqnarray*}
donde $P(A|B)$ es la probabilidad de $A$ dado $B$, $P(B|A)$ es la probabilidad de $B$ dado $A$, $P(A)$ y $P(B)$ son las probabilidades de $A$ y $B$ respectivamente.

\section{Estad\'istica Bayesiana}

La estad\'istica bayesiana proporciona un enfoque coherente para el an\'alisis de datos basado en el teorema de Bayes. Los conceptos fundamentales incluyen:

\subsection{Prior y Posterior}

\subsubsection{Distribuci\'on Prior}

La distribuci\'on prior (apriori) representa nuestra creencia sobre los par\'ametros antes de observar los datos. Es una distribuci\'on de probabilidad que refleja nuestra incertidumbre inicial sobre los par\'ametros. Por ejemplo, si creemos que un par\'ametro $\theta$ sigue una distribuci\'on normal con media $\mu_0$ y varianza $\sigma_0^2$, nuestra prior ser\'ia:
\begin{eqnarray*}
P(\theta) = \frac{1}{\sqrt{2\pi\sigma_0^2}} e^{-\frac{(\theta-\mu_0)^2}{2\sigma_0^2}}
\end{eqnarray*}

\subsubsection{Verosimilitud}

La verosimilitud (likelihood) es la probabilidad de observar los datos dados los par\'ametros. Es una funci\'on de los par\'ametros $\theta$ dada una muestra de datos $X$:
\begin{eqnarray*}
L(\theta; X) = P(X|\theta)
\end{eqnarray*}
donde $X$ son los datos observados y $\theta$ son los par\'ametros del modelo.

\subsubsection{Distribuci\'on Posterior}

La distribuci\'on posterior (a posteriori) combina la informaci\'on de la prior y la verosimilitud utilizando el teorema de Bayes. Representa nuestra creencia sobre los par\'ametros despu\'es de observar los datos:
\begin{eqnarray*}
P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)}
\end{eqnarray*}
donde $P(\theta|X)$ es la distribuci\'on posterior, $P(X|\theta)$ es la verosimilitud, $P(\theta)$ es la prior y $P(X)$ es la probabilidad marginal de los datos.

La probabilidad marginal de los datos $P(X)$ se puede calcular como:
\begin{eqnarray*}
P(X) = \int_{\Theta} P(X|\theta)P(\theta) d\theta
\end{eqnarray*}
donde $\Theta$ es el espacio de todos los posibles valores del par\'ametro $\theta$.

\section{Distribuciones de Probabilidad}

Las distribuciones de probabilidad describen c\'omo se distribuyen los valores de una variable aleatoria. Existen distribuciones de probabilidad discretas y continuas.

\subsection{Distribuciones Discretas}

Una variable aleatoria discreta toma un n\'umero finito o contable de valores. Algunas distribuciones discretas comunes incluyen:

\subsubsection{Distribuci\'on Binomial}

La distribuci\'on binomial describe el n\'umero de \'exitos en una serie de ensayos de Bernoulli independientes y con la misma probabilidad de \'exito. La funci\'on de probabilidad es:
\begin{eqnarray*}
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
\end{eqnarray*}
donde $X$ es el n\'umero de \'exitos, $n$ es el n\'umero de ensayos, $p$ es la probabilidad de \'exito en cada ensayo, y $\binom{n}{k}$ es el coeficiente binomial.

La funci\'on generadora de momentos (MGF) para la distribuci\'on binomial es:
\begin{eqnarray*}
M_X(t) = \left( 1 - p + pe^t \right)^n
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria binomial son:
\begin{eqnarray*}
E(X) &=& np \\
\text{Var}(X) &=& np(1-p)
\end{eqnarray*}

\subsubsection{Distribuci\'on de Poisson}

La distribuci\'on de Poisson describe el n\'umero de eventos que ocurren en un intervalo de tiempo fijo o en un \'area fija. La funci\'on de probabilidad es:
\begin{eqnarray*}
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
\end{eqnarray*}
donde $X$ es el n\'umero de eventos, $\lambda$ es la tasa media de eventos por intervalo, y $k$ es el n\'umero de eventos observados.

La funci\'on generadora de momentos (MGF) para la distribuci\'on de Poisson es:
\begin{eqnarray*}
M_X(t) = e^{\lambda (e^t - 1)}
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria de Poisson son:
\begin{eqnarray*}
E(X) &=& \lambda \\
\text{Var}(X) &=& \lambda
\end{eqnarray*}

\subsection{Distribuciones Continuas}

Una variable aleatoria continua toma un n\'umero infinito de valores en un intervalo continuo. Algunas distribuciones continuas comunes incluyen:

\subsubsection{Distribuci\'on Normal}

La distribuci\'on normal, tambi\'en conocida como distribuci\'on gaussiana, es una de las distribuciones m\'as importantes en estad\'istica. La funci\'on de densidad de probabilidad es:
\begin{eqnarray*}
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{eqnarray*}
donde $x$ es un valor de la variable aleatoria, $\mu$ es la media, y $\sigma$ es la desviaci\'on est\'andar.

La funci\'on generadora de momentos (MGF) para la distribuci\'on normal es:
\begin{eqnarray*}
M_X(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria normal son:
\begin{eqnarray*}
E(X) &=& \mu \\
\text{Var}(X) &=& \sigma^2
\end{eqnarray*}

\subsubsection{Distribuci\'on Exponencial}

La distribuci\'on exponencial describe el tiempo entre eventos en un proceso de Poisson. La funci\'on de densidad de probabilidad es:
\begin{eqnarray*}
f(x) = \lambda e^{-\lambda x}
\end{eqnarray*}
donde $x$ es el tiempo entre eventos y $\lambda$ es la tasa media de eventos.

La funci\'on generadora de momentos (MGF) para la distribuci\'on exponencial es:
\begin{eqnarray*}
M_X(t) = \frac{\lambda}{\lambda - t}, \quad \text{para } t < \lambda
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria exponencial son:
\begin{eqnarray*}
E(X) &=& \frac{1}{\lambda} \\
\text{Var}(X) &=& \frac{1}{\lambda^2}
\end{eqnarray*}

\section{Estad\'istica Descriptiva}

La estad\'istica descriptiva resume y describe las caracter\'isticas de un conjunto de datos. Incluye medidas de tendencia central, medidas de dispersi\'on y medidas de forma.

\subsection{Medidas de Tendencia Central}

Las medidas de tendencia central incluyen la media, la mediana y la moda.

\subsubsection{Media}

La media aritm\'etica es la suma de los valores dividida por el n\'umero de valores:
\begin{eqnarray*}
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
\end{eqnarray*}
donde $x_i$ son los valores de la muestra y $n$ es el tama\~no de la muestra.

\subsubsection{Mediana}

La mediana es el valor medio cuando los datos est\'an ordenados. Si el n\'umero de valores es impar, la mediana es el valor central. Si es par, es el promedio de los dos valores centrales.

\subsubsection{Moda}

La moda es el valor que ocurre con mayor frecuencia en un conjunto de datos.

\subsection{Medidas de Dispersi\'on}

Las medidas de dispersi\'on incluyen el rango, la varianza y la desviaci\'on est\'andar.

\subsubsection{Rango}

El rango es la diferencia entre el valor m\'aximo y el valor m\'inimo de los datos:
\begin{eqnarray*}
Rango = x_{\text{max}} - x_{\text{min}}
\end{eqnarray*}

\subsubsection{Varianza}

La varianza es la media de los cuadrados de las diferencias entre los valores y la media:
\begin{eqnarray*}
\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
\end{eqnarray*}

\subsubsection{Desviaci\'on Est\'andar}

La desviaci\'on est\'andar es la ra\'iz cuadrada de la varianza:
\begin{eqnarray*}
\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{eqnarray*}

\section{Inferencia Estad\'istica}

La inferencia estad\'istica es el proceso de sacar conclusiones sobre una poblaci\'on a partir de una muestra. Incluye la estimaci\'on de par\'ametros y la prueba de hip\'otesis.

\subsection{Estimaci\'on de Par\'ametros}

La estimaci\'on de par\'ametros implica el uso de datos muestrales para estimar los par\'ametros de una poblaci\'on.

\subsubsection{Estimador Puntual}

Un estimador puntual proporciona un \'unico valor como estimaci\'on de un par\'ametro de la poblaci\'on. Por ejemplo, la media muestral $\bar{x}$ es un estimador puntual de la media poblacional $\mu$. Otros ejemplos de estimadores puntuales son:

\begin{itemize}
    \item \textbf{Mediana muestral ($\tilde{x}$)}: Estimador de la mediana poblacional.
    \item \textbf{Varianza muestral ($s^2$)}: Estimador de la varianza poblacional $\sigma^2$, definido como:
    \begin{eqnarray*}
    s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
    \end{eqnarray*}
    \item \textbf{Desviaci\'on est\'andar muestral ($s$)}: Estimador de la desviaci\'on est\'andar poblacional $\sigma$, definido como:
    \begin{eqnarray*}
    s = \sqrt{s^2}
    \end{eqnarray*}
\end{itemize}

\subsubsection{Propiedades de los Estimadores Puntuales}

Los estimadores puntuales deben cumplir ciertas propiedades deseables, como:

\begin{itemize}
    \item \textbf{Insesgadez}: Un estimador es insesgado si su valor esperado es igual al valor del par\'ametro que estima.
    \begin{eqnarray*}
    E(\hat{\theta}) = \theta
    \end{eqnarray*}
    \item \textbf{Consistencia}: Un estimador es consistente si converge en probabilidad al valor del par\'ametro a medida que el tama\~no de la muestra tiende a infinito.
    \item \textbf{Eficiencia}: Un estimador es eficiente si tiene la varianza m\'as baja entre todos los estimadores insesgados.
\end{itemize}

\subsubsection{Estimador por Intervalo}

Un estimador por intervalo proporciona un rango de valores dentro del cual se espera que se encuentre el par\'ametro poblacional con un cierto nivel de confianza. Por ejemplo, un intervalo de confianza para la media es:
\begin{eqnarray*}
\left( \bar{x} - z \frac{\sigma}{\sqrt{n}}, \bar{x} + z \frac{\sigma}{\sqrt{n}} \right)
\end{eqnarray*}
donde $z$ es el valor cr\'itico correspondiente al nivel de confianza deseado, $\sigma$ es la desviaci\'on est\'andar poblacional y $n$ es el tama\~no de la muestra.

\subsection{Prueba de Hip\'otesis}

La prueba de hip\'otesis es un procedimiento para decidir si una afirmaci\'on sobre un par\'ametro poblacional es consistente con los datos muestrales.

\subsubsection{Hip\'otesis Nula y Alternativa}

La hip\'otesis nula ($H_0$) es la afirmaci\'on que se somete a prueba, y la hip\'otesis alternativa ($H_a$) es la afirmaci\'on que se acepta si se rechaza la hip\'otesis nula.

\subsubsection{Nivel de Significancia}

El nivel de significancia ($\alpha$) es la probabilidad de rechazar la hip\'otesis nula cuando es verdadera. Un valor com\'unmente utilizado es $\alpha = 0.05$.

\subsubsection{Estad\'istico de Prueba}

El estad\'istico de prueba es una medida calculada a partir de los datos muestrales que se utiliza para decidir si se rechaza la hip\'otesis nula. Por ejemplo, en una prueba $t$ para la media:
\begin{eqnarray*}
t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}
\end{eqnarray*}
donde $\bar{x}$ es la media muestral, $\mu_0$ es la media poblacional bajo la hip\'otesis nula, $s$ es la desviaci\'on est\'andar muestral y $n$ es el tama\~no de la muestra.

\subsubsection{P-valor}

El p-valor es la probabilidad de obtener un valor del estad\'istico de prueba al menos tan extremo como el observado, bajo la suposici\'on de que la hip\'otesis nula es verdadera. Si el p-valor es menor que el nivel de significancia $\alpha$, se rechaza la hip\'otesis nula. El p-valor se interpreta de la siguiente manera:

\begin{itemize}
    \item \textbf{P-valor bajo (p < 0.05)}: Evidencia suficiente para rechazar la hip\'otesis nula.
    \item \textbf{P-valor alto (p > 0.05)}: No hay suficiente evidencia para rechazar la hip\'otesis nula.
\end{itemize}

\subsubsection{Tipos de Errores}

En la prueba de hip\'otesis, se pueden cometer dos tipos de errores:

\begin{itemize}
    \item \textbf{Error Tipo I ($\alpha$)}: Rechazar la hip\'otesis nula cuando es verdadera.
    \item \textbf{Error Tipo II ($\beta$)}: No rechazar la hip\'otesis nula cuando es falsa.
\end{itemize}

\subsubsection{Tabla de Errores en la Prueba de Hip\'otesis}

A continuaci\'on se presenta una tabla que muestra los posibles resultados en una prueba de hip\'otesis, incluyendo los falsos positivos (error tipo I) y los falsos negativos (error tipo II):

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Hip\'otesis Nula Verdadera} & \textbf{Hip\'otesis Nula Falsa} \\
\hline
\textbf{Rechazar $H_0$} & Error Tipo I ($\alpha$) & Aceptar $H_a$ \\
\hline
\textbf{No Rechazar $H_0$} & Aceptar $H_0$ & Error Tipo II ($\beta$) \\
\hline
\end{tabular}
\caption{Resultados de la Prueba de Hip\'otesis}
\label{tab:hypothesis_testing}
\end{table}



\chapter{Matemáticas Detrás de la Regresión Logística}
\section{Introducci\'on}

La regresi\'on log\'istica es una t\'ecnica de modelado estad\'istico utilizada para predecir la probabilidad de un evento binario en funci\'on de una o m\'as variables independientes. Este cap\'itulo profundiza en las matem\'aticas subyacentes a la regresi\'on log\'istica, incluyendo la funci\'on log\'istica, la funci\'on de verosimilitud, y los m\'etodos para estimar los coeficientes del modelo.

\section{Funci\'on Log\'istica}

La funci\'on log\'istica es la base de la regresi\'on log\'istica. Esta funci\'on transforma una combinaci\'on lineal de variables independientes en una probabilidad.

\subsection{Definici\'on}

La funci\'on log\'istica se define como:
\begin{eqnarray*}
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}
\end{eqnarray*}
donde $p$ es la probabilidad de que el evento ocurra, $\beta_0, \beta_1, \ldots, \beta_n$ son los coeficientes del modelo, y $X_1, X_2, \ldots, X_n$ son las variables independientes.

\subsection{Propiedades}

La funci\'on log\'istica tiene varias propiedades importantes:
\begin{itemize}
    \item \textbf{Rango}: La funci\'on log\'istica siempre produce un valor entre 0 y 1, lo que la hace adecuada para modelar probabilidades.
    \item \textbf{Monoton\'ia}: La funci\'on es mon\'otona creciente, lo que significa que a medida que la combinaci\'on lineal de variables independientes aumenta, la probabilidad tambi\'en aumenta.
    \item \textbf{Simetr\'ia}: La funci\'on log\'istica es sim\'etrica en torno a $p = 0.5$.
\end{itemize}

\section{Funci\'on de Verosimilitud}

La funci\'on de verosimilitud se utiliza para estimar los coeficientes del modelo de regresi\'on log\'istica. Esta funci\'on mide la probabilidad de observar los datos dados los coeficientes del modelo.

\subsection{Definici\'on}

Para un conjunto de $n$ observaciones, la funci\'on de verosimilitud $L$ se define como el producto de las probabilidades individuales de observar cada dato:
\begin{eqnarray*}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{eqnarray*}
donde $y_i$ es el valor observado de la variable dependiente para la $i$-\'esima observaci\'on y $p_i$ es la probabilidad predicha de que $Y_i = 1$.

\subsection{Funci\'on de Log-Verosimilitud}

Para simplificar los c\'alculos, trabajamos con el logaritmo de la funci\'on de verosimilitud, conocido como la funci\'on de log-verosimilitud. Tomar el logaritmo convierte el producto en una suma:
\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{eqnarray*}

Sustituyendo $p_i$:
\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{eqnarray*}

\section{Estimaci\'on de Coeficientes}

Los coeficientes del modelo de regresi\'on log\'istica se estiman maximizando la funci\'on de log-verosimilitud. Este proceso generalmente se realiza mediante m\'etodos iterativos como el algoritmo de Newton-Raphson.

\subsection{Gradiente y Hessiana}

Para maximizar la funci\'on de log-verosimilitud, necesitamos calcular su gradiente y su matriz Hessiana.

\subsubsection{Gradiente}

El gradiente de la funci\'on de log-verosimilitud con respecto a los coeficientes $\beta$ es:
\begin{eqnarray*}
\mathbf{g}(\beta) = \frac{\partial \log L}{\partial \beta} = \sum_{i=1}^{n} \mathbf{X}_i (y_i - p_i)
\end{eqnarray*}
donde $\mathbf{X}_i$ es el vector de valores de las variables independientes para la $i$-\'esima observaci\'on.

\subsubsection{Hessiana}

La matriz Hessiana de la funci\'on de log-verosimilitud con respecto a los coeficientes $\beta$ es:
\begin{eqnarray*}
\mathbf{H}(\beta) = \frac{\partial^2 \log L}{\partial \beta \partial \beta^T} = -\sum_{i=1}^{n} p_i (1 - p_i) \mathbf{X}_i \mathbf{X}_i^T
\end{eqnarray*}

\subsection{Algoritmo Newton-Raphson}

El algoritmo Newton-Raphson se utiliza para encontrar los valores de los coeficientes que maximizan la funci\'on de log-verosimilitud. El algoritmo se puede resumir en los siguientes pasos:
\begin{enumerate}
    \item Inicializar el vector de coeficientes $\beta^{(0)}$ (por ejemplo, con ceros o valores peque\~nos aleatorios).
    \item Calcular el gradiente $\mathbf{g}(\beta^{(k)})$ y la matriz Hessiana $\mathbf{H}(\beta^{(k)})$ en la iteraci\'on $k$.
    \item Actualizar los coeficientes utilizando la f\'ormula:
    \begin{eqnarray*}
    \beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
    \end{eqnarray*}
    \item Repetir los pasos 2 y 3 hasta que la diferencia entre $\beta^{(k+1)}$ y $\beta^{(k)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}

\section{Validaci\'on del Modelo}

Una vez que se han estimado los coeficientes del modelo de regresi\'on log\'istica, es importante validar el modelo para asegurarse de que proporciona predicciones precisas.

\subsection{Curva ROC y AUC}

La curva ROC (Receiver Operating Characteristic) es una herramienta gr\'afica utilizada para evaluar el rendimiento de un modelo de clasificaci\'on binaria. El \'area bajo la curva (AUC) mide la capacidad del modelo para distinguir entre las clases.

\subsection{Matriz de Confusi\'on}

La matriz de confusi\'on es una tabla que resume el rendimiento de un modelo de clasificaci\'on al comparar las predicciones del modelo con los valores reales. Los t\'erminos en la matriz de confusi\'on incluyen verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos.



\chapter{Preparación de Datos y Selección de Variables}


\section{Introducci\'on}

La preparaci\'on de datos y la selecci\'on de variables son pasos cruciales en el proceso de modelado estad\'istico. Un modelo bien preparado y con las variables adecuadas puede mejorar significativamente la precisi\'on y la interpretabilidad del modelo. Este cap\'itulo proporciona una revisi\'on detallada de las t\'ecnicas de limpieza de datos, tratamiento de datos faltantes, codificaci\'on de variables categ\'oricas y selecci\'on de variables.

\section{Importancia de la Preparaci\'on de Datos}

La calidad de los datos es fundamental para el \'exito de cualquier an\'alisis estad\'istico. Los datos sin limpiar pueden llevar a modelos inexactos y conclusiones err\'oneas. La preparaci\'on de datos incluye varias etapas:
\begin{itemize}
    \item Limpieza de datos
    \item Tratamiento de datos faltantes
    \item Codificaci\'on de variables categ\'oricas
    \item Selecci\'on y transformaci\'on de variables
\end{itemize}

\section{Limpieza de Datos}

La limpieza de datos es el proceso de detectar y corregir (o eliminar) los datos incorrectos, incompletos o irrelevantes. Este proceso incluye:
\begin{itemize}
    \item Eliminaci\'on de duplicados
    \item Correcci\'on de errores tipogr\'aficos
    \item Consistencia de formato
    \item Tratamiento de valores extremos (outliers)
\end{itemize}

\section{Tratamiento de Datos Faltantes}

Los datos faltantes son un problema com\'un en los conjuntos de datos y pueden afectar la calidad de los modelos. Hay varias estrategias para manejar los datos faltantes:
\begin{itemize}
    \item \textbf{Eliminaci\'on de Datos Faltantes}: Se eliminan las filas o columnas con datos faltantes.
    \item \textbf{Imputaci\'on}: Se reemplazan los valores faltantes con estimaciones, como la media, la mediana o la moda.
    \item \textbf{Modelos Predictivos}: Se utilizan modelos predictivos para estimar los valores faltantes.
\end{itemize}

\subsection{Imputaci\'on de la Media}

Una t\'ecnica com\'un es reemplazar los valores faltantes con la media de la variable. Esto se puede hacer de la siguiente manera:
\begin{eqnarray*}
x_i = \begin{cases} 
      x_i & \text{si } x_i \text{ no es faltante} \\
      \bar{x} & \text{si } x_i \text{ es faltante}
   \end{cases}
\end{eqnarray*}
donde $\bar{x}$ es la media de la variable.

\section{Codificaci\'on de Variables Categ\'oricas}

Las variables categ\'oricas deben ser convertidas a un formato num\'erico antes de ser usadas en un modelo de regresi\'on log\'istica. Hay varias t\'ecnicas para codificar variables categ\'oricas:

\subsection{Codificaci\'on One-Hot}

La codificaci\'on one-hot crea una columna binaria para cada categor\'ia. Por ejemplo, si tenemos una variable categ\'orica con tres categor\'ias (A, B, C), se crean tres columnas:
\begin{eqnarray*}
\text{A} &=& [1, 0, 0] \\
\text{B} &=& [0, 1, 0] \\
\text{C} &=& [0, 0, 1]
\end{eqnarray*}

\subsection{Codificaci\'on Ordinal}

La codificaci\'on ordinal asigna un valor entero \'unico a cada categor\'ia, preservando el orden natural de las categor\'ias. Por ejemplo:
\begin{eqnarray*}
\text{Bajo} &=& 1 \\
\text{Medio} &=& 2 \\
\text{Alto} &=& 3
\end{eqnarray*}

\section{Selecci\'on de Variables}

La selecci\'on de variables es el proceso de elegir las variables m\'as relevantes para el modelo. Existen varias t\'ecnicas para la selecci\'on de variables:

\subsection{M\'etodos de Filtrado}

Los m\'etodos de filtrado seleccionan variables basadas en criterios estad\'isticos, como la correlaci\'on o la chi-cuadrado. Algunas t\'ecnicas comunes incluyen:
\begin{itemize}
    \item \textbf{An\'alisis de Correlaci\'on}: Se seleccionan variables con alta correlaci\'on con la variable dependiente y baja correlaci\'on entre ellas.
    \item \textbf{Pruebas de Chi-cuadrado}: Se utilizan para variables categ\'oricas para determinar la asociaci\'on entre la variable independiente y la variable dependiente.
\end{itemize}

\subsection{M\'etodos de Wrapper}

Los m\'etodos de wrapper eval\'uan m\'ultiples combinaciones de variables y seleccionan la combinaci\'on que optimiza el rendimiento del modelo. Ejemplos incluyen:
\begin{itemize}
    \item \textbf{Selecci\'on hacia Adelante}: Comienza con un modelo vac\'io y agrega variables una por una, seleccionando la variable que mejora m\'as el modelo en cada paso.
    \item \textbf{Selecci\'on hacia Atr\'as}: Comienza con todas las variables y elimina una por una, removiendo la variable que tiene el menor impacto en el modelo en cada paso.
    \item \textbf{Selecci\'on Paso a Paso}: Combina la selecci\'on hacia adelante y hacia atr\'as, agregando y eliminando variables seg\'un sea necesario.
\end{itemize}

\subsection{M\'etodos Basados en Modelos}

Los m\'etodos basados en modelos utilizan t\'ecnicas de regularizaci\'on como Lasso y Ridge para seleccionar variables. Estas t\'ecnicas a\~naden un t\'ermino de penalizaci\'on a la funci\'on de costo para evitar el sobreajuste.

\subsubsection{Regresi\'on Lasso}

La regresi\'on Lasso (Least Absolute Shrinkage and Selection Operator) a\~nade una penalizaci\'on $L_1$ a la funci\'on de costo:
\begin{eqnarray*}
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
\end{eqnarray*}
donde $\lambda$ es el par\'ametro de regularizaci\'on que controla la cantidad de penalizaci\'on.

\subsubsection{Regresi\'on Ridge}

La regresi\'on Ridge a\~nade una penalizaci\'on $L_2$ a la funci\'on de costo:
\begin{eqnarray*}
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
\end{eqnarray*}
donde $\lambda$ es el par\'ametro de regularizaci\'on.

\section{Implementaci\'on en R}

\subsection{Limpieza de Datos}

Para ilustrar la limpieza de datos en R, considere el siguiente conjunto de datos:
\begin{verbatim}
data <- data.frame(
  var1 = c(1, 2, 3, NA, 5),
  var2 = c("A", "B", "A", "B", "A"),
  var3 = c(10, 15, 10, 20, 25)
)

# Eliminaci\'on de filas con datos faltantes
data_clean <- na.omit(data)

# Imputaci\'on de la media
data$var1[is.na(data$var1)] <- mean(data$var1, na.rm = TRUE)
\end{verbatim}

\subsection{Codificaci\'on de Variables Categ\'oricas}

Para codificar variables categ\'oricas, utilice la funci\'on `model.matrix`:
\begin{verbatim}
data <- data.frame(
  var1 = c(1, 2, 3, 4, 5),
  var2 = c("A", "B", "A", "B", "A")
)

# Codificaci\'on one-hot
data_onehot <- model.matrix(~ var2 - 1, data = data)
\end{verbatim}

\subsection{Selecci\'on de Variables}

Para la selecci\'on de variables, utilice el paquete `caret`:
\begin{verbatim}
library(caret)

# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)
trainIndex <- createDataPartition(data$var1, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dataTrain <- data[trainIndex,]
dataTest <- data[-trainIndex,]

# Modelo de regresi\'on log\'istica
model <- train(var1 ~ ., data = dataTrain, method = "glm", family = "binomial")

# Selecci\'on de variables
model <- step(model, direction = "both")
summary(model)
\end{verbatim}



\chapter{Evaluación del Modelo y Validación Cruzada}


\section{Introducción}

Evaluar la calidad y el rendimiento de un modelo de regresión logística es crucial para asegurar que las predicciones sean precisas y útiles. Este capítulo se centra en las técnicas y métricas utilizadas para evaluar modelos de clasificación binaria, así como en la validación cruzada, una técnica para evaluar la generalización del modelo.

\section{Métricas de Evaluación del Modelo}

Las métricas de evaluación permiten cuantificar la precisión y el rendimiento de un modelo. Algunas de las métricas más comunes incluyen:

\subsection{Curva ROC y AUC}

La curva ROC (Receiver Operating Characteristic) es una representación gráfica de la sensibilidad (verdaderos positivos) frente a 1 - especificidad (falsos positivos). El área bajo la curva (AUC) mide la capacidad del modelo para distinguir entre las clases.

\begin{eqnarray*}
\text{Sensibilidad} &=& \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\text{Especificidad} &=& \frac{\text{TN}}{\text{TN} + \text{FP}}
\end{eqnarray*}

\subsection{Matriz de Confusión}

La matriz de confusión es una tabla que muestra el rendimiento del modelo comparando las predicciones con los valores reales. Los términos incluyen:
\begin{itemize}
    \item \textbf{Verdaderos Positivos (TP)}: Predicciones correctas de la clase positiva.
    \item \textbf{Falsos Positivos (FP)}: Predicciones incorrectas de la clase positiva.
    \item \textbf{Verdaderos Negativos (TN)}: Predicciones correctas de la clase negativa.
    \item \textbf{Falsos Negativos (FN)}: Predicciones incorrectas de la clase negativa.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Predicción Positiva} & \textbf{Predicción Negativa} \\
\hline
\textbf{Real Positiva} & TP & FN \\
\hline
\textbf{Real Negativa} & FP & TN \\
\hline
\end{tabular}
\caption{Matriz de Confusión}
\label{tab:confusion_matrix}
\end{table}

\subsection{Precisión, Recall y F1-Score}

\begin{eqnarray*}
\text{Precisión} &=& \frac{\text{TP}}{\text{TP} + \text{FP}} \\
\text{Recall} &=& \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\text{F1-Score} &=& 2 \cdot \frac{\text{Precisión} \cdot \text{Recall}}{\text{Precisión} + \text{Recall}}
\end{eqnarray*}

\subsection{Log-Loss}

La pérdida logarítmica (Log-Loss) mide la precisión de las probabilidades predichas. La fórmula es:
\begin{eqnarray*}
\text{Log-Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{eqnarray*}
donde $y_i$ son los valores reales y $p_i$ son las probabilidades predichas.

\section{Validación Cruzada}

La validación cruzada es una técnica para evaluar la capacidad de generalización de un modelo. Existen varios tipos de validación cruzada:

\subsection{K-Fold Cross-Validation}

En K-Fold Cross-Validation, los datos se dividen en K subconjuntos. El modelo se entrena K veces, cada vez utilizando K-1 subconjuntos para el entrenamiento y el subconjunto restante para la validación.

\begin{eqnarray*}
\text{Error Medio} = \frac{1}{K} \sum_{k=1}^{K} \text{Error}_k
\end{eqnarray*}

\subsection{Leave-One-Out Cross-Validation (LOOCV)}

En LOOCV, cada observación se usa una vez como conjunto de validación y las restantes como conjunto de entrenamiento. Este método es computacionalmente costoso pero útil para conjuntos de datos pequeños.

\section{Ajuste y Sobreajuste del Modelo}

El ajuste adecuado del modelo es crucial para evitar el sobreajuste (overfitting) y el subajuste (underfitting).

\subsection{Sobreajuste}

El sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento, capturando ruido y patrones irrelevantes. Los síntomas incluyen una alta precisión en el entrenamiento y baja precisión en la validación.

\subsection{Subajuste}

El subajuste ocurre cuando un modelo no captura los patrones subyacentes de los datos. Los síntomas incluyen baja precisión tanto en el entrenamiento como en la validación.

\subsection{Regularización}

La regularización es una técnica para prevenir el sobreajuste añadiendo un término de penalización a la función de costo. Las técnicas comunes incluyen:
\begin{itemize}
    \item \textbf{Regresión Lasso (L1)}
    \item \textbf{Regresión Ridge (L2)}
\end{itemize}

\section{Implementación en R}

\subsection{Evaluación del Modelo}

\begin{verbatim}
# Cargar el paquete necesario
library(caret)

# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)
trainIndex <- createDataPartition(data$var1, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dataTrain <- data[trainIndex,]
dataTest <- data[-trainIndex,]

# Entrenar el modelo de regresión logística
model <- train(var1 ~ ., data = dataTrain, method = "glm", family = "binomial")

# Predicciones en el conjunto de prueba
predictions <- predict(model, dataTest)

# Matriz de confusión
confusionMatrix(predictions, dataTest$var1)
\end{verbatim}

\subsection{Validación Cruzada}

\begin{verbatim}
# K-Fold Cross-Validation
control <- trainControl(method = "cv", number = 10)
model_cv <- train(var1 ~ ., data = dataTrain, method = "glm", 
                  family = "binomial", trControl = control)

# Evaluación del modelo con validación cruzada
print(model_cv)
\end{verbatim}



\chapter{Diagnóstico del Modelo y Ajuste de Parámetros}


\section{Introducci\'on}

El diagn\'ostico del modelo y el ajuste de par\'ametros son pasos esenciales para mejorar la precisi\'on y la robustez de los modelos de regresi\'on log\'istica. Este cap\'itulo se enfoca en las t\'ecnicas para diagnosticar problemas en los modelos y en m\'etodos para ajustar los par\'ametros de manera \'optima.

\section{Diagn\'ostico del Modelo}

El diagn\'ostico del modelo implica evaluar el rendimiento del modelo y detectar posibles problemas, como el sobreajuste, la multicolinealidad y la influencia de puntos de datos individuales.

\subsection{Residuos}

Los residuos son las diferencias entre los valores observados y los valores predichos por el modelo. El an\'alisis de residuos puede revelar patrones que indican problemas con el modelo.

\begin{eqnarray*}
\text{Residuo}_i = y_i - \hat{y}_i
\end{eqnarray*}

\subsubsection{Residuos Estudiantizados}

Los residuos estudiantizados se ajustan por la variabilidad del residuo y se utilizan para detectar outliers.

\begin{eqnarray*}
r_i = \frac{\text{Residuo}_i}{\hat{\sigma} \sqrt{1 - h_i}}
\end{eqnarray*}
donde $h_i$ es el leverage del punto de datos.

\subsection{Influencia}

La influencia mide el impacto de un punto de datos en los coeficientes del modelo. Los puntos con alta influencia pueden distorsionar el modelo.

\subsubsection{Distancia de Cook}

La distancia de Cook es una medida de la influencia de un punto de datos en los coeficientes del modelo.

\begin{eqnarray*}
D_i = \frac{r_i^2}{p} \cdot \frac{h_i}{1 - h_i}
\end{eqnarray*}
donde $p$ es el n\'umero de par\'ametros en el modelo.

\subsection{Multicolinealidad}

La multicolinealidad ocurre cuando dos o m\'as variables independientes est\'an altamente correlacionadas. Esto puede inflar las varianzas de los coeficientes y hacer que el modelo sea inestable.

\subsubsection{Factor de Inflaci\'on de la Varianza (VIF)}

El VIF mide cu\'anto se inflan las varianzas de los coeficientes debido a la multicolinealidad.

\begin{eqnarray*}
\text{VIF}_j = \frac{1}{1 - R_j^2}
\end{eqnarray*}
donde $R_j^2$ es el coeficiente de determinaci\'on de la regresi\'on de la variable $j$ contra todas las dem\'as variables.

\section{Ajuste de Par\'ametros}

El ajuste de par\'ametros implica seleccionar los valores \'optimos para los hiperpar\'ametros del modelo. Esto puede mejorar el rendimiento y prevenir el sobreajuste.

\subsection{Grid Search}

El grid search es un m\'etodo exhaustivo para ajustar los par\'ametros. Se define una rejilla de posibles valores de par\'ametros y se eval\'ua el rendimiento del modelo para cada combinaci\'on.

\subsection{Random Search}

El random search selecciona aleatoriamente combinaciones de valores de par\'ametros dentro de un rango especificado. Es menos exhaustivo que el grid search, pero puede ser m\'as eficiente.

\subsection{Bayesian Optimization}

La optimizaci\'on bayesiana utiliza modelos probabil\'isticos para seleccionar iterativamente los valores de par\'ametros m\'as prometedores.

\section{Implementaci\'on en R}

\subsection{Diagn\'ostico del Modelo}

\begin{verbatim}
# Cargar el paquete necesario
library(car)

# Residuos estudentizados
dataTrain$resid <- rstudent(model)
hist(dataTrain$resid, breaks = 20, main = "Residuos Estudentizados")

# Distancia de Cook
dataTrain$cook <- cooks.distance(model)
plot(dataTrain$cook, type = "h", main = "Distancia de Cook")

# Factor de Inflaci\'on de la Varianza
vif_values <- vif(model)
print(vif_values)
\end{verbatim}

\subsection{Ajuste de Par\'ametros}

\begin{verbatim}
# Grid Search con caret
control <- trainControl(method = "cv", number = 10)
tune_grid <- expand.grid(.alpha = c(0, 0.5, 1), .lambda = seq(0.01, 0.1, by = 0.01))

model_tune <- train(var1 ~ ., data = dataTrain, method = "glmnet", 
                    trControl = control, tuneGrid = tune_grid)

print(model_tune)
\end{verbatim}



\chapter{Interpretación de los Resultados}

\section{Introducci\'on}

Interpretar correctamente los resultados de un modelo de regresi\'on log\'istica es esencial para tomar decisiones informadas. Este cap\'itulo se centra en la interpretaci\'on de los coeficientes del modelo, las odds ratios, los intervalos de confianza y la significancia estad\'istica.

\section{Coeficientes de Regresi\'on Log\'istica}

Los coeficientes de regresi\'on log\'istica representan la relaci\'on entre las variables independientes y la variable dependiente en t\'erminos de log-odds. 

\subsection{Interpretaci\'on de los Coeficientes}

Cada coeficiente $\beta_j$ en el modelo de regresi\'on log\'istica se interpreta como el cambio en el log-odds de la variable dependiente por unidad de cambio en la variable independiente $X_j$.

\begin{eqnarray*}
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{eqnarray*}

\subsection{Signo de los Coeficientes}

\begin{itemize}
    \item \textbf{Coeficiente Positivo}: Un coeficiente positivo indica que un aumento en la variable independiente est\'a asociado con un aumento en el log-odds de la variable dependiente.
    \item \textbf{Coeficiente Negativo}: Un coeficiente negativo indica que un aumento en la variable independiente est\'a asociado con una disminuci\'on en el log-odds de la variable dependiente.
\end{itemize}

\section{Odds Ratios}

Las odds ratios proporcionan una interpretaci\'on m\'as intuitiva de los coeficientes de regresi\'on log\'istica. La odds ratio para una variable independiente $X_j$ se calcula como $e^{\beta_j}$.

\subsection{C\'alculo de las Odds Ratios}

\begin{eqnarray*}
\text{OR}_j = e^{\beta_j}
\end{eqnarray*}

\subsection{Interpretaci\'on de las Odds Ratios}

\begin{itemize}
    \item \textbf{OR > 1}: Un OR mayor que 1 indica que un aumento en la variable independiente est\'a asociado con un aumento en las odds de la variable dependiente.
    \item \textbf{OR < 1}: Un OR menor que 1 indica que un aumento en la variable independiente est\'a asociado con una disminuci\'on en las odds de la variable dependiente.
    \item \textbf{OR = 1}: Un OR igual a 1 indica que la variable independiente no tiene efecto sobre las odds de la variable dependiente.
\end{itemize}

\section{Intervalos de Confianza}

Los intervalos de confianza proporcionan una medida de la incertidumbre asociada con los estimadores de los coeficientes. Un intervalo de confianza del 95\% para un coeficiente $\beta_j$ indica que, en el 95\% de las muestras, el intervalo contendr\'a el valor verdadero de $\beta_j$.

\subsection{C\'alculo de los Intervalos de Confianza}

Para calcular un intervalo de confianza del 95\% para un coeficiente $\beta_j$, utilizamos la f\'ormula:
\begin{eqnarray*}
\beta_j \pm 1.96 \cdot \text{SE}(\beta_j)
\end{eqnarray*}
donde $\text{SE}(\beta_j)$ es el error est\'andar de $\beta_j$.

\section{Significancia Estad\'istica}

La significancia estad\'istica se utiliza para determinar si los coeficientes del modelo son significativamente diferentes de cero. Esto se eval\'ua mediante pruebas de hip\'otesis.

\subsection{Prueba de Hip\'otesis}

Para cada coeficiente $\beta_j$, la hip\'otesis nula $H_0$ es que $\beta_j = 0$. La hip\'otesis alternativa $H_a$ es que $\beta_j \neq 0$.

\subsection{P-valor}

El p-valor indica la probabilidad de obtener un coeficiente tan extremo como el observado, asumiendo que la hip\'otesis nula es verdadera. Un p-valor menor que el nivel de significancia $\alpha$ (t\'ipicamente 0.05) indica que podemos rechazar la hip\'otesis nula.

\section{Implementaci\'on en R}

\subsection{C\'alculo de Coeficientes y Odds Ratios}

\begin{verbatim}
# Cargar el paquete necesario
library(broom)

# Entrenar el modelo de regresi\'on log\'istica
model <- glm(var1 ~ ., data = dataTrain, family = "binomial")

# Coeficientes del modelo
coef(model)

# Odds ratios
exp(coef(model))
\end{verbatim}

\subsection{Intervalos de Confianza}

\begin{verbatim}
# Intervalos de confianza para los coeficientes
confint(model)

# Intervalos de confianza para las odds ratios
exp(confint(model))
\end{verbatim}

\subsection{P-valores y Significancia Estad\'istica}

\begin{verbatim}
# Resumen del modelo con p-valores
summary(model)
\end{verbatim}



\chapter{Regresión Logística Multinomial y Análisis de Supervivencia}

\section{Introducci\'on}

La regresi\'on log\'istica multinomial y el an\'alisis de supervivencia son extensiones de la regresi\'on log\'istica binaria. Este cap\'itulo se enfoca en las t\'ecnicas y aplicaciones de estos m\'etodos avanzados.

\section{Regresi\'on Log\'istica Multinomial}

La regresi\'on log\'istica multinomial se utiliza cuando la variable dependiente tiene m\'as de dos categor\'ias.

\subsection{Modelo Multinomial}

El modelo de regresi\'on log\'istica multinomial generaliza el modelo binario para manejar m\'ultiples categor\'ias. La probabilidad de que una observaci\'on pertenezca a la categor\'ia $k$ se expresa como:

\begin{eqnarray*}
P(Y = k) = \frac{e^{\beta_{0k} + \beta_{1k} X_1 + \ldots + \beta_{nk} X_n}}{\sum_{j=1}^{K} e^{\beta_{0j} + \beta_{1j} X_1 + \ldots + \beta_{nj} X_n}}
\end{eqnarray*}

\subsection{Estimaci\'on de Par\'ametros}

Los coeficientes del modelo multinomial se estiman utilizando m\'axima verosimilitud, similar a la regresi\'on log\'istica binaria.

\section{An\'alisis de Supervivencia}

El an\'alisis de supervivencia se utiliza para modelar el tiempo hasta que ocurre un evento de inter\'es, como la muerte o la falla de un componente.

\subsection{Funci\'on de Supervivencia}

La funci\'on de supervivencia $S(t)$ describe la probabilidad de que una observaci\'on sobreviva m\'as all\'a del tiempo $t$:

\begin{eqnarray*}
S(t) = P(T > t)
\end{eqnarray*}

\subsection{Modelo de Riesgos Proporcionales de Cox}

El modelo de Cox es un modelo de regresi\'on semiparam\'etrico utilizado para analizar datos de supervivencia:

\begin{eqnarray*}
h(t|X) = h_0(t) e^{\beta_1 X_1 + \ldots + \beta_p X_p}
\end{eqnarray*}
donde $h(t|X)$ es la tasa de riesgo en el tiempo $t$ dado el vector de covariables $X$ y $h_0(t)$ es la tasa de riesgo basal.

\section{Implementaci\'on en R}

\subsection{Regresi\'on Log\'istica Multinomial}

\begin{verbatim}
# Cargar el paquete necesario
library(nnet)

# Entrenar el modelo de regresi\'on log\'istica multinomial
model_multinom <- multinom(var1 ~ ., data = dataTrain)

# Resumen del modelo
summary(model_multinom)
\end{verbatim}

\subsection{An\'alisis de Supervivencia}

\begin{verbatim}
# Cargar el paquete necesario
library(survival)

# Crear el objeto de supervivencia
surv_object <- Surv(time = data$time, event = data$status)

# Ajustar el modelo de Cox
model_cox <- coxph(surv_object ~ var1 + var2, data = data)

# Resumen del modelo
summary(model_cox)
\end{verbatim}



\chapter{Implementación de Regresión Logística en Datos Reales}
\section{Introducci\'on}

Implementar un modelo de regresi\'on log\'istica en datos reales implica varias etapas, desde la limpieza de datos hasta la evaluaci\'on y validaci\'on del modelo. Este cap\'itulo presenta un ejemplo pr\'actico de la implementaci\'on de un modelo de regresi\'on log\'istica utilizando un conjunto de datos real.

\section{Conjunto de Datos}

Para este ejemplo, utilizaremos un conjunto de datos disponible p\'ublicamente que contiene informaci\'on sobre clientes bancarios. El objetivo es predecir si un cliente suscribir\'a un dep\'osito a plazo fijo.

\section{Preparaci\'on de Datos}

\subsection{Carga y Exploraci\'on de Datos}

Primero, cargamos y exploramos el conjunto de datos para entender su estructura y contenido.

\begin{verbatim}
# Cargar el paquete necesario
library(dplyr)

# Cargar el conjunto de datos
data <- read.csv("bank.csv")

# Explorar los datos
str(data)
summary(data)
\end{verbatim}

\subsection{Limpieza de Datos}

El siguiente paso es limpiar los datos, lo que incluye tratar los valores faltantes y eliminar las duplicidades.

\begin{verbatim}
# Eliminar duplicados
data <- data %>% distinct()

# Imputar valores faltantes (si existen)
data <- data %>% mutate_if(is.numeric, ~ifelse(is.na(.), mean(., na.rm = TRUE), .))
\end{verbatim}

\subsection{Codificaci\'on de Variables Categ\'oricas}

Convertimos las variables categ\'oricas en variables num\'ericas utilizando la codificaci\'on one-hot.

\begin{verbatim}
# Codificaci\'on one-hot de variables categ\'oricas
data <- data %>% mutate(across(where(is.factor), ~ as.numeric(as.factor(.))))
\end{verbatim}

\section{Divisi\'on de Datos}

Dividimos los datos en conjuntos de entrenamiento y prueba.

\begin{verbatim}
# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)
trainIndex <- createDataPartition(data$y, p = .8, list = FALSE, times = 1)
dataTrain <- data[trainIndex,]
dataTest <- data[-trainIndex,]
\end{verbatim}

\section{Entrenamiento del Modelo}

Entrenamos un modelo de regresi\'on log\'istica utilizando el conjunto de entrenamiento.

\begin{verbatim}
# Entrenar el modelo de regresi\'on log\'istica
model <- glm(y ~ ., data = dataTrain, family = "binomial")

# Resumen del modelo
summary(model)
\end{verbatim}

\section{Evaluaci\'on del Modelo}

Evaluamos el rendimiento del modelo utilizando el conjunto de prueba.

\begin{verbatim}
# Predicciones en el conjunto de prueba
predictions <- predict(model, dataTest, type = "response")

# Convertir probabilidades a etiquetas
predicted_labels <- ifelse(predictions > 0.5, 1, 0)

# Matriz de confusi\'on
confusionMatrix(predicted_labels, dataTest$y)
\end{verbatim}

\section{Interpretaci\'on de los Resultados}

Interpretamos los coeficientes del modelo y las odds ratios.

\begin{verbatim}
# Coeficientes del modelo
coef(model)

# Odds ratios
exp(coef(model))
\end{verbatim}



\chapter{Resumen y Proyecto Final}
\section{Resumen de Conceptos Clave}

En este curso, hemos cubierto una variedad de conceptos y t\'ecnicas esenciales para la regresi\'on log\'istica. Los conceptos clave incluyen:

\begin{itemize}
    \item \textbf{Fundamentos de Probabilidad y Estad\'istica}: Comprensi\'on de distribuciones de probabilidad, medidas de tendencia central y dispersi\'on, inferencia estad\'istica y pruebas de hip\'otesis.
    \item \textbf{Regresi\'on Log\'istica}: Modelo de regresi\'on log\'istica binaria y multinomial, interpretaci\'on de coeficientes y odds ratios, m\'etodos de estimaci\'on y validaci\'on.
    \item \textbf{Preparaci\'on de Datos}: Limpieza de datos, tratamiento de valores faltantes, codificaci\'on de variables categ\'oricas y selecci\'on de variables.
    \item \textbf{Evaluaci\'on del Modelo}: Curva ROC, AUC, matriz de confusi\'on, precisi\'on, recall, F1-score y validaci\'on cruzada.
    \item \textbf{Diagn\'ostico del Modelo}: An\'alisis de residuos, influencia, multicolinealidad y ajuste de par\'ametros.
    \item \textbf{An\'alisis de Supervivencia}: Modelos de supervivencia, funci\'on de supervivencia y modelos de riesgos proporcionales de Cox.
\end{itemize}

\section{Buenas Pr\'acticas}

Al implementar modelos de regresi\'on log\'istica, es importante seguir buenas pr\'acticas para garantizar la precisi\'on y la robustez de los modelos. Algunas buenas pr\'acticas incluyen:

\begin{itemize}
    \item \textbf{Exploraci\'on y Preparaci\'on de Datos}: Realizar un an\'alisis exploratorio exhaustivo y preparar los datos adecuadamente antes de construir el modelo.
    \item \textbf{Evaluaci\'on y Validaci\'on del Modelo}: Utilizar m\'etricas adecuadas para evaluar el rendimiento del modelo y validar el modelo utilizando t\'ecnicas como la validaci\'on cruzada.
    \item \textbf{Interpretaci\'on de Resultados}: Interpretar correctamente los coeficientes del modelo y las odds ratios, y comunicar los resultados de manera clara y concisa.
    \item \textbf{Revisi\'on y Ajuste del Modelo}: Diagnosticar problemas en el modelo y ajustar los par\'ametros para mejorar el rendimiento.
\end{itemize}

\section{Proyecto Final}

Para aplicar los conceptos y t\'ecnicas aprendidos en este curso, te proponemos realizar un proyecto final utilizando un conjunto de datos de tu elecci\'on. El proyecto debe incluir las siguientes etapas:

\subsection{Selecci\'on del Conjunto de Datos}

Elige un conjunto de datos relevante que contenga una variable dependiente binaria o multinomial y varias variables independientes.

\subsection{Exploraci\'on y Preparaci\'on de Datos}

Realiza un an\'alisis exploratorio de los datos y prepara los datos para el modelado. Esto incluye la limpieza de datos, el tratamiento de valores faltantes y la codificaci\'on de variables categ\'oricas.

\subsection{Entrenamiento y Evaluaci\'on del Modelo}

Entrena un modelo de regresi\'on log\'istica utilizando el conjunto de datos preparado y eval\'ua su rendimiento utilizando m\'etricas apropiadas.

\subsection{Interpretaci\'on de Resultados}

Interpreta los coeficientes del modelo y las odds ratios, y proporciona una explicaci\'on clara de los resultados.

\subsection{Presentaci\'on del Proyecto}

Presenta tu proyecto en un informe detallado que incluya la descripci\'on del conjunto de datos, los pasos de preparaci\'on y modelado, los resultados del modelo y las conclusiones.



%==<>====<>====<>====<>====<>====<>====<>====<>====<>====<>====
\part{SEGUNDA PARTE: ANALISIS DE SUPERVIVENCIA}
%==<>====<>====<>====<>====<>====<>====<>====<>====<>====<>====

\chapter{Introducción al Análisis de Supervivencia}

\section{Conceptos Básicos}
El análisis de supervivencia es una rama de la estad\'istica que se ocupa del análisis del tiempo que transcurre hasta que ocurre un evento de inter\'es, com\'unmente referido como "tiempo de falla". Este campo es ampliamente utilizado en medicina, biolog\'ia, ingenier\'ia, ciencias sociales, y otros campos.

\section{Definici\'on de Eventos y Tiempos}
En el análisis de supervivencia, un "evento" se refiere a la ocurrencia de un evento espec\'ifico, como la muerte, la falla de un componente, la reca\'ida de una enfermedad, etc. El "tiempo de supervivencia" es el tiempo que transcurre desde un punto de inicio definido hasta la ocurrencia del evento.

\section{Censura}
La censura ocurre cuando la informaci\'on completa sobre el tiempo hasta el evento no está disponible para todos los individuos en el estudio. Hay tres tipos principales de censura:
\begin{itemize}
    \item \textbf{Censura a la derecha:} Ocurre cuando el evento de inter\'es no se ha observado para algunos sujetos antes del final del estudio.
    \item \textbf{Censura a la izquierda:} Ocurre cuando el evento de inter\'es ocurri\'o antes del inicio del periodo de observaci\'on.
    \item \textbf{Censura por intervalo:} Ocurre cuando el evento de inter\'es se sabe que ocurri\'o en un intervalo de tiempo, pero no se conoce el momento exacto.
\end{itemize}

\section{Funci\'on de Supervivencia}
La funci\'on de supervivencia, $S(t)$, se define como la probabilidad de que un individuo sobreviva más allá de un tiempo $t$. Matemáticamente, se expresa como:
\begin{eqnarray*}
S(t) = P(T > t)
\end{eqnarray*}
donde $T$ es una variable aleatoria que representa el tiempo hasta el evento. La funci\'on de supervivencia tiene las siguientes propiedades:
\begin{itemize}
    \item $S(0) = 1$: Esto indica que al inicio (tiempo $t=0$), la probabilidad de haber experimentado el evento es cero, por lo tanto, la supervivencia es del 100%.
    \item $\lim_{t \to \infty} S(t) = 0$: A medida que el tiempo tiende al infinito, la probabilidad de que cualquier individuo a\'un no haya experimentado el evento tiende a cero.
    \item $S(t)$ es una funci\'on no creciente: Esto significa que a medida que el tiempo avanza, la probabilidad de supervivencia no aumenta.
\end{itemize}

\section{Funci\'on de Densidad de Probabilidad}
La funci\'on de densidad de probabilidad $f(t)$ describe la probabilidad de que el evento ocurra en un instante de tiempo espec\'ifico. Se define como:
\begin{eqnarray*}
f(t) = \frac{dF(t)}{dt}
\end{eqnarray*}
donde $F(t)$ es la funci\'on de distribuci\'on acumulada, $F(t) = P(T \leq t)$. La relaci\'on entre $S(t)$ y $f(t)$ es:
\begin{eqnarray*}
f(t) = -\frac{dS(t)}{dt}
\end{eqnarray*}

\section{Funci\'on de Riesgo}
La funci\'on de riesgo, $\lambda(t)$, tambi\'en conocida como funci\'on de tasa de fallas o hazard rate, se define como la tasa instant\'anea de ocurrencia del evento en el tiempo $t$, dado que el individuo ha sobrevivido hasta el tiempo $t$. Matem\'aticamente, se expresa como:
\begin{eqnarray*}
\lambda(t) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t}
\end{eqnarray*}
Esto se puede reescribir usando $f(t)$ y $S(t)$ como:
\begin{eqnarray*}
\lambda(t) = \frac{f(t)}{S(t)}
\end{eqnarray*}

\section{Relaci\'on entre Funci\'on de Supervivencia y Funci\'on de Riesgo}
La funci\'on de supervivencia y la funci\'on de riesgo est\'an relacionadas a trav\'es de la siguiente ecuaci\'on:
\begin{eqnarray*}
S(t) = \exp\left(-\int_0^t \lambda(u) \, du\right)
\end{eqnarray*}
Esta f\'ormula se deriva del hecho de que la funci\'on de supervivencia es la probabilidad acumulativa de no haber experimentado el evento hasta el tiempo $t$, y $\lambda(t)$ es la tasa instant\'anea de ocurrencia del evento.

La funci\'on de riesgo tambi\'en puede ser expresada como:
\begin{eqnarray*}
\lambda(t) = -\frac{d}{dt} \log S(t)
\end{eqnarray*}

\section{Deducci\'on de la Funci\'on de Supervivencia}
La relaci\'on entre la funci\'on de supervivencia y la funci\'on de riesgo se puede deducir integrando la funci\'on de riesgo:
\begin{eqnarray*}
S(t) &=& \exp\left(-\int_0^t \lambda(u) \, du\right) \\
\log S(t) &=& -\int_0^t \lambda(u) \, du \\
\frac{d}{dt} \log S(t) &=& -\lambda(t) \\
\lambda(t) &=& -\frac{d}{dt} \log S(t)
\end{eqnarray*}

\section{Ejemplo de C\'alculo}
Supongamos que tenemos una muestra de tiempos de supervivencia $T_1, T_2, \ldots, T_n$. Podemos estimar la funci\'on de supervivencia emp\'irica como:
\begin{eqnarray*}
\hat{S}(t) = \frac{\text{N\'umero de individuos que sobreviven m\'as all\'a de } t}{\text{N\'umero total de individuos en riesgo en } t}
\end{eqnarray*}
y la funci\'on de riesgo emp\'irica como:
\begin{eqnarray*}
\hat{\lambda}(t) = \frac{\text{N\'umero de eventos en } t}{\text{N\'umero de individuos en riesgo en } t}
\end{eqnarray*}

\section{Conclusi\'on}
El an\'alisis de supervivencia es una herramienta poderosa para analizar datos de tiempo hasta evento. Entender los conceptos b\'asicos como la funci\'on de supervivencia y la funci\'on de riesgo es fundamental para el an\'alisis m\'as avanzado.


\chapter{Función de Supervivencia y Función de Riesgo}
\section{Introducci\'on}
Este cap\'itulo profundiza en la definici\'on y propiedades de la funci\'on de supervivencia y la funci\'on de riesgo, dos conceptos fundamentales en el análisis de supervivencia. Entender estas funciones y su relaci\'on es crucial para modelar y analizar datos de tiempo hasta evento.

\section{Funci\'on de Supervivencia}
La funci\'on de supervivencia, $S(t)$, describe la probabilidad de que un individuo sobreviva más allá de un tiempo $t$. Formalmente, se define como:
\begin{eqnarray*}
S(t) = P(T > t)
\end{eqnarray*}
donde $T$ es una variable aleatoria que representa el tiempo hasta el evento.

\subsection{Propiedades de la Funci\'on de Supervivencia}
La funci\'on de supervivencia tiene varias propiedades importantes:
\begin{itemize}
    \item $S(0) = 1$: Indica que la probabilidad de haber experimentado el evento en el tiempo 0 es cero.
    \item $\lim_{t \to \infty} S(t) = 0$: A medida que el tiempo tiende al infinito, la probabilidad de supervivencia tiende a cero.
    \item $S(t)$ es una funci\'on no creciente: A medida que el tiempo avanza, la probabilidad de supervivencia no aumenta.
\end{itemize}

\subsection{Derivaci\'on de $S(t)$}
Si la funci\'on de densidad de probabilidad $f(t)$ del tiempo de supervivencia $T$ es conocida, la funci\'on de supervivencia puede derivarse como:
\begin{eqnarray*}
S(t) &=& P(T > t) \\
     &=& 1 - P(T \leq t) \\
     &=& 1 - F(t) \\
     &=& 1 - \int_0^t f(u) \, du
\end{eqnarray*}
donde $F(t)$ es la funci\'on de distribuci\'on acumulada.

\subsection{Ejemplo de Cálculo de $S(t)$}
Consideremos un ejemplo donde el tiempo de supervivencia $T$ sigue una distribuci\'on exponencial con tasa $\lambda$. La funci\'on de densidad de probabilidad $f(t)$ es:
\begin{eqnarray*}
f(t) = \lambda e^{-\lambda t}, \quad t \geq 0
\end{eqnarray*}
La funci\'on de distribuci\'on acumulada $F(t)$ es:
\begin{eqnarray*}
F(t) = \int_0^t \lambda e^{-\lambda u} \, du = 1 - e^{-\lambda t}
\end{eqnarray*}
Por lo tanto, la funci\'on de supervivencia $S(t)$ es:
\begin{eqnarray*}
S(t) = 1 - F(t) = e^{-\lambda t}
\end{eqnarray*}

\section{Funci\'on de Riesgo}
La funci\'on de riesgo, $\lambda(t)$, proporciona la tasa instant\'anea de ocurrencia del evento en el tiempo $t$, dado que el individuo ha sobrevivido hasta el tiempo $t$. Matem\'aticamente, se define como:
\begin{eqnarray*}
\lambda(t) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t}
\end{eqnarray*}

\subsection{Relaci\'on entre $\lambda(t)$ y $f(t)$}
La funci\'on de riesgo se puede relacionar con la funci\'on de densidad de probabilidad $f(t)$ y la funci\'on de supervivencia $S(t)$ de la siguiente manera:
\begin{eqnarray*}
\lambda(t) &=& \frac{f(t)}{S(t)}
\end{eqnarray*}

\subsection{Derivaci\'on de $\lambda(t)$}
La derivaci\'on de $\lambda(t)$ se basa en la definici\'on condicional de la probabilidad:
\begin{eqnarray*}
\lambda(t) &=& \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t} \\
           &=& \lim_{\Delta t \to 0} \frac{\frac{P(t \leq T < t + \Delta t \text{ y } T \geq t)}{P(T \geq t)}}{\Delta t} \\
           &=& \lim_{\Delta t \to 0} \frac{\frac{P(t \leq T < t + \Delta t)}{P(T \geq t)}}{\Delta t} \\
           &=& \frac{f(t)}{S(t)}
\end{eqnarray*}

\section{Relaci\'on entre Funci\'on de Supervivencia y Funci\'on de Riesgo}
La funci\'on de supervivencia y la funci\'on de riesgo est\'an estrechamente relacionadas. La relaci\'on se expresa mediante la siguiente ecuaci\'on:
\begin{eqnarray*}
S(t) = \exp\left(-\int_0^t \lambda(u) \, du\right)
\end{eqnarray*}

\subsection{Deducci\'on de la Relaci\'on}
Para deducir esta relaci\'on, consideramos la derivada logar\'itmica de la funci\'on de supervivencia:
\begin{eqnarray*}
S(t) &=& \exp\left(-\int_0^t \lambda(u) \, du\right) \\
\log S(t) &=& -\int_0^t \lambda(u) \, du \\
\frac{d}{dt} \log S(t) &=& -\lambda(t) \\
\lambda(t) &=& -\frac{d}{dt} \log S(t)
\end{eqnarray*}

\section{Interpretaci\'on de la Funci\'on de Riesgo}
La funci\'on de riesgo, $\lambda(t)$, se interpreta como la tasa instant\'anea de ocurrencia del evento por unidad de tiempo, dado que el individuo ha sobrevivido hasta el tiempo $t$. Es una medida local del riesgo de falla en un instante espec\'ifico.

\subsection{Ejemplo de C\'alculo de $\lambda(t)$}
Consideremos nuevamente el caso donde el tiempo de supervivencia $T$ sigue una distribuci\'on exponencial con tasa $\lambda$. La funci\'on de densidad de probabilidad $f(t)$ es:
\begin{eqnarray*}
f(t) = \lambda e^{-\lambda t}
\end{eqnarray*}
La funci\'on de supervivencia $S(t)$ es:
\begin{eqnarray*}
S(t) = e^{-\lambda t}
\end{eqnarray*}
La funci\'on de riesgo $\lambda(t)$ se calcula como:
\begin{eqnarray*}
\lambda(t) &=& \frac{f(t)}{S(t)} \\
           &=& \frac{\lambda e^{-\lambda t}}{e^{-\lambda t}} \\
           &=& \lambda
\end{eqnarray*}
En este caso, $\lambda(t)$ es constante y igual a $\lambda$, lo que es una caracter\'istica de la distribuci\'on exponencial.

\section{Funciones de Riesgo Acumulada y Media Residual}
La funci\'on de riesgo acumulada $H(t)$ se define como:
\begin{eqnarray*}
H(t) = \int_0^t \lambda(u) \, du
\end{eqnarray*}
Esta funci\'on proporciona la suma acumulada de la tasa de riesgo hasta el tiempo $t$.

La funci\'on de vida media residual $e(t)$ se define como la esperanza del tiempo de vida restante dado que el individuo ha sobrevivido hasta el tiempo $t$:
\begin{eqnarray*}
e(t) = \mathbb{E}[T - t \mid T > t] = \int_t^\infty S(u) \, du
\end{eqnarray*}

\section{Ejemplo de C\'alculo de Funci\'on de Riesgo Acumulada y Vida Media Residual}
Consideremos nuevamente la distribuci\'on exponencial con tasa $\lambda$. La funci\'on de riesgo acumulada $H(t)$ es:
\begin{eqnarray*}
H(t) &=& \int_0^t \lambda \, du \\
     &=& \lambda t
\end{eqnarray*}

La funci\'on de vida media residual $e(t)$ es:
\begin{eqnarray*}
e(t) &=& \int_t^\infty e^{-\lambda u} \, du \\
     &=& \left[ \frac{-1}{\lambda} e^{-\lambda u} \right]_t^\infty \\
     &=& \frac{1}{\lambda} e^{-\lambda t} \\
     &=& \frac{1}{\lambda}
\end{eqnarray*}
En este caso, la vida media residual es constante e igual a $\frac{1}{\lambda}$, otra caracter\'istica de la distribuci\'on exponencial.

\section{Conclusi\'on}
La funci\'on de supervivencia y la funci\'on de riesgo son herramientas fundamentales en el an\'alisis de supervivencia. Entender su definici\'on, propiedades, y la relaci\'on entre ellas es esencial para modelar y analizar correctamente los datos de tiempo hasta evento. Las funciones de riesgo acumulada y vida media residual proporcionan informaci\'on adicional sobre la din\'amica del riesgo a lo largo del tiempo.



\chapter{Estimador de Kaplan-Meier}

\section{Introducci\'on}
El estimador de Kaplan-Meier, tambi\'en conocido como la funci\'on de supervivencia emp\'irica, es una herramienta no param\'etrica para estimar la funci\'on de supervivencia a partir de datos censurados. Este m\'etodo es especialmente \'util cuando los tiempos de evento están censurados a la derecha.

\section{Definici\'on del Estimador de Kaplan-Meier}
El estimador de Kaplan-Meier se define como:
\begin{eqnarray*}
\hat{S}(t) = \prod_{t_i \leq t} \left(1 - \frac{d_i}{n_i}\right)
\end{eqnarray*}
donde:
\begin{itemize}
    \item $t_i$ es el tiempo del $i$-\'esimo evento,
    \item $d_i$ es el n\'umero de eventos que ocurren en $t_i$,
    \item $n_i$ es el n\'umero de individuos en riesgo justo antes de $t_i$.
\end{itemize}

\section{Propiedades del Estimador de Kaplan-Meier}
El estimador de Kaplan-Meier tiene las siguientes propiedades:
\begin{itemize}
    \item Es una funci\'on escalonada que disminuye en los tiempos de los eventos observados.
    \item Puede manejar datos censurados a la derecha.
    \item Proporciona una estimaci\'on no param\'etrica de la funci\'on de supervivencia.
\end{itemize}

\subsection{Funci\'on Escalonada}
La funci\'on escalonada del estimador de Kaplan-Meier significa que $\hat{S}(t)$ permanece constante entre los tiempos de los eventos y disminuye en los tiempos de los eventos. Matem\'aticamente, si $t_i$ es el tiempo del $i$-\'esimo evento, entonces:
\begin{eqnarray*}
\hat{S}(t) = \hat{S}(t_i) \quad \text{para} \ t_i \leq t < t_{i+1}
\end{eqnarray*}

\subsection{Manejo de Datos Censurados}
El estimador de Kaplan-Meier maneja datos censurados a la derecha al ajustar la estimaci\'on de la funci\'on de supervivencia s\'olo en los tiempos en que ocurren eventos. Si un individuo es censurado antes de experimentar el evento, no contribuye a la disminuci\'on de $\hat{S}(t)$ en el tiempo de censura. Esto asegura que la censura no sesga la estimaci\'on de la supervivencia.

\subsection{Estimaci\'on No Param\'etrica}
El estimador de Kaplan-Meier es no param\'etrico porque no asume ninguna forma espec\'ifica para la distribuci\'on de los tiempos de supervivencia. En cambio, utiliza la informaci\'on emp\'irica disponible para estimar la funci\'on de supervivencia.

\section{Deducci\'on del Estimador de Kaplan-Meier}
La deducci\'on del estimador de Kaplan-Meier se basa en el principio de probabilidad condicional. Consideremos un conjunto de tiempos de supervivencia observados $t_1, t_2, \ldots, t_k$ con eventos en cada uno de estos tiempos. El estimador de la probabilidad de supervivencia m\'as all\'a del tiempo $t$ es el producto de las probabilidades de sobrevivir m\'as all\'a de cada uno de los tiempos de evento observados hasta $t$.

\subsection{Probabilidad Condicional}
La probabilidad de sobrevivir m\'as all\'a de $t_i$, dado que el individuo ha sobrevivido justo antes de $t_i$, es:
\begin{eqnarray*}
P(T > t_i \mid T \geq t_i) = 1 - \frac{d_i}{n_i}
\end{eqnarray*}
donde $d_i$ es el n\'umero de eventos en $t_i$ y $n_i$ es el n\'umero de individuos en riesgo justo antes de $t_i$.

\subsection{Producto de Probabilidades Condicionales}
La probabilidad de sobrevivir m\'as all\'a de un tiempo $t$ cualquiera, dada la secuencia de tiempos de evento, es el producto de las probabilidades condicionales de sobrevivir m\'as all\'a de cada uno de los tiempos de evento observados hasta $t$. As\'i, el estimador de Kaplan-Meier se obtiene como:
\begin{eqnarray*}
\hat{S}(t) = \prod_{t_i \leq t} \left(1 - \frac{d_i}{n_i}\right)
\end{eqnarray*}

\section{Ejemplo de C\'alculo}
Supongamos que tenemos los siguientes tiempos de supervivencia observados para cinco individuos: 2, 3, 5, 7, 8. Supongamos adem\'as que tenemos censura a la derecha en el tiempo 10. Los tiempos de evento y el n\'umero de individuos en riesgo justo antes de cada evento son:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
Tiempo ($t_i$) & Eventos ($d_i$) & En Riesgo ($n_i$) \\
\hline
2 & 1 & 5 \\
3 & 1 & 4 \\
5 & 1 & 3 \\
7 & 1 & 2 \\
8 & 1 & 1 \\
\hline
\end{tabular}
\caption{Ejemplo de c\'alculo del estimador de Kaplan-Meier}
\end{table}

Usando estos datos, el estimador de Kaplan-Meier se calcula como:
\begin{eqnarray*}
\hat{S}(2) &=& 1 - \frac{1}{5} = 0.8 \\
\hat{S}(3) &=& 0.8 \times \left(1 - \frac{1}{4}\right) = 0.8 \times 0.75 = 0.6 \\
\hat{S}(5) &=& 0.6 \times \left(1 - \frac{1}{3}\right) = 0.6 \times 0.6667 = 0.4 \\
\hat{S}(7) &=& 0.4 \times \left(1 - \frac{1}{2}\right) = 0.4 \times 0.5 = 0.2 \\
\hat{S}(8) &=& 0.2 \times \left(1 - \frac{1}{1}\right) = 0.2 \times 0 = 0 \\
\end{eqnarray*}

\section{Intervalos de Confianza para el Estimador de Kaplan-Meier}
Para calcular intervalos de confianza para el estimador de Kaplan-Meier, se puede usar la transformaci\'on logar\'itmica y la aproximaci\'on normal. Un intervalo de confianza aproximado para $\log(-\log(\hat{S}(t)))$ se obtiene como:
\begin{eqnarray*}
\log(-\log(\hat{S}(t))) \pm z_{\alpha/2} \sqrt{\frac{1}{d_i(n_i - d_i)}}
\end{eqnarray*}
donde $z_{\alpha/2}$ es el percentil correspondiente de la distribuci\'on normal est\'andar.

\section{Transformaci\'on Logar\'itmica Inversa}
La transformaci\'on logar\'itmica inversa se utiliza para obtener los l\'imites del intervalo de confianza para $S(t)$:
\begin{eqnarray*}
\hat{S}(t) = \exp\left(-\exp\left(\log(-\log(\hat{S}(t))) \pm z_{\alpha/2} \sqrt{\frac{1}{d_i(n_i - d_i)}}\right)\right)
\end{eqnarray*}

\section{C\'alculo Detallado de Intervalos de Confianza}
Para un c\'alculo m\'as detallado de los intervalos de confianza, consideremos un tiempo espec\'ifico $t_j$. La varianza del estimador de Kaplan-Meier en $t_j$ se puede estimar usando Greenwood's formula:
\begin{eqnarray*}
\text{Var}(\hat{S}(t_j)) = \hat{S}(t_j)^2 \sum_{t_i \leq t_j} \frac{d_i}{n_i(n_i - d_i)}
\end{eqnarray*}
El intervalo de confianza aproximado para $\hat{S}(t_j)$ es entonces:
\begin{eqnarray*}
\hat{S}(t_j) \pm z_{\alpha/2} \sqrt{\text{Var}(\hat{S}(t_j))}
\end{eqnarray*}

\section{Ejemplo de Intervalo de Confianza}
Supongamos que en el ejemplo anterior queremos calcular el intervalo de confianza para $\hat{S}(3)$. Primero, calculamos la varianza:
\begin{eqnarray*}
\text{Var}(\hat{S}(3)) &=& \hat{S}(3)^2 \left( \frac{1}{5 \times 4} + \frac{1}{4 \times 3} \right) \\
                       &=& 0.6^2 \left( \frac{1}{20} + \frac{1}{12} \right) \\
                       &=& 0.36 \left( 0.05 + 0.0833 \right) \\
                       &=& 0.36 \times 0.1333 \\
                       &=& 0.048
\end{eqnarray*}
El intervalo de confianza es entonces:
\begin{eqnarray*}
0.6 \pm 1.96 \sqrt{0.048} = 0.6 \pm 1.96 \times 0.219 = 0.6 \pm 0.429
\end{eqnarray*}
Por lo tanto, el intervalo de confianza para $\hat{S}(3)$ es aproximadamente $(0.171, 1.029)$. Dado que una probabilidad no puede exceder 1, ajustamos el intervalo a $(0.171, 1.0)$.

\section{Interpretaci\'on del Estimador de Kaplan-Meier}
El estimador de Kaplan-Meier proporciona una estimaci\'on emp\'irica de la funci\'on de supervivencia que es f\'acil de interpretar y calcular. Su capacidad para manejar datos censurados lo hace especialmente \'util en estudios de supervivencia.

\section{Conclusi\'on}
El estimador de Kaplan-Meier es una herramienta poderosa para estimar la funci\'on de supervivencia en presencia de datos censurados. Su c\'alculo es relativamente sencillo y proporciona una estimaci\'on no param\'etrica robusta de la supervivencia a lo largo del tiempo. La interpretaci\'on adecuada de este estimador y su intervalo de confianza asociado es fundamental para el an\'alisis de datos de supervivencia.



\chapter{Comparación de Curvas de Supervivencia}

\section{Introducci\'on}
Comparar curvas de supervivencia es crucial para determinar si existen diferencias significativas en las tasas de supervivencia entre diferentes grupos. Las pruebas de hip\'otesis, como el test de log-rank, son herramientas comunes para esta comparaci\'on.

\section{Test de Log-rank}
El test de log-rank se utiliza para comparar las curvas de supervivencia de dos o más grupos. La hip\'otesis nula es que no hay diferencia en las funciones de riesgo entre los grupos.

\subsection{F\'ormula del Test de Log-rank}
El estad\'istico del test de log-rank se define como:
\begin{eqnarray*}
\chi^2 = \frac{\left(\sum_{i=1}^k (O_i - E_i)\right)^2}{\sum_{i=1}^k V_i}
\end{eqnarray*}
donde:
\begin{itemize}
    \item $O_i$ es el n\'umero observado de eventos en el grupo $i$.
    \item $E_i$ es el n\'umero esperado de eventos en el grupo $i$.
    \item $V_i$ es la varianza del n\'umero de eventos en el grupo $i$.
\end{itemize}

\subsection{Cálculo de $E_i$ y $V_i$}
El n\'umero esperado de eventos $E_i$ y la varianza $V_i$ se calculan como:
\begin{eqnarray*}
E_i &=& \frac{d_i \cdot n_i}{n} \\
V_i &=& \frac{d_i \cdot (n - d_i) \cdot n_i \cdot (n - n_i)}{n^2 \cdot (n - 1)}
\end{eqnarray*}
donde:
\begin{itemize}
    \item $d_i$ es el n\'umero total de eventos en el grupo $i$.
    \item $n_i$ es el n\'umero de individuos en riesgo en el grupo $i$.
    \item $n$ es el n\'umero total de individuos en todos los grupos.
\end{itemize}

\section{Ejemplo de C\'alculo del Test de Log-rank}
Supongamos que tenemos dos grupos con los siguientes datos de eventos:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Grupo & Tiempo ($t_i$) & Eventos ($O_i$) & En Riesgo ($n_i$) \\
\hline
1 & 2 & 1 & 5 \\
1 & 4 & 1 & 4 \\
2 & 3 & 1 & 4 \\
2 & 5 & 1 & 3 \\
\hline
\end{tabular}
\caption{Ejemplo de datos para el test de log-rank}
\end{table}

Calculemos $E_i$ y $V_i$ para cada grupo:

\begin{eqnarray*}
E_1 &=& \frac{2 \cdot 5}{9} + \frac{2 \cdot 4}{8} = \frac{10}{9} + \frac{8}{8} = 1.11 + 1 = 2.11 \\
V_1 &=& \frac{2 \cdot 7 \cdot 5 \cdot 4}{81 \cdot 8} = \frac{2 \cdot 7 \cdot 5 \cdot 4}{648} = \frac{280}{648} = 0.432 \\
E_2 &=& \frac{2 \cdot 4}{9} + \frac{2 \cdot 3}{8} = \frac{8}{9} + \frac{6}{8} = 0.89 + 0.75 = 1.64 \\
V_2 &=& \frac{2 \cdot 7 \cdot 4 \cdot 4}{81 \cdot 8} = \frac{2 \cdot 7 \cdot 4 \cdot 4}{648} = \frac{224}{648} = 0.346 \\
\end{eqnarray*}

El estad\'istico de log-rank se calcula como:
\begin{eqnarray*}
\chi^2 &=& \frac{\left((1 - 2.11) + (1 - 1.64)\right)^2}{0.432 + 0.346} \\
       &=& \frac{\left(-1.11 - 0.64\right)^2}{0.778} \\
       &=& \frac{3.04}{0.778} \\
       &=& 3.91
\end{eqnarray*}

El valor p se puede obtener comparando $\chi^2$ con una distribuci\'on $\chi^2$ con un grado de libertad (dado que estamos comparando dos grupos).

\section{Interpretaci\'on del Test de Log-rank}
Un valor p peque\~no (generalmente menos de 0.05) indica que hay una diferencia significativa en las curvas de supervivencia entre los grupos. Un valor p grande sugiere que no hay suficiente evidencia para rechazar la hip\'otesis nula de que las curvas de supervivencia son iguales.

\section{Pruebas Alternativas}
Adem\'as del test de log-rank, existen otras pruebas para comparar curvas de supervivencia, como el test de Wilcoxon (Breslow), que da m\'as peso a los eventos en tiempos tempranos.

\section{Conclusi\'on}
El test de log-rank es una herramienta esencial para comparar curvas de supervivencia entre diferentes grupos. Su c\'alculo se basa en la diferencia entre los eventos observados y esperados en cada grupo, y su interpretaci\'on puede ayudar a identificar diferencias significativas en la supervivencia.



\chapter{Modelos de Riesgos Proporcionales de Cox}

\section{Introducci\'on}
El modelo de riesgos proporcionales de Cox, propuesto por David Cox en 1972, es una de las herramientas más utilizadas en el análisis de supervivencia. Este modelo permite evaluar el efecto de varias covariables en el tiempo hasta el evento, sin asumir una forma espec\'ifica para la distribuci\'on de los tiempos de supervivencia.

\section{Definici\'on del Modelo de Cox}
El modelo de Cox se define como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta^T X)
\end{eqnarray*}
donde:
\begin{itemize}
    \item $\lambda(t \mid X)$ es la funci\'on de riesgo en el tiempo $t$ dado el vector de covariables $X$.
    \item $\lambda_0(t)$ es la funci\'on de riesgo basal en el tiempo $t$.
    \item $\beta$ es el vector de coeficientes del modelo.
    \item $X$ es el vector de covariables.
\end{itemize}

\section{Supuesto de Proporcionalidad de Riesgos}
El modelo de Cox asume que las razones de riesgo entre dos individuos son constantes a lo largo del tiempo. Matemáticamente, si $X_i$ y $X_j$ son las covariables de dos individuos, la raz\'on de riesgos se expresa como:
\begin{eqnarray*}
\frac{\lambda(t \mid X_i)}{\lambda(t \mid X_j)} = \frac{\lambda_0(t) \exp(\beta^T X_i)}{\lambda_0(t) \exp(\beta^T X_j)} = \exp(\beta^T (X_i - X_j))
\end{eqnarray*}

\section{Estimaci\'on de los Par\'ametros}
Los par\'ametros $\beta$ se estiman utilizando el m\'etodo de m\'axima verosimilitud parcial. La funci\'on de verosimilitud parcial se define como:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^k \frac{\exp(\beta^T X_i)}{\sum_{j \in R(t_i)} \exp(\beta^T X_j)}
\end{eqnarray*}
donde $R(t_i)$ es el conjunto de individuos en riesgo en el tiempo $t_i$.

\subsection{Funci\'on de Log-Verosimilitud Parcial}
La funci\'on de log-verosimilitud parcial es:
\begin{eqnarray*}
\log L(\beta) = \sum_{i=1}^k \left(\beta^T X_i - \log \sum_{j \in R(t_i)} \exp(\beta^T X_j)\right)
\end{eqnarray*}

\subsection{Derivadas Parciales y Maximizaci\'on}
Para encontrar los estimadores de m\'axima verosimilitud, resolvemos el sistema de ecuaciones obtenido al igualar a cero las derivadas parciales de $\log L(\beta)$ con respecto a $\beta$:
\begin{eqnarray*}
\frac{\partial \log L(\beta)}{\partial \beta} = \sum_{i=1}^k \left(X_i - \frac{\sum_{j \in R(t_i)} X_j \exp(\beta^T X_j)}{\sum_{j \in R(t_i)} \exp(\beta^T X_j)}\right) = 0
\end{eqnarray*}

\section{Interpretaci\'on de los Coeficientes}
Cada coeficiente $\beta_i$ representa el logaritmo de la raz\'on de riesgos asociado con un incremento unitario en la covariable $X_i$. Un valor positivo de $\beta_i$ indica que un aumento en $X_i$ incrementa el riesgo del evento, mientras que un valor negativo indica una reducci\'on del riesgo.

\section{Evaluaci\'on del Modelo}
El modelo de Cox se eval\'ua utilizando varias t\'ecnicas, como el an\'alisis de residuos de Schoenfeld para verificar el supuesto de proporcionalidad de riesgos, y el uso de curvas de supervivencia estimadas para evaluar la bondad de ajuste.

\subsection{Residuos de Schoenfeld}
Los residuos de Schoenfeld se utilizan para evaluar la proporcionalidad de riesgos. Para cada evento en el tiempo $t_i$, el residuo de Schoenfeld para la covariable $X_j$ se define como:
\begin{eqnarray*}
r_{ij} = X_{ij} - \hat{X}_{ij}
\end{eqnarray*}
donde $\hat{X}_{ij}$ es la covariable ajustada.

\subsection{Curvas de Supervivencia Ajustadas}
Las curvas de supervivencia ajustadas se obtienen utilizando la funci\'on de riesgo basal estimada y los coeficientes del modelo. La funci\'on de supervivencia ajustada se define como:
\begin{eqnarray*}
\hat{S}(t \mid X) = \hat{S}_0(t)^{\exp(\beta^T X)}
\end{eqnarray*}
donde $\hat{S}_0(t)$ es la funci\'on de supervivencia basal estimada.

\section{Ejemplo de Aplicaci\'on del Modelo de Cox}
Consideremos un ejemplo con tres covariables: edad, sexo y tratamiento. Supongamos que los datos se ajustan a un modelo de Cox y obtenemos los siguientes coeficientes:
\begin{eqnarray*}
\hat{\beta}_{edad} = 0.02, \quad \hat{\beta}_{sexo} = -0.5, \quad \hat{\beta}_{tratamiento} = 1.2
\end{eqnarray*}

La funci\'on de riesgo ajustada se expresa como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(0.02 \cdot \text{edad} - 0.5 \cdot \text{sexo} + 1.2 \cdot \text{tratamiento})
\end{eqnarray*}

\section{Conclusi\'on}
El modelo de riesgos proporcionales de Cox es una herramienta poderosa para analizar datos de supervivencia con m\'ultiples covariables. Su flexibilidad y la falta de suposiciones fuertes sobre la distribuci\'on de los tiempos de supervivencia lo hacen ampliamente aplicable en diversas disciplinas.



\chapter{Diagnóstico y Validación de Modelos de Cox}

\section{Introducci\'on}
Una vez ajustado un modelo de Cox, es crucial realizar diagn\'osticos y validaciones para asegurar que el modelo es apropiado y que los supuestos subyacentes son válidos. Esto incluye la verificaci\'on del supuesto de proporcionalidad de riesgos y la evaluaci\'on del ajuste del modelo.

\section{Supuesto de Proporcionalidad de Riesgos}
El supuesto de proporcionalidad de riesgos implica que la raz\'on de riesgos entre dos individuos es constante a lo largo del tiempo. Si este supuesto no se cumple, las inferencias hechas a partir del modelo pueden ser incorrectas.

\subsection{Residuos de Schoenfeld}
Los residuos de Schoenfeld se utilizan para evaluar la proporcionalidad de riesgos. Para cada evento en el tiempo $t_i$, el residuo de Schoenfeld para la covariable $X_j$ se define como:
\begin{eqnarray*}
r_{ij} = X_{ij} - \hat{X}_{ij}
\end{eqnarray*}
donde $\hat{X}_{ij}$ es la covariable ajustada. Si los residuos de Schoenfeld no muestran una tendencia sistemática cuando se trazan contra el tiempo, el supuesto de proporcionalidad de riesgos es razonable.

\section{Bondad de Ajuste}
La bondad de ajuste del modelo de Cox se eval\'ua comparando las curvas de supervivencia observadas y ajustadas, y utilizando estad\'isticas de ajuste global.

\subsection{Curvas de Supervivencia Ajustadas}
Las curvas de supervivencia aaustadas se obtienen utilizando la funci\'on de riesgo basal estimada y los coeficientes del modelo. La funci\'on de supervivencia ajustada se define como:
\begin{eqnarray*}
\hat{S}(t \mid X) = \hat{S}_0(t)^{\exp(\beta^T X)}
\end{eqnarray*}
donde $\hat{S}_0(t)$ es la funci\'on de supervivencia basal estimada. Comparar estas curvas con las curvas de Kaplan-Meier para diferentes niveles de las covariables puede proporcionar una validaci\'on visual del ajuste del modelo.

\subsection{Estad\'isticas de Ajuste Global}
Las estad\'isticas de ajuste global, como el test de la desviaci\'on y el test de la bondad de ajuste de Grambsch y Therneau, se utilizan para evaluar el ajuste global del modelo de Cox.

\section{Diagn\'ostico de Influencia}
El diagn\'ostico de influencia identifica observaciones individuales que tienen un gran impacto en los estimados del modelo. Los residuos de devianza y los residuos de martingala se utilizan com\'unmente para este prop\'osito.

\subsection{Residuos de Deviance}
Los residuos de deviance se definen como:
\begin{eqnarray*}
D_i = \text{sign}(O_i - E_i) \sqrt{-2 \left(O_i \log \frac{O_i}{E_i} - (O_i - E_i)\right)}
\end{eqnarray*}
donde $O_i$ es el n\'umero observado de eventos y $E_i$ es el n\'umero esperado de eventos. Observaciones con residuos de deviance grandes en valor absoluto pueden ser influyentes.

\subsection{Residuos de Martingala}
Los residuos de martingala se definen como:
\begin{eqnarray*}
M_i = O_i - E_i
\end{eqnarray*}
donde $O_i$ es el n\'umero observado de eventos y $E_i$ es el n\'umero esperado de eventos. Los residuos de martingala se utilizan para detectar observaciones que no se ajustan bien al modelo.

\section{Ejemplo de Diagn\'ostico}
Consideremos un modelo de Cox ajustado con las covariables edad, sexo y tratamiento. Para diagnosticar la influencia de observaciones individuales, calculamos los residuos de deviance y martingala para cada observaci\'on.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Observaci\'on & Edad & Sexo & Tratamiento & Residuo de Deviance \\
\hline
1 & 50 & 0 & 1 & 1.2 \\
2 & 60 & 1 & 0 & -0.5 \\
3 & 45 & 0 & 1 & -1.8 \\
4 & 70 & 1 & 0 & 0.3 \\
\hline
\end{tabular}
\caption{Residuos de deviance para observaciones individuales}
\end{table}

Observaciones con residuos de deviance grandes en valor absoluto (como la observaci\'on 3) pueden ser influyentes y requieren una revisi\'on adicional.

\section{Conclusi\'on}
El diagn\'ostico y la validaci\'on son pasos cr\'iticos en el an\'slisis de modelos de Cox. Evaluar el supuesto de proporcionalidad de riesgos, la bondad de ajuste y la influencia de observaciones individuales asegura que las inferencias y conclusiones derivadas del modelo sean v\'slidas y fiables.



\chapter{Modelos Acelerados de Fallos}
\section{Introducci\'on}
Los modelos acelerados de fallos (AFT) son una alternativa a los modelos de riesgos proporcionales de Cox. En lugar de asumir que las covariables afectan la tasa de riesgo, los modelos AFT asumen que las covariables multiplican el tiempo de supervivencia por una constante.

\section{Definici\'on del Modelo AFT}
Un modelo AFT se expresa como:
\begin{eqnarray*}
T = T_0 \exp(\beta^T X)
\end{eqnarray*}
donde:
\begin{itemize}
    \item $T$ es el tiempo de supervivencia observado.
    \item $T_0$ es el tiempo de supervivencia bajo condiciones basales.
    \item $\beta$ es el vector de coeficientes del modelo.
    \item $X$ es el vector de covariables.
\end{itemize}

\subsection{Transformaci\'on Logar\'itmica}
El modelo AFT se puede transformar logar\'itmicamente para obtener una forma lineal:
\begin{eqnarray*}
\log(T) = \log(T_0) + \beta^T X
\end{eqnarray*}

\section{Estimaci\'on de los Parámetros}
Los parámetros del modelo AFT se estiman utilizando el m\'etodo de máxima verosimilitud. La funci\'on de verosimilitud se define como:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^n f(t_i \mid X_i; \beta)
\end{eqnarray*}
donde $f(t_i \mid X_i; \beta)$ es la funci\'on de densidad de probabilidad del tiempo de supervivencia $t_i$ dado el vector de covariables $X_i$ y los par\'ametros $\beta$.

\subsection{Funci\'on de Log-Verosimilitud}
La funci\'on de log-verosimilitud es:
\begin{eqnarray*}
\log L(\beta) = \sum_{i=1}^n \log f(t_i \mid X_i; \beta)
\end{eqnarray*}

\subsection{Maximizaci\'on de la Verosimilitud}
Los estimadores de m\'axima verosimilitud se obtienen resolviendo el sistema de ecuaciones obtenido al igualar a cero las derivadas parciales de $\log L(\beta)$ con respecto a $\beta$:
\begin{eqnarray*}
\frac{\partial \log L(\beta)}{\partial \beta} = 0
\end{eqnarray*}

\section{Distribuciones Comunes en Modelos AFT}
En los modelos AFT, el tiempo de supervivencia $T$ puede seguir varias distribuciones comunes, como la exponencial, Weibull, log-normal y log-log\'istica. Cada una de estas distribuciones tiene diferentes propiedades y aplicaciones.

\subsection{Modelo Exponencial AFT}
En un modelo exponencial AFT, el tiempo de supervivencia $T$ sigue una distribuci\'on exponencial con par\'ametro $\lambda$:
\begin{eqnarray*}
f(t) = \lambda \exp(-\lambda t)
\end{eqnarray*}
La funci\'on de supervivencia es:
\begin{eqnarray*}
S(t) = \exp(-\lambda t)
\end{eqnarray*}
La transformaci\'on logar\'itmica del tiempo de supervivencia es:
\begin{eqnarray*}
\log(T) = \log\left(\frac{1}{\lambda}\right) + \beta^T X
\end{eqnarray*}

\subsection{Modelo Weibull AFT}
En un modelo Weibull AFT, el tiempo de supervivencia $T$ sigue una distribuci\'on Weibull con par\'ametros $\lambda$ y $k$:
\begin{eqnarray*}
f(t) = \lambda k t^{k-1} \exp(-\lambda t^k)
\end{eqnarray*}
La funci\'on de supervivencia es:
\begin{eqnarray*}
S(t) = \exp(-\lambda t^k)
\end{eqnarray*}
La transformaci\'on logar\'itmica del tiempo de supervivencia es:
\begin{eqnarray*}
\log(T) = \log\left(\left(\frac{1}{\lambda}\right)^{1/k}\right) + \frac{\beta^T X}{k}
\end{eqnarray*}

\section{Interpretaci\'on de los Coeficientes}
En los modelos AFT, los coeficientes $\beta_i$ se interpretan como factores multiplicativos del tiempo de supervivencia. Un valor positivo de $\beta_i$ indica que un aumento en la covariable $X_i$ incrementa el tiempo de supervivencia, mientras que un valor negativo indica una reducci\'on del tiempo de supervivencia.

\section{Ejemplo de Aplicaci\'on del Modelo AFT}
Consideremos un ejemplo con tres covariables: edad, sexo y tratamiento. Supongamos que los datos se ajustan a un modelo Weibull AFT y obtenemos los siguientes coeficientes:
\begin{eqnarray*}
\hat{\beta}_{edad} = -0.02, \quad \hat{\beta}_{sexo} = 0.5, \quad \hat{\beta}_{tratamiento} = -1.2
\end{eqnarray*}

La funci\'on de supervivencia ajustada se expresa como:
\begin{eqnarray*}
S(t \mid X) = \exp\left(-\left(\frac{t \exp(-0.02 \cdot \text{edad} + 0.5 \cdot \text{sexo} - 1.2 \cdot \text{tratamiento})}{\lambda}\right)^k\right)
\end{eqnarray*}

\section{Conclusi\'on}
Los modelos AFT proporcionan una alternativa flexible a los modelos de riesgos proporcionales de Cox. Su enfoque en la multiplicaci\'on del tiempo de supervivencia por una constante permite una interpretaci\'on intuitiva y aplicaciones en diversas \'areas.



\chapter{Análisis Multivariado de Supervivencia}

\section{Introducci\'on}
El análisis multivariado de supervivencia extiende los modelos de supervivencia para incluir m\'ultiples covariables, permitiendo evaluar su efecto simultáneo sobre el tiempo hasta el evento. Los modelos de Cox y AFT son com\'unmente utilizados en este contexto.

\section{Modelo de Cox Multivariado}
El modelo de Cox multivariado se define como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta^T X)
\end{eqnarray*}
donde $X$ es un vector de covariables.

\subsection{Estimaci\'on de los Parámetros}
Los parámetros $\beta$ se estiman utilizando el m\'etodo de máxima verosimilitud parcial, como se discuti\'o anteriormente. La funci\'on de verosimilitud parcial se maximiza para obtener los estimadores de los coeficientes.

\section{Modelo AFT Multivariado}
El modelo AFT multivariado se expresa como:
\begin{eqnarray*}
T = T_0 \exp(\beta^T X)
\end{eqnarray*}

\subsection{Estimaci\'on de los Par\'ametros}
Los par\'ametros $\beta$ se estiman utilizando el m\'etodo de m\'axima verosimilitud, similar al caso univariado. La funci\'on de verosimilitud se maximiza para obtener los estimadores de los coeficientes.

\section{Interacci\'on y Efectos No Lineales}
En el an\'alisis multivariado, es importante considerar la posibilidad de interacciones entre covariables y efectos no lineales. Estos se pueden incluir en los modelos extendiendo las funciones de riesgo o supervivencia.

\subsection{Interacciones}
Las interacciones entre covariables se pueden modelar a\~nadiendo t\'erminos de interacci\'on en el modelo:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2)
\end{eqnarray*}
donde $X_1 X_2$ es el t\'ermino de interacci\'on.

\subsection{Efectos No Lineales}
Los efectos no lineales se pueden modelar utilizando funciones no lineales de las covariables, como polinomios o splines:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta_1 X + \beta_2 X^2)
\end{eqnarray*}

\section{Selecci\'on de Variables}
La selecci\'on de variables es crucial en el an\'alisis multivariado para evitar el sobreajuste y mejorar la interpretabilidad del modelo. M\'etodos como la regresi\'on hacia atr\'as, la regresi\'on hacia adelante y la selecci\'on por criterios de informaci\'on (AIC, BIC) son com\'unmente utilizados.

\subsection{Regresi\'on Hacia Atr\'as}
La regresi\'on hacia atr\'as comienza con todas las covariables en el modelo y elimina iterativamente la covariable menos significativa hasta que todas las covariables restantes sean significativas.

\subsection{Regresi\'on Hacia Adelante}
La regresi\'on hacia adelante comienza con un modelo vac\'io y a\~nade iterativamente la covariable m\'as significativa hasta que no se pueda a\~nadir ninguna covariable adicional significativa.

\subsection{Criterios de Informaci\'on}
Los criterios de informaci\'on, como el AIC (Akaike Information Criterion) y el BIC (Bayesian Information Criterion), se utilizan para seleccionar el modelo que mejor se ajusta a los datos con la menor complejidad posible:
\begin{eqnarray*}
AIC &=& -2 \log L + 2k \\
BIC &=& -2 \log L + k \log n
\end{eqnarray*}
donde $L$ es la funci\'on de verosimilitud del modelo, $k$ es el n\'umero de par\'ametros en el modelo y $n$ es el tama\~no de la muestra.

\section{Ejemplo de An\'alisis Multivariado}
Consideremos un ejemplo con tres covariables: edad, sexo y tratamiento. Ajustamos un modelo de Cox multivariado y obtenemos los siguientes coeficientes:
\begin{eqnarray*}
\hat{\beta}_{edad} = 0.03, \quad \hat{\beta}_{sexo} = -0.6, \quad \hat{\beta}_{tratamiento} = 1.5
\end{eqnarray*}

La funci\'on de riesgo ajustada se expresa como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(0.03 \cdot \text{edad} - 0.6 \cdot \text{sexo} + 1.5 \cdot \text{tratamiento})
\end{eqnarray*}

\section{Conclusi\'on}
El an\'alisis multivariado de supervivencia permite evaluar el efecto conjunto de m\'ultiples covariables sobre el tiempo hasta el evento. La inclusi\'on de interacciones y efectos no lineales, junto con la selecci\'on adecuada de variables, mejora la precisi\'on y la interpretabilidad de los modelos de supervivencia.



\chapter{Supervivencia en Datos Complicados}

\section{Introducci\'on}
El análisis de supervivencia en datos complicados se refiere a la evaluaci\'on de datos de supervivencia que presentan desaf\'ios adicionales, como la censura por intervalo, datos truncados y datos con m\'ultiples tipos de eventos. Estos escenarios requieren m\'etodos avanzados para un análisis adecuado.

\section{Censura por Intervalo}
La censura por intervalo ocurre cuando el evento de inter\'es se sabe que ocurri\'o dentro de un intervalo de tiempo, pero no se conoce el momento exacto. Esto es com\'un en estudios donde las observaciones se realizan en puntos de tiempo discretos.

\subsection{Modelo para Datos Censurados por Intervalo}
Para datos censurados por intervalo, la funci\'on de verosimilitud se modifica para incluir la probabilidad de que el evento ocurra dentro de un intervalo:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^n P(T_i \in [L_i, U_i] \mid X_i; \beta)
\end{eqnarray*}
donde $[L_i, U_i]$ es el intervalo de tiempo durante el cual se sabe que ocurri\'o el evento para el individuo $i$.

\section{Datos Truncados}
Los datos truncados ocurren cuando los tiempos de supervivencia est\'an sujetos a un umbral, y solo se observan los individuos cuyos tiempos de supervivencia superan (o est\'an por debajo de) ese umbral. Existen dos tipos principales de truncamiento: truncamiento a la izquierda y truncamiento a la derecha.

\subsection{Modelo para Datos Truncados}
Para datos truncados a la izquierda, la funci\'on de verosimilitud se ajusta para considerar solo los individuos que superan el umbral de truncamiento:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^n \frac{f(t_i \mid X_i; \beta)}{1 - F(L_i \mid X_i; \beta)}
\end{eqnarray*}
donde $L_i$ es el umbral de truncamiento para el individuo $i$.

\section{An\'alisis de Competing Risks}
En estudios donde pueden ocurrir m\'ultiples tipos de eventos (competing risks), es crucial modelar adecuadamente el riesgo asociado con cada tipo de evento. La probabilidad de ocurrencia de cada evento compite con las probabilidades de ocurrencia de otros eventos.

\subsection{Modelo de Competing Risks}
Para un an\'alisis de competing risks, la funci\'on de riesgo se descompone en funciones de riesgo espec\'ificas para cada tipo de evento:
\begin{eqnarray*}
\lambda(t) = \sum_{j=1}^m \lambda_j(t)
\end{eqnarray*}
donde $\lambda_j(t)$ es la funci\'on de riesgo para el evento $j$.

\section{M\'etodos de Imputaci\'on}
Los m\'etodos de imputaci\'on se utilizan para manejar datos faltantes o censurados en estudios de supervivencia. La imputaci\'on m\'ultiple es un enfoque com\'un que crea m\'ultiples conjuntos de datos completos imputando valores faltantes varias veces y luego combina los resultados.

\subsection{Imputaci\'on M\'ultiple}
La imputaci\'on m\'ultiple para datos de supervivencia se realiza en tres pasos:
\begin{enumerate}
    \item Imputar los valores faltantes m\'ultiples veces para crear varios conjuntos de datos completos.
    \item Analizar cada conjunto de datos completo por separado utilizando m\'etodos de supervivencia est\'andar.
    \item Combinar los resultados de los an\'alisis separados para obtener estimaciones y varianzas combinadas.
\end{enumerate}

\section{Ejemplo de An\'alisis con Datos Complicados}
Consideremos un estudio con datos censurados por intervalo y competing risks. Ajustamos un modelo para los datos censurados por intervalo y obtenemos los siguientes coeficientes para las covariables edad y tratamiento:
\begin{eqnarray*}
\hat{\beta}_{edad} = 0.04, \quad \hat{\beta}_{tratamiento} = -0.8
\end{eqnarray*}

La funci\'on de supervivencia ajustada se expresa como:
\begin{eqnarray*}
S(t \mid X) = \exp\left(-\left(\frac{t \exp(0.04 \cdot \text{edad} - 0.8 \cdot \text{tratamiento})}{\lambda}\right)^k\right)
\end{eqnarray*}

\section{Conclusi\'on}
El an\'alisis de supervivencia en datos complicados requiere m\'etodos avanzados para manejar censura por intervalo, datos truncados y competing risks. La aplicaci\'on de modelos adecuados y m\'etodos de imputaci\'on asegura un an\'alisis preciso y completo de estos datos complejos.



\chapter{Proyecto Final y Revisión}

\section{Introducci\'on}
El proyecto final proporciona una oportunidad para aplicar los conceptos y t\'ecnicas aprendidas en el curso de análisis de supervivencia. Este cap\'itulo incluye una gu\'ia para desarrollar un proyecto de análisis de supervivencia y una revisi\'on de los conceptos clave.

\section{Desarrollo del Proyecto}
El proyecto final debe incluir los siguientes componentes:
\begin{enumerate}
    \item Definici\'on del problema: Identificar la pregunta de investigaci\'on y los objetivos del análisis de supervivencia.
    \item Descripci\'on de los datos: Presentar los datos utilizados, incluyendo las covariables y la estructura de los datos.
    \item Análisis exploratorio: Realizar un análisis descriptivo de los datos, incluyendo la censura y la distribuci\'on de los tiempos de supervivencia.
    \item Ajuste del modelo: Ajustar modelos de supervivencia adecuados (Kaplan-Meier, Cox, AFT) y evaluar su bondad de ajuste.
    \item Diagn\'ostico del modelo: Realizar diagn\'osticos para evaluar los supuestos del modelo y la influencia de observaciones individuales.
    \item Interpretaci\'on de resultados: Interpretar los coeficientes del modelo y las curvas de supervivencia ajustadas.
    \item Conclusiones: Resumir los hallazgos del análisis y proporcionar recomendaciones basadas en los resultados.
\end{enumerate}

\section{Revisi\'on de Conceptos Clave}
Una revisi\'on de los conceptos clave del an\'alisis de supervivencia incluye:
\begin{itemize}
    \item \textbf{Funci\'on de Supervivencia:} Define la probabilidad de sobrevivir m\'as all\'a de un tiempo espec\'ifico.
    \item \textbf{Funci\'on de Riesgo:} Define la tasa instant\'anea de ocurrencia del evento.
    \item \textbf{Estimador de Kaplan-Meier:} Proporciona una estimaci\'on no param\'etrica de la funci\'on de supervivencia.
    \item \textbf{Test de Log-rank:} Compara curvas de supervivencia entre diferentes grupos.
    \item \textbf{Modelo de Cox:} Eval\'ua el efecto de m\'ultiples covariables sobre el tiempo hasta el evento, asumiendo proporcionalidad de riesgos.
    \item \textbf{Modelos AFT:} Modelan el efecto de las covariables multiplicando el tiempo de supervivencia por una constante.
    \item \textbf{An\'alisis Multivariado:} Considera interacciones y efectos no lineales entre m\'ultiples covariables.
    \item \textbf{Supervivencia en Datos Complicados:} Maneja censura por intervalo, datos truncados y competing risks.
\end{itemize}

\section{Ejemplo de Proyecto Final}
A continuaci\'on se presenta un ejemplo de estructura de un proyecto final de an\'alisis de supervivencia:

\subsection{Definici\'on del Problema}
Analizar el efecto del tratamiento y la edad sobre la supervivencia de pacientes con una enfermedad espec\'ifica.

\subsection{Descripci\'on de los Datos}
Datos de supervivencia de 100 pacientes, con covariables: edad, sexo y tipo de tratamiento. Los tiempos de supervivencia est\'an censurados a la derecha.

\subsection{An\'alisis Exploratorio}
Realizar histogramas y curvas de Kaplan-Meier para explorar la distribuci\'on de los tiempos de supervivencia y la censura.

\subsection{Ajuste del Modelo}
Ajustar un modelo de Cox y un modelo AFT con las covariables edad y tratamiento.

\subsection{Diagn\'ostico del Modelo}
Evaluar la proporcionalidad de riesgos y realizar an\'alisis de residuos para identificar observaciones influyentes.

\subsection{Interpretaci\'on de Resultados}
Interpretar los coeficientes del modelo y las curvas de supervivencia ajustadas para diferentes niveles de las covariables.

\subsection{Conclusiones}
Resumir los hallazgos y proporcionar recomendaciones sobre el efecto del tratamiento y la edad en la supervivencia de los pacientes.

\section{Conclusi\'on}
El proyecto final es una oportunidad para aplicar los conocimientos adquiridos en un contexto pr\'actico. La revisi\'on de los conceptos clave y la aplicaci\'on de t\'ecnicas adecuadas de an\'alisis de supervivencia aseguran un an\'alisis riguroso y significativo.



\chapter{Documentos Adicionales}



\subsection{Parte I. Introducci\'on a la Bioestad\'istica}

\section{Introducci\'on}
%\begin{frame}\frametitle{Introducci\'on}
%____________________________________________________________
\subsection{Definici\'on de Estad\'istica}
%____________________________________________________________
\begin{itemize}
\item La Estad\'istica es una ciencia formal que estudia la recolecci\'on, an\'alisis e interpretaci\'on de datos de una muestra representativa, ya sea para ayudar en la toma de decisiones o para explicar condiciones regulares o irregulares de alg\'un fen\'omeno o estudio aplicado, de ocurrencia en forma aleatoria o condicional. 

\item Sin embargo, la estad\'istica es m\'as que eso, es decir, es el veh\'iculo que permite llevar a cabo el proceso relacionado con la investigaci\'on cient\'ifica. 

\item Es transversal a una amplia variedad de disciplinas, desde la f\'isica hasta las ciencias sociales, desde las ciencias de la salud hasta el control de calidad. Se usa para la toma de decisiones en \'areas de negocios o instituciones gubernamentales.

\end{itemize}




%\end{frame}
%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}
\item Como dijera Huntsberger: \textit{La palabra estad\'istica a menudo nos trae a la mente im\'agenes de n\'umeros apilados en grandes arreglos y tablas, de vol\'umenes de cifras relativas a nacimientos, muertes, impuestos, poblaciones, ingresos, deudas, cr\'editos y as\'i sucesivamente}.  Huntsberger tiene raz\'on pues al instante de escuchar esta palabra estas son las im\'agenes que llegan a nuestra cabeza.

\item La Estad\'istica es mucho m\'as que s\'olo n\'umeros apilados y gr\'aficas bonitas.  

\item Es una ciencia con tanta antiguedad como la escritura, y es por s\'i misma auxiliar de todas las dem\'as ciencias. 

\item Los mercados, la medicina, la ingenier\'ia, los gobiernos, etc. Se nombran entre los m\'as destacados clientes de \'esta. 


\end{itemize}



%\end{frame}
%\begin{frame}\frametitle{Introducci\'on}

\begin{itemize}
\item La ausencia de \'esta conllevar\'ia a un caos generalizado, dejando a los administradores y ejecutivos sin informaci\'on vital a la hora de tomar decisiones en tiempos de incertidumbre.

\item La Estad\'istica que conocemos hoy en d\'ia debe gran parte de su realizaci\'on a los trabajos matem\'aticos de aquellos hombres que desarrollaron la teor\'ia de las probabilidades, con la cual se adhiri\'o a la Estad\'istica a las ciencias formales. 


\end{itemize}

%\end{frame}

%\begin{frame}\frametitle{Introducci\'on}

\begin{Def}
La Estad\'istica es la ciencia cuyo objetivo es reunir una informaci\'on cuantitativa concerniente a individuos, grupos, series de hechos, etc. y deducir de ello gracias al an\'alisis de estos datos unos significados precisos o unas previsiones para el futuro.
\end{Def}

\begin{itemize}
\item La estad\'istica, en general, es la ciencia que trata de la recopilaci\'on, organizaci\'on presentaci\'on, an\'alisis e interpretaci\'on de datos num\'ericos con el fin de realizar una toma de decisi\'on m\'as efectiva.

\end{itemize}

%\end{frame}


%\begin{frame}

\begin{itemize}
\item Otros autores la definen como la expresi\'on cuantitativa del conocimiento dispuesta en forma adecuada para el escrutinio y an\'alisis.

\item Los estudiantes confunden com\'unmente los dem\'as t\'erminos asociados con las Estad\'isticas, una confusi\'on que es conveniente aclarar debido a que esta palabra tiene tres significados: 

\begin{itemize}
\item la palabra estad\'istica, en primer t\'ermino se usa para referirse a la informaci\'on estad\'istica; 
\item tambi\'en se utiliza para referirse al conjunto de t\'ecnicas y m\'etodos que se utilizan para analizar la informaci\'on estad\'istica; y
\item  el t\'ermino estad\'istico, en singular y en masculino, se refiere a una medida derivada de una muestra.
\end{itemize}
\end{itemize}
%\end{frame}


%____________________________________________________________
\subsection{Utilidad e Importancia}
%____________________________________________________________


%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}
\item Los m\'etodos estad\'isticos tradicionalmente se utilizan para prop\'ositos descriptivos, para organizar y resumir datos num\'ericos. La estad\'istica descriptiva, por ejemplo trata de la tabulaci\'on de datos, su presentaci\'on en forma gr\'afica o ilustrativa y el c\'alculo de medidas descriptivas.

\item Ahora bien, las t\'ecnicas estad\'isticas se aplican de manera amplia en mercadotecnia, contabilidad, control de calidad y en otras actividades; estudios de consumidores; an\'alisis de resultados en deportes; administradores de instituciones; en la educaci\'on; organismos pol\'iticos; m\'edicos; y por otras personas que intervienen en la toma de decisiones.

\end{itemize}

%\end{frame}

%____________________________________________________________
\subsection{Historia de la Estad\'istica}
%____________________________________________________________

%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item Es dif\'icil conocer los or\'igenes de la Estad\'istica. Desde los comienzos de la civilizaci\'on han existido formas sencillas de estad\'istica, pues ya se utilizaban representaciones gr\'aficas y otros s\'imbolos en pieles, rocas, palos de madera y paredes de cuevas para contar el n\'umero de personas, animales o ciertas cosas. 
\item Su origen empieza posiblemente en la isla de Cerde\~na, donde existen monumentos prehist\'oricos pertenecientes a los Nuragas, las primeros habitantes de la isla; estos monumentos constan de bloques de basalto superpuestos sin mortero y en cuyas paredes de encontraban grabados toscos signos que han sido interpretados con mucha verosimilidad como muescas que serv\'ian para llevar la cuenta del ganado y la caza.

\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item Los babilonios usaban ya peque\~nas tablillas de arcilla para recopilar datos en tablas sobre la producci\'on agr\'icola y los g\'eneros vendidos o cambiados mediante trueque. 
\item Otros vestigios pueden ser hallados en el antiguo Egipto, cuyos faraones lograron recopilar, hacia el a\~no 3050 antes de Cristo, prolijos datos relativos a la poblaci\'on y la riqueza del pa\'is. 
\item De acuerdo al historiador griego Her\'odoto, dicho registro de riqueza y poblaci\'on se hizo con el objetivo de preparar la construcci\'on de las pir\'amides. 



\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item En el mismo Egipto, Rams\'es II hizo un censo de las tierras con el objeto de verificar un nuevo reparto. 
\item En el antiguo Israel la Biblia da referencias, en el libro de los N\'umeros, de los datos estad\'isticos obtenidos en dos recuentos de la poblaci\'on hebrea. El rey David por otra parte, orden\'o a Joab, general del ej\'ercito hacer un censo de Israel con la finalidad de conocer el n\'umero de la poblaci\'on.

\item Tambi\'en los chinos efectuaron censos hace m\'as de cuarenta siglos. 
\item Los griegos efectuaron censos peri\'odicamente con fines tributarios, sociales (divisi\'on de tierras) y militares (c\'alculo de recursos y hombres disponibles). 

\end{itemize}


%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item La investigaci\'on hist\'orica revela que se realizaron 69 censos para calcular los impuestos, determinar los derechos de voto y ponderar la potencia guerrera.
\item Fueron los romanos, maestros de la organizaci\'on pol\'itica, quienes mejor supieron emplear los recursos de la estad\'istica. Cada cinco a\~nos realizaban un censo de la poblaci\'on y sus funcionarios p\'ublicos ten\'ian la obligaci\'on de anotar nacimientos, defunciones y matrimonios, sin olvidar los recuentos peri\'odicos del ganado y de las riquezas contenidas en las tierras conquistadas. 
\item Para el nacimiento de Cristo suced\'ia uno de estos empadronamientos de la poblaci\'on bajo la autoridad del imperio. 

\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item Durante los mil a\~nos siguientes a la ca\'ida del imperio Romano se realizaron muy pocas operaciones Estad\'isticas, con la notable excepci\'on de las relaciones de tierras pertenecientes a la Iglesia, compiladas por Pipino el Breve en el 758 y por Carlomagno en el 762 DC. 
\item Durante el siglo IX se realizaron en Francia algunos censos parciales de siervos. 
\item En Inglaterra, Guillermo el Conquistador recopil\'o el Domesday Book o libro del Gran Catastro para el a\~no 1086, un documento de la propiedad, extensi\'on y valor de las tierras de Inglaterra. Esa obra fue el primer compendio estad\'istico de Inglaterra. 

\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item Aunque Carlomagno, en Francia; y Guillermo el Conquistador, en Inglaterra, trataron de revivir la t\'ecnica romana, los m\'etodos estad\'isticos permanecieron casi olvidados durante la Edad Media.

\item Durante los siglos XV, XVI, y XVII, hombres como Leonardo de Vinci, Nicol\'as Cop\'ernico, Galileo, Neper, William Harvey, Sir Francis Bacon y Ren\'e Descartes, hicieron grandes operaciones al m\'etodo cient\'ifico, de tal forma que cuando se crearon los Estados Nacionales y surgi\'o como fuerza el comercio internacional exist\'ia ya un m\'etodo capaz de aplicarse a los datos econ\'omicos. 

\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item Para el a\~no 1532 empezaron a registrarse en Inglaterra las defunciones debido al temor que Enrique VII ten\'ia por la peste.  M\'as o menos por la misma \'epoca, en Francia la ley exigi\'o a los cl\'erigos registrar los bautismos, fallecimientos y matrimonios. 
\item Durante un brote de peste que apareci\'o a fines de la d\'ecada de 1500, el gobierno ingl\'es comenz\'o a publicar estad\'istica semanales de los decesos. Esa costumbre continu\'o muchos a\~nos, y en 1632 estos Bills of Mortality (Cuentas de Mortalidad) conten\'ian los nacimientos y fallecimientos por sexo.

\end{itemize}

%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}

\item En 1662, el capit\'an John Graunt us\'o documentos que abarcaban treinta a\~nos y efectu\'o predicciones sobre el n\'umero de personas que morir\'ian de varias enfermedades y sobre las proporciones de nacimientos de varones y mujeres que cabr\'ia esperar. 

\item El trabajo de Graunt, condensado en su obra \textit{Natural and Political Observations...Made upon the Bills of Mortality}, fue un esfuerzo innovador en el an\'alisis estad\'istico. Por el a\~no 1540 el alem\'an Sebasti\'an Muster realiz\'o una compilaci\'on estad\'istica de los recursos nacionales, comprensiva de datos sobre organizaci\'on pol\'itica, instrucciones sociales, comercio y poder\'io militar. 
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}

\item Durante el siglo XVII aport\'o indicaciones m\'as concretas de m\'etodos de observaci\'on y an\'alisis cuantitativo y ampli\'o los campos de la inferencia y la teor\'ia Estad\'istica.
\item Los eruditos del siglo XVII demostraron especial inter\'es por la Estad\'istica Demogr\'afica como resultado de la especulaci\'on sobre si la poblaci\'on aumentaba, decrec\'ia o permanec\'ia est\'atica. 
\item En los tiempos modernos tales m\'etodos fueron resucitados por algunos reyes que necesitaban conocer las riquezas monetarias y el potencial humano de sus respectivos pa\'ises. 

\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}


\item El primer empleo de los datos estad\'isticos para fines ajenos a la pol\'itica tuvo lugar en 1691 y estuvo a cargo de Gaspar Neumann, un profesor alem\'an que viv\'ia en Breslau. 
\item Este investigador se propuso destruir la antigua creencia popular de que en los a\~nos terminados en siete mor\'ia m\'as gente que en los restantes, y para lograrlo hurg\'o pacientemente en los archivos parroquiales de la ciudad. 
\item Despu\'es de revisar miles de partidas de defunci\'on pudo demostrar que en tales a\~nos no fallec\'ian m\'as personas que en los dem\'as. 
\item Los procedimientos de Neumann fueron conocidos por el astr\'onomo ingl\'es Halley, descubridor del cometa que lleva su nombre, quien los aplic\'o al estudio de la vida humana. 


\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item  Sus c\'alculos sirvieron de base para las tablas de mortalidad que hoy utilizan todas las compa\~n\'ias de seguros. 
\item Durante el siglo XVII y principios del XVIII, matem\'aticos como Bernoulli, Francis Maseres, Lagrange y Laplace desarrollaron la teor\'ia de probabilidades. 
\item No obstante durante cierto tiempo, la teor\'ia de las probabilidades limit\'o su aplicaci\'on a los juegos de azar y hasta el siglo XVIII no comenz\'o a aplicarse a los grandes problemas cient\'ificos.
\end{itemize}

%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item Godofredo Achenwall, profesor de la Universidad de Gotinga, acu\~n\'o en 1760 la palabra estad\'istica, que extrajo del t\'ermino italiano statista (estadista). 
\item Cre\'ia, y con sobrada raz\'on, que los datos de la nueva ciencia ser\'ian el aliado m\'as eficaz del gobernante consciente. 
\item La ra\'iz remota de la palabra se halla, por otra parte, en el t\'ermino latino status, que significa estado o situaci\'on; Esta etimolog\'ia aumenta el valor intr\'inseco de la palabra, por cuanto la estad\'istica revela el sentido cuantitativo de las m\'as variadas situaciones. 
\item Jacques Qu\'etelect es quien aplica las Estad\'isticas a las ciencias sociales. Este interpret\'o la teor\'ia de la probabilidad para su uso en las ciencias sociales y resolver la aplicaci\'on del principio de promedios y de la variabilidad a los fen\'omenos sociales.

\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item  Qu\'etelect fue el primero en realizar la aplicaci\'on pr\'actica de todo el m\'etodo Estad\'istico, entonces conocido, a las diversas ramas de la ciencia.

\item  Entretanto, en el per\'iodo del 1800 al 1820 se desarrollaron dos conceptos matem\'aticos fundamentales para la teor\'ia Estad\'istica; la teor\'ia de los errores de observaci\'on, aportada por Laplace y Gauss; y la teor\'ia de los m\'inimos cuadrados desarrollada por Laplace, Gauss y Legendre. 
\item  A finales del siglo XIX, Sir Francis Gaston ide\'o el m\'etodo conocido por Correlaci\'on, que ten\'ia por objeto medir la influencia relativa de los factores sobre las variables.

\end{itemize}

%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
\begin{itemize}
\item De aqu\'i parti\'o el desarrollo del coeficiente de correlaci\'on creado por Karl Pearson y otros cultivadores de la ciencia biom\'etrica como J. Pease Norton, R. H. Hooker y G. Udny Yule, que efectuaron amplios estudios sobre la medida de las relaciones.

\item Los progresos m\'as recientes en el campo de la Estad\'istica se refieren al ulterior desarrollo del c\'alculo de probabilidades, particularmente en la rama denominada indeterminismo o relatividad, se ha demostrado que el determinismo fue reconocido en la F\'isica como resultado de las investigaciones at\'omicas y que este principio se juzga aplicable tanto a las ciencias sociales como a las f\'isicas.

\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:Historia}
%____________________________________________________________
\subsection{Etapas de Desarrollo de la Estad\'istica}
%____________________________________________________________
La historia de la estad\'istica est\'a resumida en tres grandes etapas o fases.
\begin{itemize}
\item[Fase 1:] \textbf{Los Censos:}
Desde el momento en que se constituye una autoridad pol\'itica, la idea de inventariar de una forma m\'as o menos regular la poblaci\'on y las riquezas existentes en el territorio est\'a ligada a la conciencia de soberan\'ia y a los primeros esfuerzos administrativos.
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}
\item[Fase 2:] \textbf{De la Descripci\'on de los Conjuntos a la Aritm\'etica Pol\'itica:}
Las ideas mercantilistas extra\~nan una intensificaci\'on de este tipo de investigaci\'on. Colbert multiplica las encuestas sobre art\'iculos manufacturados, el comercio y la poblaci\'on: los intendentes del Reino env\'ian a Par\'is sus memorias. Vauban, m\'as conocido por sus fortificaciones o su Dime Royale, que es la primera propuesta de un impuesto sobre los ingresos, se se\~nala como el verdadero precursor de los sondeos. 
M\'as tarde, Buf\'on se preocupa de esos problemas antes de dedicarse a la historia natural. La escuela inglesa proporciona un nuevo progreso al superar la fase puramente descriptiva.
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}
\item[Fase 2:]
 Sus tres principales representantes son Graunt, Petty y Halley. El pen\'ultimo es autor de la famosa Aritm\'etica Pol\'itica. Chaptal, ministro del interior franc\'es, publica en 1801 el primer censo general de poblaci\'on, desarrolla los estudios industriales, de las producciones y los cambios, haci\'endose sistem\'aticos durantes las dos terceras partes del siglo XIX.
%\end{itemize}
%%\end{frame}
%%\begin{frame}\frametitle{Introducci\'on}
%\begin{itemize}

\item[Fase 3:] \textbf{Estad\'istica y C\'alculo de Probabilidades:}
El c\'alculo de probabilidades se incorpora r\'apidamente como un instrumento de an\'alisis extremadamente poderoso para el estudio de los fen\'omenos econ\'omicos y sociales y en general para el estudio de fen\'omenos cuyas causas son demasiados complejas para conocerlos totalmente y hacer posible su an\'alisis.
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Estad\'istica Descriptiva}


%____________________________________________________________
\subsection{Divisi\'on de la Estad\'istica}
%____________________________________________________________
La Estad\'istica para su mejor estudio se ha dividido en dos grandes ramas: \textbf{la Estad\'istica Descriptiva y la Estad\'istica Inferencial}.
\begin{itemize}
\item \textbf{Descriptiva:} consiste sobre todo en la presentaci\'on de datos en forma de tablas y gr\'aficas. Esta comprende cualquier actividad relacionada con los datos y est\'a dise\~nada para resumir o describir los mismos sin factores pertinentes adicionales; esto es, sin intentar inferir nada que vaya m\'as all\'a de los datos, como tales.
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Estad\'istica Inferencial}
\begin{itemize}
\item \textbf{Inferencial:} se deriva de muestras, de observaciones hechas s\'olo acerca de una parte de un conjunto numeroso de elementos y esto implica que su an\'alisis requiere de generalizaciones que van m\'as all\'a de los datos. Como consecuencia, la caracter\'istica m\'as importante del reciente crecimiento de la estad\'istica ha sido un cambio en el \'enfasis de los m\'etodos que describen a m\'etodos que sirven para hacer generalizaciones. La Estad\'istica Inferencial investiga o analiza una poblaci\'on partiendo de una muestra
tomada.
\end{itemize}
%\end{frame}
%____________________________________________________________
\subsection{Estad\'istica Inferencial}
%____________________________________________________________

%\begin{frame}\frametitle{Introducci\'on: Estad\'istica Inferencial}

Los m\'etodos b\'asicos de la estad\'istica inferencial son la estimaci\'on y el contrastede hip\'otesis, que juegan un papel fundamental en la investigaci\'on. Por tanto, algunos de los objetivos que se persiguen son:
\begin{itemize}
\item Calcular los par\'ametros de la distribuci\'on de medias o proporciones muestrales de tama\~no $n$, extra\'idas de una poblaci\'on de media y varianza conocidas.

\item Estimar la media o la proporci\'on de una poblaci\'on a partir de la media o proporci\'on muestral.

\item Utilizar distintos tama\~nos muestrales para controlar la confianza y el error admitido.
\item Contrastar los resultados obtenidos a partir de muestras.
\item Visualizar gr\'aficamente, mediante las respectivas curvas normales, las estimaciones realizadas.

\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Estad\'istica Inferencial}
\begin{itemize}
\item En definitiva, la idea es, a partir de una poblaci\'on se extrae una muestra por algunos de los m\'etodos existentes, con la que se generan datos num\'ericos que se van a utilizar para generar estad\'isticos con los que realizar estimaciones o contrastes poblacionales. 
\item Existen dos formas de estimar par\'ametros: la \textit{estimaci\'on puntual} y la \textit{estimaci\'on por intervalo de confianza}. En la primera se busca, con base en los datos muestrales, un \'unico valor estimado para el par\'ametro. Para la segunda, se determina un intervalo dentro del cual se encuentra el valor del par\'ametro, con una probabilidad determinada.
\end{itemize}

%%\end{frame}
%%\begin{frame}\frametitle{Introducci\'on: Estimaci\'on de Par\'ametros}


%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Estimaci\'on de Par\'ametros}
\begin{itemize}
\item Si el objetivo del tratamiento estad\'istico inferencial, es efectuar generalizaciones acerca de la estructura, composici\'on o comportamiento de las poblaciones no observadas, a partir de una parte de la poblaci\'on, ser\'a necesario que la parcela de poblaci\'on examinada sea representativa del total. 

\item Por ello, la selecci\'on de la muestra requiere unos requisitos que lo garanticen, debe ser representativa y aleatoria. 

\item Adem\'as, la cantidad de elementos que integran la muestra (el tama\~no de la muestra) depende de m\'ultiples factores, como el dinero y el tiempo disponibles para el estudio, la importancia del tema analizado, la confiabilidad que se espera de los resultados, las caracter\'isticas propias del fen\'omeno analizado, etc\'etera. 
\end{itemize}

%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: M\'etodo Estad\'istico}
\begin{itemize}
\item As\'i, a partir de la muestra seleccionada se realizan algunos c\'alculos y se estima el valor de los par\'ametros de la poblaci\'on tales como la media, la varianza, la desviaci\'on est\'andar, o la forma de la distribuci\'on, etc.

\end{itemize}
%\end{frame}
%____________________________________________________________
\subsection{M\'etodo Estad\'istico}
%____________________________________________________________

%\begin{frame}\frametitle{Introducci\'on: M\'etodo Estad\'istico}
El conjunto de los m\'etodos que se utilizan para medir las caracter\'isticas de la informaci\'on, para resumir los valores individuales, y para analizar los datos a fin de
extraerles el m\'aximo de informaci\'on, es lo que se llama \textit{m\'etodos estad\'isticos}. Los m\'etodos de an\'alisis para la informaci\'on cuantitativa se pueden dividir en los siguientes
seis pasos:
\begin{enumerate}
\item Definici\'on del problema.
\item Recopilaci\'on de la informaci\'on existente.
\item Obtenci\'on de informaci\'on original.
\item Clasificaci\'on.
\item Presentaci\'on.
\item An\'alisis.
\end{enumerate}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: M\'etodo Estad\'istico}
\begin{itemize}
\item El centro de gravedad de la metodolog\'ia estad\'istica se empieza a desplazar t\'ecnicas de computaci\'on intensiva aplicadas a grandes masas de datos, y se empieza a considerar el m\'etodo estad\'istico como un proceso iterativo de b\'usqueda del modelo ideal.

\item Las aplicaciones en este periodo de la Estad\'istica a la Econom\'ia conducen a una disciplina con contenido propio: la Econometr\'ia. La investigaci\'on estad\'istica en problemas militares durante la segunda guerra mundial y los nuevos m\'etodos de programaci\'on matem\'atica, dan lugar a la Investigaci\'on Operativa

\end{itemize}

%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Errores Estad\'isticos}

%____________________________________________________________
\subsection{Errores Estad\'isticos Comunes}
%____________________________________________________________
Al momento de recopilar los datos que ser\'an procesados se es susceptible de cometer errores as\'i como durante los c\'omputos de los mismos. No obstante, hay otros errores que no tienen nada que ver con la digitaci\'on y que no son tan f\'acilmente identificables. Algunos de \'estos errores son:
\begin{itemize}
\item\textbf{Sesgo:} Es imposible ser completamente objetivo o no tener ideas preconcebidas antes de comenzar a estudiar un problema, y existen muchas maneras en que una perspectiva o estado mental pueda influir en la recopilaci\'on y en el an\'alisis de la informaci\'on. En estos casos se dice que hay un sesgo cuando el individuo da mayor peso a los datos que apoyan su opini\'on que a aquellos que la contradicen. Un caso extremo de sesgo ser\'ia la situaci\'on donde primero se toma una decisi\'on y despu\'es se utiliza el an\'alisis estad\'istico para justificar la decisi\'on ya tomada.
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: M\'etodo Estad\'istico}
\begin{itemize}

\item\textbf{Datos No Comparables:} el establecer comparaciones es una de las partes m\'as importantes del an\'alisis estad\'istico, pero es extremadamente importante que tales comparaciones se hagan entre datos que sean comparables.
\item \textbf{Proyecci\'on descuidada de tendencias:} la proyecci\'on simplista de tendencias pasadas hacia el futuro es uno de los errores que m\'as ha desacreditado el uso del an\'alisis
estad\'istico.
%\end{itemize}
%%\end{frame}
%%\begin{frame}\frametitle{Introducci\'on: M\'etodo Estad\'istico}
%\begin{itemize}
\item\textbf{Muestreo Incorrecto:} en la mayor\'ia de los estudios sucede que el volumen de informaci\'on disponible es tan inmenso que se hace necesario estudiar muestras, para derivar conclusiones acerca de la poblaci\'on a que pertenece la muestra. Si la muestra se selecciona correctamente, tendr\'a b\'asicamente las mismas propiedades que la poblaci\'on de la cual fue extra\'ida; pero si el muestreo se realiza incorrectamente, entonces puede suceder que los resultados no signifiquen nada
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: M\'etodo Estad\'istico}
En resumen se puede decir que la Estad\'istica es un conjunto de procedimientos para reunir, clasificar, codificar, procesar, analizar y resumir informaci\'on num\'erica adquirida sistem\'aticamente (Ritchey, 2002). Permite hacer inferencias a partir de una muestra para extrapolarlas a una poblaci\'on. Aunque normalmente se asocia a muchos c\'alculos y operaciones aritm\'eticas, y aunque las matem\'aticas est\'an involucradas, en su mayor parte sus fundamentos y uso apropiado pueden dominarse sin hacer referencia a habilidades matem\'aticas avanzadas. 

De hecho se trata de una forma de ver la realidad basada en el an\'alisis cuidadoso de los hechos (Ritchey, 2002). Es necesaria sin embargo la sistematizaci\'on para reducir el efecto que las emociones y las experiencias individuales puedan tener al interpretar esa realidad.
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Refuerzo}

De esta manera la estad\'istica se relaciona con el m\'etodo cient\'ifico complement\'andolo como herramienta de an\'alisis y, aunque la investigaci\'on cient\'ifica no requiere necesariamente de la estad\'istica, \'esta valida muchos de los resultados cuantitativos derivados de la investigaci\'on. 

La obtenci\'on del conocimiento debe hacerse de manera sistem\'atica por lo que deben planearse todos los pasos que llevan desde el planteamiento de un problema, pasando por la elaboraci\'on de hip\'otesis y la manera en que van a ser probadas; la selecci\'on de sujetos (muestreo), los escenarios, los instrumentos que se utilizar\'an para obtener los datos, definir el procedimiento que se seguir\'a para esto \'ultimo, los controles que se deben hacer para asegurar que las intervenciones son las causas m\'as probables de los cambios esperados (dise\~no); 
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Refuerzo}

El tratamiento de los datos de la investigaci\'on cient\'ifica tiene varias etapas:
\begin{itemize}
\item En la etapa de recolecci\'on de datos del m\'etodo cient\'ifico, se define a la poblaci\'on de inter\'es y se selecciona una muestra o conjunto de personas representativas de la misma, se realizan experimentos o se emplean instrumentos ya existentes o de nueva creaci\'on, para medir los atributos de inter\'es necesarios para responder a las preguntas de investigaci\'on.Durante lo que es llamado trabajo de campo se obtienen los datos en crudo, es decir las respuestas directas de los sujetos uno por uno, se codifican (se les asignan valores a las respuestas), se capturan y se verifican para ser utilizados en las
siguientes etapas.
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Refuerzo}
\begin{itemize}

\item En la etapa de recuento, se organizan y ordenan los datos obtenidos de la muestra. Esta ser\'a descrita en la siguiente etapa utilizando la estad\'istica descriptiva, todas las investigaciones utilizan estad\'istica descriptiva, para conocer de manera organizada y resumida las caracter\'isticas de la muestra.

\item En la etapa de an\'alisis se utilizan las pruebas estad\'isticas (estad\'istica inferencial) y en la interpretaci\'on se acepta o rechaza la hip\'otesis nula.
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}

\item En investigaci\'on, el fen\'omeno en estudio puede ser cualitativo que implicar\'ia comprenderlo y explicarlo, o cuantitativo para compararlo y hacer inferencias. Se puede decir que si se hace an\'alisis se usan m\'etodos cuantitativos y si se hace descripci\'on se usan m\'etodos cualitativos. 


\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on}

Medici\'on Para poder emplear el m\'etodo estad\'istico en un estudio es necesario medir las variables. 
\begin{itemize}
\item Medir: es asignar valores a las propiedades de los objetos bajo ciertas reglas, esas reglas son los niveles de medici\'on

\item Cuantificar: es asignar valores a algo tomando un patr\'on de referencia. Por ejemplo, cuantificar es ver cu\'antos hombres y cu\'antas mujeres hay.

\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Refuerzo}
\begin{itemize}
\item Variable: es una caracter\'istica o propiedad que asume diferentes valores dentro de una poblaci\'on de
inter\'es y cuya variaci\'on es susceptible de medirse.
Las variables pueden clasificarse de acuerdo al tipo de valores que puede tomar como:
\begin{itemize}
\item Discretas o categ\'oricas.- en las que los valores se relacionan a nombres, etiquetas o categor\'ias, no existe un significado num\'erico directo
\item Continuas.- los valores tienen un correlato num\'erico directo, son continuos y susceptibles de fraccionarse y de poder utilizarse en operaciones aritm\'eticas De acuerdo a la cantidad de valores
\item Dicot\'omica.- s\'olo tienen dos valores posibles, la caracter\'istica est\'a ausente o presente
\item Policot\'omica.- pueden tomar tres valores o m\'as, pueden tomarse matices diferentes, en grados, jerarqu\'ias o magnitudes continuas.
\end{itemize}
\end{itemize}
%\end{frame}
%\end{document}
%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}
\item En cuanto a una clasificaci\'on estad\'istica
\begin{itemize}
\item Aleatoria.- Aquella en la cual desconocemos el valor porque fluct\'ua de acuerdo a un evento debido al azar
\item Determin\'istica.- Aquella variable de la que se conoce el valor
\item Independiente.- aquellas variables que son manipuladas por el investigador. Define los
grupos
\item Dependiente.- son mediciones que ocurren durante el experimento o tratamiento
(resultado de la independiente), es la que se mide y compara entre los grupos
\end{itemize}
\end{itemize}
%\end{frame}
%\end{document}
%\begin{frame}\frametitle{Introducci\'on}
Niveles de Medici\'on
\begin{itemize}
\item Nominal
Las propiedades de la medici\'on nominal son:
\begin{itemize}
\item Exhaustiva: implica a todas las opciones
\item A los sujetos se les asignan categor\'ias, por lo que son mutuamente excluyentes. Es decir, la
variable est\'a presente o no; tiene o no una caracter\'istica
\end{itemize}
\item Ordinal
Las propiedades de la medici\'on ordinal son:
\begin{itemize}
\item El nivel ordinal posee transitividad, por lo que se tiene la capacidad de identificar que es mejor o mayor que otra, en ese sentido se pueden establecer jerarqu\'ias
\item Las distancias entre un valor y otro no son iguales.
\end{itemize}
\end{itemize}

%\end{frame}
%\end{document}
%\begin{frame}\frametitle{Introducci\'ono}
\begin{itemize}

\item Intervalo
\begin{itemize}
\item  El nivel de medici\'on intervalar requiere distancias iguales entre cada valor. Por lo general
utiliza datos cuantitativos. Por ejemplo: temperatura, atributos psicol\'ogicos (CI, nivel de
autoestima, pruebas de conocimientos, etc.)
\item Las unidades de calificaci\'on son equivalentes en todos los puntos de la escala. Una escala de
intervalos implica: clasificaci\'on, magnitud y unidades de tama\~nos iguales (Brown, 2000).
\item Se pueden hacer operaciones aritm\'eticas
\item Cuando se le pide al sujeto que califique una situaci\'on del 0 al 10 puede tomarse como un
nivel de medici\'on de intervalo, siempre y cuando se incluya el 0.
\end{itemize}
\end{itemize}
%\end{frame}
%\end{document}
%\begin{frame}\frametitle{Introducci\'on: Refuerzo}
\begin{itemize}

\item Raz\'on
\begin{itemize}
\item La escala empieza a partir del 0 absoluto, por lo tanto incluye s\'olo los n\'umeros por su valor
en s\'i, por lo que no pueden existir los n\'umeros con signo negativo. Por ejemplo: Peso
corporal en kg., edad en a\~nos, estatura en cm.
\item Convencionalmente los datos que son de nivel absoluto o de raz\'on son manejados como los
datos intervalares.
\end{itemize}

\end{itemize}
%\end{frame}

\subsection{T\'erminos comunes utilizados en Estad\'istica}
%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}
\item \textbf{Variable: } Consideraciones que una variable son una caracter\'istica o fen\'omeno que puede tomar distintos valores.

\item \textbf{Dato: } Mediciones o cualidades que han sido recopiladas como resultado de observaciones.

\item \textbf{Poblaci\'on: } Se considera el \'area de la cual son extra\'idos los datos. Es decir, es el conjunto de elementos o individuos que poseen una caracter\'istica com\'un y medible acerca de lo cual se desea informaci\'on. Es tambi\'en llamado
Universo.
\item \textbf{Muestra: } Es un subconjunto de la poblaci\'on, seleccionado de acuerdo a una regla o alg\'un plan de muestreo.

\item \textbf{Censo: } Recopilaci\'on de todos los datos (de inter\'es para la investigaci\'on) de la poblaci\'on.

\item \textbf{Estad\'istica: } Es una funci\'on o f\'ormula que depende de los datos de la muestra (es variable).
\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}

\item \textbf{Par\'ametro: } Caracter\'istica medible de la poblaci\'on. Es un resumen num\'erico de alguna variable observada de la poblaci\'on. Los par\'ametros normales que se estudian son: \textit{ La media poblacional, la media poblacional, Proporci\'on.}

\item \textbf{Estimador}: Un estimador  de un par\'ametro , es un estad\'istico que se emplea para conocer el par\'ametro desconocido.
\item \textbf{Estad\'istico}: Es una funci\'on de los valores de la muestra. Es una variable aleatoria, cuyos valores dependen de la muestra seleccionada. Su distribuci\'on de probabilidad, se conoce como \textit{Distribuci\'on muestral del estad\'istico}.
\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Introducci\'ono}
\begin{itemize}
\item \textbf{Estimaci\'on}: Este t\'ermino indica que a partir de lo observado en una muestra (un resumen estad\'istico con las medidas que conocemos de Descriptiva) se extrapola o generaliza dicho resultado muestral a la poblaci\'on total, de modo que lo estimado es el valor generalizado a la poblaci\'on. Consiste en la b\'usqueda del valor de los par\'ametros poblacionales objeto de estudio. Puede ser puntual o por intervalo de confianza:
\begin{itemize}
\item\textit{Puntual:}  cuando buscamos un valor concreto. Un estimador de un par\'ametro poblacional es una funci\'on de los datos muestrales. En pocas palabras, es una f\'ormula que depende de los valores obtenidos de una muestra, para realizar estimaciones. Lo que se pretende obtener es el valor exacto de un par\'ametro. 
\end{itemize}
\end{itemize}

%\end{frame}
%\end{document}
%\begin{frame}\frametitle{Introducci\'on}

Las propiedades deseables de un estimador son los siguientes:
\begin{itemize}
\item Insesgado: Diremos que un estimador de un par\'ametro es insesgado si su esperanza coincide con el verdadero valor del par\'ametro. En el caso de que no coincidan, diremos que el estimador es sesgado.
\item  Eficiencia: Dados dos estimadores  para un mismo par\'ametro , se dice que uno es m\'a eficiente que el otro si tiene menor varianza.
\item Suficiencia: Se dice que un estimador de un par\'ametro es suficiente cuando para su c\'alculo utiliza toda la informaci\'on de la muestra.
\item Consistencia: Decimos que un estimador  de un par\'ametro  es consistente si la distribuci\'on del estimador tiende a concentrarse en un cierto punto cuando el tama\~no de la muestra tiende a infinito.
\end{itemize}

%\end{frame}
%\end{document}

%\begin{frame}\frametitle{Introducci\'on}
Demostrar que un cierto estimador cumple estas propiedades puede ser
complicado en determinadas ocasiones. Existen varios m\'etodos que nos van a permitir obtener los estimadores puntuales. Los m\'as importantes son:
\begin{itemize}
\item  M\'etodo de Momentos: se basa en que los momentos poblacionales y se estiman mediante los momentos muestrales. Suelen dar estimadores consistentes.
\item M\'etodo de M\'inimos Cuadrados: consiste en obtener un estimador que hace m\'inima una determinada funci\'on.
\item M\'etodo de M\'axima Verosimilitud: consiste en tomar como par\'ametro poblacional el valor de la muestra que sea m\'as probable, es decir, que tenga mayor probabilidad. Se suelen obtener estimadores consistentes y eficientes. Es el m\'as utilizado.
\end{itemize}
%\end{frame}
%\end{document}
%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}

\item\textit{Intervalo de confianza:}  cuando determinamos un intervalo, dentro del cual se supone que va a estar el valor del par\'ametro que se busca con una cierta probabilidad. El intervalo de confianza est\'a determinado por dos valores dentro de los cuales afirmamos que est\'a el verdadero par\'ametro con cierta probabilidad. Son unos l\'imites o margen de variabilidad que damos al valor estimado, para poder afirmar, bajo un
criterio de probabilidad, que el verdadero valor no los rebasar\'a.

Este intervalo contiene al par\'ametro estimado con una determinada certeza o nivel de confianza.
\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Introducci\'on}
En la estimaci\'on por intervalos se usan los siguientes conceptos:
\begin{itemize}
\item Variabilidad del par\'ametro: Si no se conoce, puede obtenerse una
aproximaci\'on en los datos o en un estudio piloto. Tambi\'en hay m\'etodos para calcular el tama\~no de la muestra que prescinden de este aspecto. Habitualmente se usa como medida de esta variabilidad la desviaci\'on t\'ipica poblacional.
\item Error de la estimaci\'on: Es una medida de su precisi\'on que se corresponde con la amplitud del intervalo de confianza. Cuanta m\'as precisi\'on se desee en la estimaci\'on de un par\'ametro, m\'as estrecho deber\'a ser el intervalo de confianza y, por tanto, menor el error, y m\'as sujetos deber\'an incluirse en la muestra estudiada. 

\item Nivel de confianza: Es la probabilidad de que el verdadero valor del par\'ametro estimado en la poblaci\'on se sit\'ue en el intervalo de confianza obtenido. El nivel de confianza se denota por $1-\alpha$

\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}

\item $p$-value : Tambi\'en llamado nivel de significaci\'on. Es la probabilidad (en tanto por uno) de fallar en nuestra estimaci\'on, esto es, la diferencia entre la certeza (1) y el nivel de confianza $1-\alpha$. 

\item Valor cr\'itico: Se representa por $Z_{\alpha/2}$ . Es el valor de la abscisa en una determinada distribuci\'on que deja a su derecha un \'area igual a 1/2, siendo $1-\alpha$ el nivel de confianza. Normalmente los valores cr\'iticos est\'an tabulados o pueden calcularse en funci\'on de la distribuci\'on de la poblaci\'on. 
\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Introducci\'on}
Para un tama\~no fijo de la muestra, los conceptos de error y nivel de confianza van relacionados. Si admitimos un error mayor, esto es, aumentamos el tama\~no del intervalo de confianza, tenemos tambi\'en una mayor probabilidad de \'exito en nuestra estimaci\'on, es decir, un mayor nivel de confianza. Por tanto, un aspecto que debe de tenerse en cuenta es el tama\~no muestral, ya que para disminuir el error que se comente habr\'a que aumentar el tama\~no muestral. Esto se resolver\'a, para un intervalo de confianza cualquiera, despejando el tama\~no de la muestra en cualquiera de las formulas de los intervalos de confianza que veremos a continuaci\'on, a partir del error m\'aximo permitido. Los intervalos de confianza pueden ser unilaterales o bilaterales:


%\end{frame}
%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}




\item \textbf{Contraste de Hip\'otesis}:  Consiste en determinar si es aceptable, partiendo de datos muestrales, que la caracter\'istica o el par\'ametro poblacional estudiado tome un determinado valor o est\'e dentro de unos determinados valores.

\item \textbf{Nivel de Confianza}: Indica la proporci\'on de veces que acertar\'iamos al afirmar que el par\'ametro est\'a dentro del intervalo al seleccionar muchas muestras.
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on}

\subsection{Muestreo:} 
Una muestra es representativa en la medida que es imagen de la poblaci\'on. 

En general, podemos decir que el tama\~no de una muestra depender\'a principalmente de: \textit{Nivel de precisi\'on deseado, Recursos disponibles,  Tiempo involucrado en la investigaci\'on.}.  Adem\'as el plan de muestreo debe considerar \textit{ La poblaci\'on, Par\'ametros a medir}.
%\end{frame}

%\begin{frame}\frametitle{Introducci\'on}
Existe una gran cantidad de tipos de muestreo. En la pr\'actica los m\'as utilizados son los siguientes:
\begin{itemize}
\item \textbf{MUESTREO ALEATORIO SIMPLE: } Es un m\'etodo de selecci\'on de $n$ unidades extra\'idas de $N$, de tal manera que cada una de las posibles muestras tiene la misma probabilidad de ser escogida. (En la pr\'actica, se enumeran las unidades de 1 a $N$, y a continuaci\'on se seleccionan $n$ n\'umeros aleatorios entre 1 y $N$, ya sea de tablas o de alguna urna con fichas numeradas).
\item \textbf{MUESTREO ESTRATIFICADO ALEATORIO: } Se usa cuando la poblaci\'on est\'a agrupada en pocos estratos, cada uno de ellos son muchas entidades. Este muestreo consiste en sacar una muestra aleatoria simple de cada uno de los estratos. (Generalmente, de tama\~no proporcional al estrato).

\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}



\item \textbf{MUESTREO SISTEM\'aTICO :} Se utiliza cuando las unidades de la poblaci\'on est\'an de alguna manera totalmente ordenadas. Para seleccionar una muestra de $n$ unidades, se divide la poblaci\'on en $n$ subpoblaciones de tama\~no $K = N/n$ y se toma al azar una unidad de la $K$ primeras y de ah\'i en adelante cada $K$-\'esima unidad.

\item \textbf{MUESTREO POR CONGLOMERADO: } Se emplea cuando la poblaci\'on est\'a dividida en grupos o conglomerados peque\~nos. Consiste en obtener una muestra aleatoria simple de conglomerados y luego CENSAR cada uno de \'estos.
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on:}
\begin{itemize}

\item \textbf{MUESTREO EN DOS ETAPAS (Biet\'apico): } En este caso la muestra se toma en dos pasos:
\begin{itemize}
\item Seleccionar una muestra de unidades primarias, y Seleccionar una muestra de elementos a partir de cada unidad primaria escogida.
\item Observaci\'on: En la realidad es posible encontrarse con situaciones en las cuales no es posible aplicar libremente un tipo de muestreo, incluso estaremos obligados a mezclarlas en ocasiones.
\end{itemize}
\end{itemize}
%\end{frame}
\subsection{Variables}
%\begin{frame}\frametitle{Introducci\'on}


Las variables se pueden clasificar en dos grandes grupos.
\begin{itemize}
\item \textbf{Variables categ\'oricas: } Son aquellas que pueden ser representadas a trav\'es de s\'imbolos, letras, palabras, etc. Los valores que toman se denominan categor\'ias, y los elementos que pertenecen a estas categor\'ias, se consideran id\'enticos respecto a la caracter\'istica que se est\'a midiendo. Las variables categ\'oricas de dividen en dos tipos: Ordinal y Nominal.

\begin{itemize}
\item \textbf{Las Ordinales}, son aquellas en que las categor\'ias tienen un orden impl\'icito. Admiten grados de calidad, es decir, existe una relaci\'on total entre las categor\'ias.

\item \textbf{Las nominales,} son aquellas donde no existe una relaci\'on de orden.

\end{itemize}
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on}
\begin{itemize}

\item \textbf{Variables Num\'ericas}: Son aquellas que pueden tomar valores num\'ericos exclusivamente (mediciones). Se  dividen en dos tipos: Discretas y continuas. 
\begin{itemize}
\item\textbf{Discretas:} son aquellas que toman sus valores en un conjunto finito o infinito numerable.
\item\textbf{Continuas:}  Son aquellas que toman sus valores en un subconjunto de los n\'umeros reales, es decir en un intervalo. En general para las variables continuas el hombre ha debido inventar una medida para
poder establecer una medici\'on de ellas.

\end{itemize}
\end{itemize}

%\end{frame}



\subsection{Malos Usos de la Estad\'istica}
%\begin{frame}\frametitle{Introducci\'on}

El prop\'osito de esta secci\'on es solamente indicar los malos usos comunes de datos estad\'isticos, sin incluir el uso de m\'etodos estad\'isticos complicados. Un estudiante deber\'ia estar alerta en relaci\'on con estos malos usos y deber\'ia hacer un gran esfuerzo para evitarlos a fin de ser un verdadero estad\'istico.

\textbf{Datos estad\'isticos inadecuados}
Los datos estad\'isticos son usados como la materia prima para un estudio estad\'istico. Cuando los datos son inadecuados, la conclusi\'on extra\'ida del estudio de los datos se vuelve obviamente inv\'alida. Por ejemplo, supongamos que deseamos encontrar el ingreso familiar t\'ipico del a\~no pasado en la ciudad Y de 50,000 familias y tenemos una muestra consistente del ingreso de solamente tres familias: 1 mill\'on, 2 millones y no ingreso. Si sumamos el ingreso de las tres familias y dividimos el total por 3, obtenemos un promedio de 1 mill\'on.
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Refuerzo}

Entonces, extraemos una conclusi\'on basada en la muestra de que el ingreso familiar promedio durante el a\~no pasado en la ciudad fue de  1 mill\'on. 

Es obvio que la conclusi\'on es falsa, puesto que las cifras son extremas y el tama\~no de la muestra es demasiado peque\~no; por lo tanto la muestra no es representativa. 

Hay muchas otras clases de datos inadecuados. Por ejemplo, algunos datos son respuestas inexactas de una encuesta, porque las preguntas usadas en la misma son vagas o enga\~nosas, algunos datos son toscas estimaciones porque no hay disponibles datos exactos o es demasiado costosa su obtenci\'on, y algunos datos son irrelevantes en un problema dado, porque el estudio estad\'istico no est\'a bien planeado.
%\end{frame}


\subsection{Un sesgo del usuario}
%\begin{frame}\frametitle{Introducci\'on: Refuerzo}

 Sesgo significa que un usuario d\'e los datos perjudicialmente de m\'as \'enfasis a los hechos, los cuales son empleados para mantener su predeterminada posici\'on u opini\'on. 
 
Los estad\'isticos son frecuentemente degradados por lemas tales como :Hay tres clases de mentiras: mentiras, mentiras reprobables y estad\'istica, y Las cifras no mienten, pero los mentirosos piensas.
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Sesgo}

Hay dos clases de sesgos: conscientes e inconscientes. Ambos son comunes en el an\'alisis estad\'istico. Hay numerosos ejemplos de sesgos conscientes. 

Un anunciante frecuentemente usa la estad\'istica para probar que su producto es muy superior al producto de su competidor. 

Un pol\'itico prefiere usar la estad\'istica para sostener su punto de vista. 

Gerentes y l\'ideres de trabajadores pueden simult\'aneamente situar sus respectivas cifras estad\'isticas sobre la misma tabla de trato para mostrar que sus rechazos o peticiones son justificadas. 
%\end{frame}
%\begin{frame}\frametitle{Introducci\'on: Refuerzo}

Es casi imposible que un sesgo inconsciente est\'e completamente ausente en un trabajo estad\'istico. 

En lo que respecta al ser humano, es dif\'icil obtener una actitud completamente objetiva al abordar un problema, aun cuando un cient\'ifico deber\'ia tener una mente abierta. 

Un estad\'istico deber\'ia estar enterado del hecho de que su interpretaci\'on de los resultados del an\'alisis estad\'istico est\'a influenciado por su propia experiencia, conocimiento y antecedentes con relaci\'on al problema dado.
%\end{frame}

\subsection{ Supuestos falsos}
%\begin{frame}\frametitle{Introducci\'on: Refuerzo}

Es muy frecuente que un an\'alisis estad\'istico contemple supuestos. Un investigador debe
ser muy cuidadoso en este hecho, para evitar que \'estos sean falsos. Los supuestos falsos pueden ser originados por:
\begin{itemize}
\item Quien usa los datos
\item Quien est\'a tratando de confundir (con intencionalidad)
\item Ignorancia
\item Descuido.
\end{itemize}

%\end{frame}



%\begin{frame}\frametitle{Introducci\'on: Refuerzo}
T\'ERMINOS COMUNES UTILIZADOS EN ESTAD\'ISTICA
\begin{itemize}
\item Variable: Consideraciones que una variable son una caracter\'istica o fen\'omeno que
puede tomar distintos valores.
\item Dato: Mediciones o cualidades que han sido recopiladas como resultado de
observaciones.
\item Poblaci\'on: Se considera el \'area de la cual son extra\'idos los datos. Es decir, es el
conjunto de elementos o individuos que poseen una caracter\'istica com\'un y
medible acerca de lo cual se desea informaci\'on. Es tambi\'en llamado
Universo.
\item Muestra: Es un subconjunto de la poblaci\'on, seleccionado de acuerdo a una regla o
alg\'un plan de muestreo.
\item Censo: Recopilaci\'on de todos los datos (de inter\'es para la investigaci\'on) de la
poblaci\'on.
\item Estad\'istica: Es una funci\'on o f\'ormula que depende de los datos de la muestra (es
variable).
\end{itemize}
%\end{frame}



%\begin{frame}\frametitle{Introducci\'on: Refuerzo}
\begin{itemize}
\item Par\'ametro Caracter\'istica medible de la poblaci\'on.
\end{itemize}

\textbf{Ejemplo:} La universidad est\'a interesada en determinar el ingreso de las familias de
sus alumnos.
\begin{itemize}
\item Variable: Ingreso perc\'apita de las familias.
\item Dato: Ingreso perc\'apita de la familia de un alumno espec\'ifico.
\item Poblaci\'on: Las familias de todos los alumnos de la universidad.
\item Estad\'istica: Ingreso perc\'apita promedio de las familias seleccionadas en la
muestra.
\item Par\'ametro: Ingreso perc\'apita promedio de la poblaci\'on.
\end{itemize}
%\end{frame}


%\begin{frame}
\textbf{MUESTREO}
Una muestra es representativa en la medida que es imagen de la poblaci\'on.
En general, podemos decir que el tama\~no de una muestra depender\'a principalmente de:


\begin{itemize}
\item  Nivel de precisi\'on deseado.
\item Recursos disponibles.
\item Tiempo involucrado en la investigaci\'on.
\end{itemize}

Adem\'as el plan de muestreo debe considerar
\begin{itemize}
\item La poblaci\'on
\item Par\'ametros a medir.
\end{itemize}


%\end{frame}



%\begin{frame}\frametitle{Introducci\'on: Refuerzo}
Existe una gran cantidad de tipos de muestreo. En la pr\'actica los m\'as utilizados son los
siguientes:
\begin{itemize}
\item MUESTREO ALEATORIO SIMPLE:
Es un m\'etodo de selecci\'on de n unidades extra\'idas de $N$, de tal manera que cada una de
las posibles muestras tiene la misma probabilidad de ser escogida.
(En la pr\'actica, se enumeran las unidades de $1$ a $N$, y a continuaci\'on se seleccionan $n$
n\'umeros aleatorios entre $1$ y $N$, ya sea de tablas o de alguna urna con fichas numeradas).
\item MUESTREO ESTRATIFICADO ALEATORIO:
Se usa cuando la poblaci\'on est\'a agrupada en pocos estratos, cada uno de ellos son muchas
entidades. Este muestreo consiste en sacar una muestra aleatoria simple de cada uno de los
estratos. (Generalmente, de tama\~no proporcional al estrato).
\end{itemize}
%\end{frame}



%\begin{frame}
\begin{itemize}
\item MUESTREO SISTEM\'ATICO :
Se utiliza cuando las unidades de la poblaci\'on est\'an de alguna manera totalmente
ordenadas.
Para seleccionar una muestra de n unidades, se divide la poblaci\'on en $n$ subpoblaciones
de tama\~no $K = N/n$ y se toma al azar una unidad de la $K$ primeras y de ah\'i en adelante cada $K-$
\'esima unidad, es decir, siendo n o la primera unidad seleccionada de la sub-poblaci\'on $\left(1, 2,...K\right)$.
$\left\{n, n+ K, n+ 2K, .... , n+ (n-1) K \right\}$

\item MUESTREO POR CONGLOMERADO
Se emplea cuando la poblaci\'on est\'a dividida en grupos o conglomerados peque\~nos.
Consiste en obtener una muestra aleatoria simple de conglomerados y luego CENSAR cada uno
de \'estos.
\end{itemize}
%\end{frame}


%\begin{frame}
\begin{itemize}

\item MUESTREO EN DOS ETAPAS (Biet\'apico)
En este caso la muestra se toma en dos pasos:
\begin{itemize}
\item Seleccionar una muestra de unidades primarias, y
\item Seleccionar una muestra de elementos a partir de cada unidad primaria escogida.
\end{itemize}
\end{itemize}

\textbf{Observaci\'on}:
En la realidad es posible encontrarse con situaciones en las cuales no es posible aplicar
libremente un tipo de muestreo, incluso estaremos obligados a mezclarlas en ocasiones.
En general la Estad\'istica est\'a encargada de llevar a cabo el siguiente esquema:
\begin{itemize}
\item Recopilar
\item Organizar
\item Presentar
\item Analizar

\end{itemize}

%\end{frame}


%\begin{frame}
Tipos de variables
Las variables se pueden clasificar en dos grandes grupos.
\begin{itemize}
\item categ\'oricas:
Son aquellas que pueden ser representadas a trav\'es de s\'imbolos, letras, palabras, etc. Los
valores que toman se denominan categor\'ias, y los elementos que pertenecen a estas categor\'ias, se
consideran id\'enticos respecto a la caracter\'istica que se est\'a midiendo. 

Las variables categ\'oricas de dividen en dos tipos: Ordinal y Nominal.
\begin{itemize}
\item Las Ordinales, son aquellas en que las categor\'ias tienen un orden impl\'icito. Admiten
grados de calidad, es decir, existe una relaci\'on total entre las categor\'ias. A pesar de que esta variable admite grados de calidad, no es posible cuantificar la diferencia.
\item Las nominales, son aquellas donde no existe una relaci\'on de orden.
\end{itemize}
\end{itemize}
%\end{frame}


%\begin{frame}
\begin{itemize}
\item Variables num\'ericas. Son aquellas que pueden tomar valores num\'ericos exclusivamente (mediciones).
dividen en dos tipos. Discretas y continuas. 
\begin{itemize}
\item 
Discretas: son aquellas que toman sus valores en un conjunto finito o infinito numerable.
\item Continuas: Son aquellas que toman sus valores en un subconjunto de los n\'umeros reales,
es decir en un intervalo.
\end{itemize}
\end{itemize}
%\end{frame}
%\begin{frame}

\textbf{Observaci\'on:}
En general para las variables continuas el hombre ha debido inventar una medida para
poder establecer una medici\'on de ellas:
Ejemplo: El metro, la hora.
%\end{frame}


%\begin{frame}
Los m\'etodos b\'asicos de la estad\'istica inferencial son la estimaci\'on y el contraste
de hip\'otesis, que juegan un papel fundamental en la investigaci\'on. Por tanto, algunos de los objetivos que se persiguen en este tema son:Inferencia, estimaci\'on y contraste de hip\'otesis

\begin{itemize}
\item Calcular los par\'ametros de la distribuci\'on de medias o proporciones muestrales
de tama\~no n, extra\'idas de una poblaci\'on de media y varianza conocidas.
\item Estimar la media o la proporci\'on de una poblaci\'on a partir de la media o
proporci\'on muestral.
\item Utilizar distintos tama\~nos muestrales para controlar la confianza y el error
admitido.
\item Contrastar los resultados obtenidos a partir de muestras.
\item Visualizar gr\'aficamente, mediante las respectivas curvas normales, las
estimaciones realizadas.
\end{itemize}
%\end{frame}


%\begin{frame}

En la mayor\'ia de las investigaciones resulta imposible estudiar a todos y cada
uno de los individuos de la poblaci\'on ya sea por el coste que supondr\'ia, o por la
imposibilidad de acceder a ello. Mediante la t\'ecnica inferencial obtendremos
conclusiones para una poblaci\'on no observada en su totalidad, a partir de estimaciones o
res\'umenes num\'ericos efectuados sobre la base informativa extra\'ida de una muestra de
dicha poblaci\'on. 
\begin{itemize}
\item Por tanto, el esquema que se sigue es, a partir de una poblaci\'on se extrae una muestra por
algunos de los m\'etodos existentes, con la que se generan datos num\'ericos que se van a
utilizar para generar estad\'isticos con los que realizar estimaciones o contrastes
poblacionales.
\end{itemize}

%\end{frame}


%\begin{frame}
\begin{itemize}

\item Existen dos formas de estimar par\'ametros: la estimaci\'on puntual y la
estimaci\'on por intervalo de confianza. En la primera se busca, con base en los datos
muestrales, un \'unico valor estimado para el par\'ametro. Para la segunda, se determina un
intervalo dentro del cual se encuentra el valor del par\'ametro, con una probabilidad
determinada.
\item Si el objetivo del tratamiento estad\'istico inferencial, es efectuar generalizaciones
acerca de la estructura, composici\'on o comportamiento de las poblaciones no
observadas, a partir de una parte de la poblaci\'on, ser\'a necesario que la parcela de
poblaci\'on examinada sea representativa del total. Por ello, la selecci\'on de la muestra
requiere unos requisitos que lo garanticen, debe ser representativa y aleatoria.

\end{itemize}

%\end{frame}



%\begin{frame}
Adem\'as, la cantidad de elementos que integran la muestra (el tama\~no de la
muestra) depende de m\'ultiples factores, como el dinero y el tiempo disponibles para el
estudio, la importancia del tema analizado, la confiabilidad que se espera de los
resultados, las caracter\'isticas propias del fen\'omeno analizado, etc\'etera. As\'i, a partir de
la muestra seleccionada se realizan algunos c\'alculos y se estima el valor de los
par\'ametros de la poblaci\'on tales como la media, la varianza, la desviaci\'on est\'andar, o la
forma de la distribuci\'on, etc.

%\end{frame}



%\begin{frame}
Conceptos b\'asicos
\begin{itemize}
\item POBLACI\'ON: Conjunto de elementos sobre los que se observa un car\'acter com\'un. Se
representa con la letra $N$.
\item MUESTRA: Conjunto de unidades de una poblaci\'on. Cuanto m\'as significativa sea,
mejor ser\'a la muestra. Se representa con la letra $n$.
\item UNIDAD DE MUESTREO: Est\'a formada por uno o m\'as elementos de la poblaci\'on.
El total de unidades de muestreo constituyen la poblaci\'on. Estas unidades son disjuntas
entre s\'i y cada elemento de la poblaci\'on pertenece a una unidad de muestreo.
\item PAR\'AMETRO: Es un resumen num\'erico de alguna variable observada de la
poblaci\'on. 
\end{itemize}


%\end{frame}



%\begin{frame}
\begin{itemize}
\item ESTIMADOR: Un estimador  de un par\'ametro,  es un estad\'istico que se emplea
para conocer el par\'ametro desconocido.
\item ESTAD\'ISTICO: Es una funci\'on de los valores de la muestra. Es una variable aleatoria,
cuyos valores dependen de la muestra seleccionada. Su distribuci\'on de probabilidad, se
conoce como Distribuci\'on muestral del estad\'istico.
\item ESTIMACI\'ON: Este t\'ermino indica que a partir de lo observado en una muestra (un
resumen estad\'istico con las medidas que conocemos de Descriptiva) se extrapola o
generaliza dicho resultado muestral a la poblaci\'on total, de modo que lo estimado es el
valor generalizado a la poblaci\'on. Consiste en la b\'usqueda del valor de los par\'ametros
poblacionales objeto de estudio. Puede ser puntual o por intervalo de confianza:
\end{itemize}

%\end{frame}


%\begin{frame}
Estimaci\'on 
\begin{itemize}
\item Puntual: cuando buscamos un valor concreto.Inferencia, estimaci\'on y contraste de hip\'otesis
\item Intervalo de confianza: cuando determinamos un intervalo, dentro del cual se
supone que va a estar el valor del par\'ametro que se busca con una cierta
probabilidad.
\end{itemize}
%\end{frame}


%\begin{frame}

\begin{itemize}

\item CONTRATE DE HIP\'OTESIS: Consiste en determinar si es aceptable, partiendo de
datos muestrales, que la caracter\'istica o el par\'ametro poblacional estudiado tome un
determinado valor o est\'e dentro de unos determinados valores.

\item NIVEL DE CONFIANZA: Indica la proporci\'on de veces que acertar\'iamos al afirmar
que el par\'ametro est\'a dentro del intervalo al seleccionar muchas muestras.
\end{itemize}


EL CONCEPTO DE ESTAD\'iSTICO Y DISTRIBUCI\'ON MUESTRAL
\begin{itemize}
\item El objetivo de la inferencia es efectuar una generalizaci\'on de los resultados de la
muestra de la poblaci\'on. La tarea que nos ocupa ahora es conocer las distribuciones de
la probabilidad de ciertas funciones de la muestra, es decir, variables aleatorias
asociadas al muestreo o estad\'isticos muestrales. \'estos ser\'an \'utiles para hacer
inferencia respecto a los par\'ametros desconocidos de una poblaci\'on.

\end{itemize}
%\end{frame}


%\begin{frame}

\begin{itemize}
\item  Por ello se habla de distribuciones muestrales, ya que est\'an basados en el comportamiento de las
muestras.

\item El primer objetivo es conocer el concepto de distribuci\'on muestral de un
estad\'istico; su comportamiento probabil\'istico depender\'a del que tenga la variable $X$ y
del tama\~no de las muestras.

\item Sea una poblaci\'on donde se observa la variable aleatoria X. Esta variable X,
tendr\'a una distribuci\'on de probabilidad, que puede ser conocida o desconocida, y ciertas
caracter\'isticas o par\'ametros poblacionales. 

\item El problema ser\'a encontrar una funci\'on que proporcione el mejor estimador de El estimador, T, del par\'ametro debe tener una
distribuci\'on concentrada alrededor de la media  y la varianza debe ser lo menor posible.

\item Los estad\'isticos m\'as usuales en inferencia y su distribuci\'on asociada considerando una poblaci\'on P sobre la que se estudia un car\'acter cuantitativo son:

\end{itemize}
%\end{frame}


%\begin{frame}

\begin{itemize}
\item  CONTRATE DE HIP\'oTESIS: Consiste en determinar si es aceptable, partiendo de
datos muestrales, que la caracter\'istica o el par\'ametro poblacional estudiado tome un
determinado valor o est\'e dentro de unos determinados valores.

\item NIVEL DE CONFIANZA: Indica la proporci\'on de veces que acertar\'iamos al afirmar
que el par\'ametro est\'a dentro del intervalo al seleccionar muchas muestras.

\end{itemize}


El objetivo de la inferencia es efectuar una generalizaci\'on de los resultados de la
muestra de la poblaci\'on. La tarea que nos ocupa ahora es conocer las distribuciones de
la probabilidad de ciertas funciones de la muestra, es decir, variables aleatorias
asociadas al muestreo o estad\'isticos muestrales. \'estos ser\'an \'utiles para hacer
inferencia respecto a los par\'ametros desconocidos de una poblaci\'on. Por ello se habla de
distribuciones muestrales, ya que est\'an basados en el comportamiento de las
muestras.

El primer objetivo es conocer el concepto de distribuci\'on muestral de un
estad\'istico; su comportamiento probabil\'istico depender\'a del que tenga la variable $X$ y
del tama\~no de las muestras.


%\end{frame}
%---------------------------------------------------------
\section{2. Pruebas de Hip\'otesis}
%---------------------------------------------------------
\subsection{2.1 Tipos de errores}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}

\begin{itemize}
\item Una hip\'otesis estad\'istica es una afirmaci\'on  acerca de la distribuci\'on de probabilidad de una variable aleatoria, a menudo involucran uno o m\'as par\'ametros de la distribuci\'on.

\item Las hip\'otesis son afirmaciones respecto a la poblaci\'on o distribuci\'on bajo estudio, no en torno a la muestra.

\item La mayor\'ia de las veces, la prueba de hip\'otesis consiste en determinar si la situaci \'on experimental ha cambiado

\item el inter\'es principal es decidir sobre la veracidad o falsedad de una hip\'otesis, a este procedimiento se le llama \textit{prueba de hip\'otesis}.

\item Si la informaci\'on es consistente con la hip\'otesis, se concluye que esta es verdadera, de lo contrario que con base en la informaci\'on, es falsa.

\end{itemize}


%\end{frame}


%\begin{frame}\frametitle{Introducci\'on}
Una prueba de hip\'otesis est\'a formada por cinco partes
\begin{itemize}
\item La hip\'otesis nula, denotada por $H_{0}$.
\item La hip\'otesis alterativa, denorada por $H_{1}$.
\item El estad\'sitico de prueba y su valor $p$.
\item La regi\'on de rechazo.
\item La conclusi\'on.

\end{itemize}



%\end{frame}


%\begin{frame}\frametitle{Introducci\'on}
\begin{Def}
Las dos hip\'otesis en competencias son la \textbf{hip\'otesis alternativa $H_{1}$}, usualmente la que se desea apoyar, y la \textbf{hip\'otesis nula $H_{0}$}, opuesta a $H_{1}$.
\end{Def}


En general, es m\'as f\'acil presentar evidencia de que $H_{1}$ es cierta, que demostrar 	que $H_{0}$ es falsa, es por eso que por lo regular se comienza suponiendo que $H_{0}$ es cierta, luego se utilizan los datos de la muestra para decidir si existe evidencia a favor de $H_{1}$, m\'as que a favor de $H_{0}$, as\'i se tienen dos conclusiones:
\begin{itemize}
\item Rechazar $H_{0}$ y concluir que $H_{1}$ es verdadera.
\item Aceptar, no rechazar, $H_{0}$ como verdadera.

\end{itemize}


%\end{frame}


%\begin{frame}\frametitle{Introducci\'on}
\begin{Ejem}
Se desea demostrar que el salario promedio  por hora en cierto lugar es distinto de $19$usd, que es el promedio nacional. Entonces $H_{1}:\mu\neq19$, y $H_{0}:\mu=19$.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de dos colas}.


\begin{Ejem}
Un determinado proceso produce un promedio de $5\%$ de piezas defectuosas. Se est\'a interesado en demostrar que un simple ajuste en una m\'aquina reducir\'a $p$, la proporci\'on de piezas defectuosas producidas en este proceso. Entonces se tiene $H_{0}:p<0.3$ y $H_{1}:p=0.03$. Si se puede rechazar $H_{0}$, se concluye que el proceso ajustado produce menos del $5\%$ de piezas defectuosas.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de una cola}.
%\end{frame}


%\begin{frame}\frametitle{Introducci\'on}
La decisi\'on de rechazar o aceptar la hip\'otesis nula est\'a basada en la informaci\'on contenida en una muestra proveniente de la poblaci\'on de inter\'es. Esta informaci\'on tiene estas formas

\begin{itemize}
\item \textbf{Estad\'sitico de prueba:} un s\'olo n\'umero calculado a partir de la muestra.

\item \textbf{$p$-value:} probabilidad calculada a partir del estad\'stico de prueba.

\end{itemize}
%\end{frame}




%\begin{frame}\frametitle{Introducci\'on}
\begin{Def}
El $p$-value es la probabilidad de observar un estad\'istico de prueba tanto o m\'as alejado del valor obervado, si en realidad $H_{0}$ es verdadera.\medskip
Valores grandes del estad\'stica de prueba  y valores peque\~nos de $p$ significan que se ha observado un evento muy poco probable, si $H_{0}$ en realidad es verdadera.
\end{Def}

Todo el conjunto de valores que puede tomar el estad\'istico de prueba se divide en dos regiones. Un conjunto, formado de valores que apoyan la hip\'otesis alternativa y llevan a rechazar $H_{0}$, se denomina \textbf{regi\'on de rechazo}. El otro, conformado por los valores que sustentatn la hip\'otesis nula, se le denomina \textbf{regi\'on de aceptaci\'on}.\medskip



%\end{frame}


%\begin{frame}\frametitle{Introducci\'on}
Cuando la regi\'on de rechazo est\'a en la cola izquierda de la distribuci\'on, la  prueba se denomina \textbf{prueba lateral izquierda}. Una prueba con regi\'on de rechazo en la cola derecha se le llama \textbf{prueba lateral derecha}.\medskip

Si el estad\'stico de prueba cae en la regi\'on de rechazo, entonces se rechaza $H_{0}$. Si el estad\'stico de prueba cae en la regi\'on de aceptaci\'on, entonces la hip\'otesis nula se acepta o la prueba se juzga como no concluyente.\medskip

Dependiendo del nivel de confianza que se desea agregar a las conclusiones de la prueba, y el \textbf{nivel de significancia $\alpha$}, el riesgo que est\'a dispuesto a correr si se toma una decisi\'on incorrecta.
%\end{frame}

%\begin{frame}\frametitle{Introducci\'on}
\begin{Def}
Un \textbf{error de tipo I} para una prueba estad\'istica es el error que se tiene al rechazar la hip\'otesis nula cuando es verdadera. El \textbf{nivel de significancia} para una prueba estad\'istica de hip\'otesis es
\begin{eqnarray*}
\alpha&=&P\left\{\textrm{error tipo I}\right\}=P\left\{\textrm{rechazar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es verdadera}\right\}
\end{eqnarray*}

\end{Def}
Este valor $\alpha$ representa el valor m\'aximo de riesgo tolerable de rechazar incorrectamente $H_{0}$. Una vez establecido el nivel de significancia, la regi\'on de rechazo se define para poder determinar si se rechaza $H_{0}$ con un cierto nivel de confianza.

%\end{frame}
\section{2.2 Muestras grandes: una media poblacional}
\subsection{2.2.1 C\'alculo de valor $p$}

%\begin{frame}\frametitle{C\'alculo del valor de $p$}
\begin{Def}
El \textbf{valor de $p$} (\textbf{$p$-value}) o nivel de significancia observado de un estad\'istico de prueba es el valor m\'as peque\~ no de $\alpha$ para el cual $H_{0}$ se puede rechazar. El riesgo de cometer un error tipo $I$, si $H_{0}$ es rechazada con base en la informaci\'on que proporciona la muestra.
\end{Def}

\begin{Note}
Valores peque\~ nos de $p$ indican 	que el valor observado del estad\'stico de prueba se encuentra alejado del valor hipot\'etico de $\mu$, es decir se tiene evidencia de que $H_{0}$ es falsa y por tanto debe de rechazarse.
\end{Note}



%\end{frame}

%\begin{frame}\frametitle{C\'alculo del valor de $p$}
\begin{Note}
Valores grandes de $p$ indican que el estad\'istico de prueba observado no est\'a alejado de la medi hipot\'etica y no apoya el rechazo de $H_{0}$.
\end{Note}

\begin{Def}
Si el valor de $p$ es menor o igual que el nivel de significancia $\alpha$, determinado previamente, entonces $H_{0}$ es rechazada y se puede concluir que los resultados son estad\'isticamente significativos con un nivel de confianza del $100\left(1-\alpha\right)\%$.
\end{Def}
Es usual utilizar la siguiente clasificaci\'on de resultados



%\end{frame}

%\begin{frame}\frametitle{C\'alculo del valor de $p$}
\begin{tabular}{|c||c|l|}\hline
$p$& $H_{0}$&Significativa\\\hline\hline
$p<0.01$&Rechazar &Altamente\\\hline
$0.01\leq p<0.05$ & Rechazar&Estad\'isticamente\\\hline
$0.05\leq p <0.1$ & No rechazar & Tendencia estad\'istica\\\hline
$0.01\leq p$ & No rechazar & No son estad\'isticamente\\\hline
\end{tabular}

\begin{Note}
Para determinar el valor de $p$, encontrar el \'area en la cola despu\'es del estad\'istico de prueba. Si la prueba es de una cola, este es el valor de $p$. Si es de dos colas, \'este valor encontrado es la mitad del valor de $p$. Rechazar $H_{0}$ cuando el valor de $p<\alpha$.
\end{Note}

%\end{frame}
%\begin{frame}\frametitle{C\'alculo del valor de $p$}

Hay dos tipos de errores al realizar una prueba de hip\'otesis
\begin{center}
\begin{tabular}{c|cc}
& $H_{0}$ es Verdadera & $H_{0}$ es Falsa\\\hline\hline
Rechazar $H_{0}$ & Error tipo I & $\surd$\\
Aceptar $H_{0}$ & $\surd$ & Error tipo II
\end{tabular}
\end{center}
\begin{Def}
La probabilidad de cometer el error tipo II se define por $\beta$ donde
\begin{eqnarray*}
\beta&=&P\left\{\textrm{error tipo II}\right\}=P\left\{\textrm{Aceptar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{Aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}
\end{eqnarray*}
\end{Def}

%\end{frame}

%\begin{frame}\frametitle{Potencia de la prueba}


\begin{Note}
Cuando $H_{0}$ es falsa y $H_{1}$ es verdadera, no siempre es posible especificar un valor exacto de $\mu$, sino m\'as bien un rango de posibles valores.\medskip
En lugar de arriesgarse a tomar una decisi\'on incorrecta, es mejor conlcuir que \textit{no hay evidencia suficiente para rechazar $H_{0}$}, es decir en lugar de aceptar $H_{0}$, \textit{no rechazar $H_{0}$}.

\end{Note}
%\end{frame}


%\begin{frame}\frametitle{Potencia de la prueba}
La bondad de una prueba estad\'istica se mide por el tama\~ no de $\alpha$ y $\beta$, ambas deben de ser peque\~ nas. Una manera muy efectiva de medir la potencia de la prueba es calculando el complemento del error tipo $II$:
\begin{eqnarray*}
1-\beta&= &P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{1}\textrm{ es verdadera}\right\}
\end{eqnarray*}
\begin{Def}
La \textbf{potencia de la prueba}, $1-\beta$, mide la capacidad de que la prueba funciona como se necesita.
\end{Def}

%\end{frame}

%\begin{frame}\frametitle{Ejemplo ilustrativo}
\begin{Ejem}
La producci\'on diariade una planta qu\'imica local ha promediado 880 toneladas en los \'ultimos a\~nos. A la gerente de control de calidad le gustar\'ia saber si este promedio ha cambiado en meses recientes. Ella selecciona al azar 50 d\'ias de la base de datos computarizada y calcula el promedio y la desviaci\'on est\'andar de las $n=50$  producciones como $\overline{x}=871$ toneladas y $s=21$ toneladas, respectivamente. Pruebe la hip\'otesis  apropiada usando $\alpha=0.05$.

\end{Ejem}

%\end{frame}

%\begin{frame}\frametitle{Ejemplo ilustrativo}
\begin{Sol}
La hip\'otesis nula apropiada es:
\begin{eqnarray*}
H_{0}&:& \mu=880\\
&&\textrm{ y la hip\'otesis alternativa }H_{1}\textrm{ es }\\
H_{1}&:& \mu\neq880
\end{eqnarray*}
el estimador puntual para $\mu$ es $\overline{x}$, entonces el estad\'istico de prueba es\medskip
\begin{eqnarray*}
z&=&\frac{\overline{x}-\mu_{0}}{s/\sqrt{n}}\\
&=&\frac{871-880}{21/\sqrt{50}}=-3.03
\end{eqnarray*}
\end{Sol}


%\end{frame}

%\begin{frame}\frametitle{Ejemplo ilustrativo}
\begin{Sol}
Para esta prueba de  dos colas, hay que determinar los dos valores de $z_{\alpha/2}$, es decir,  $z_{\alpha/2}=\pm1.96$, como $z>z_{\alpha/2}$, $z$  cae en la zona de rechazo, por lo tanto  la gerente puede rechazar la hip\'otesis nula y concluir que el promedio efectivamente ha cambiado.\medskip
La probabilidad de rechazar $H_{0}$ cuando esta es verdadera es de  $0.05$.


Recordemos que el valor observado del estad\'istico de prueba es $z=-3.03$, la regi\'on de rechazo m\'as peque\~na que puede usarse y todav\'ia seguir rechazando $H_{0}$ es $|z|>3.03$, \\
entonces $p=2(0.012)=0.0024$, que a su vez es menor que el nivel de significancia $\alpha$ asignado inicialmente, y adem\'as los resultados son  \textbf{altamente significativos}.


\end{Sol}
%\end{frame}

%\begin{frame}\frametitle{Ejemplo ilustrativo}
Finalmente determinemos la potencia de la prueba cuando $\mu$ en realidad es igual a $870$ toneladas.

Recordar que la regi\'on de aceptaci\'on est\'a entre $-1.96$ y $1.96$, para $\mu=880$, equivalentemente $$874.18<\overline{x}<885.82$$
$\beta$ es la probabilidad de aceptar $H_{0}$ cuando $\mu=870$, calculemos los valores de $z$ correspondientes a $874.18$ y $885.82$ \medskip
Entonces
\begin{eqnarray*}
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{874.18-870}{21/\sqrt{50}}=1.41\\
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{885.82-870}{21/\sqrt{50}}=5.33
\end{eqnarray*}

%\end{frame}

%\begin{frame}\frametitle{Ejemplo ilustrativo}
por lo tanto
\begin{eqnarray*}
\beta&=&P\left\{\textrm{aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{874.18<\mu<885.82\textrm{ cuando }\mu=870\right\}\\
&=&P\left\{1.41<z<5.33\right\}=P\left\{1.41<z\right\}\\
&=&1-0.9207=0.0793
\end{eqnarray*}
entonces, la potencia de la prueba es
$$1-\beta=1-0.0793=0.9207$$ que es la probabilidad de rechazar correctamente $H_{0}$ cuando $H_{0}$ es falsa.
%\end{frame}

%\begin{frame}\frametitle{Ejemplo ilustrativo}
Determinar la potencia de la prueba para distintos valores de $H_{1}$ y graficarlos, \textit{curva de potencia}
\begin{center}
\begin{tabular}{c||c}
$H_{1}$ & $\left(1-\beta\right)$ \\\hline 
\hline 
865 &  \\ \hline 
870 &  \\ \hline 
872 &  \\ \hline 
875 &  \\ \hline 
877 &  \\ \hline 
880 &  \\ \hline 
883 &  \\ \hline 
885 &  \\ \hline 
888 &  \\ \hline 
890 &  \\ \hline 
895 &  \\ \hline 
\end{tabular} 

\end{center}
%\end{frame}


%\begin{frame}\frametitle{List de Ejercicios}
\begin{enumerate}
\item Encontrar las regiones de rechazo para el estad\'istico $z$, para una prueba de
\begin{itemize}
\item[a) ]  dos colas para $\alpha=0.01,0.05,0.1$
\item[b) ]  una cola superior para $\alpha=0.01,0.05,0.1$
\item[c) ] una cola inferior para $\alpha=0.01,0.05,0.1$

\end{itemize}


\item Suponga que el valor del estad\'istico de prueba es 
\begin{itemize}
\item[a) ]$z=-2.41$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[b) ] $z=2.16$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[c) ] $z=1.15$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[d) ] $z=-2.78$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[e) ] $z=-1.81$, sacar las conclusiones correspondientes para los incisos anteriores.

\end{itemize}
\end{enumerate}
%\end{frame}


%\begin{frame}\frametitle{List de Ejercicios}
\begin{itemize}
\item[3. ] Encuentre el valor de $p$ para las pruebas de hip\'otesis correspondientes a los valores de $z$ del ejercicio anterior.

\item[4. ] Para las pruebas dadas en el ejercicio 2, utilice el valor de $p$, determinado en el ejercicio 3,  para determinar la significancia de los resultados.


\end{itemize}

%\end{frame}

%\begin{frame}\frametitle{Lista de Ejercicios}
\begin{itemize}
\item[5. ] Una muestra aleatoria de $n=45$ observaciones de una poblaci\'on con media $\overline{x}=2.4$, y desviaci\'on est\'andar $s=0.29$. Suponga que el objetivo es demostrar que la media poblacional $\mu$ excede $2.3$.
\begin{itemize}
\item[a) ] Defina la hip\'otesis nula y alternativa para la prueba.
\item[b) ] Determine la regi\'on de rechazo para un nivel de significancia de: $\alpha=0.1,0.05,0.01$.
\item[c) ] Determine el error est\'andar de la media muestral.
\item[d) ] Calcule el valor de $p$ para los estad\'isticos de prueba definidos en los incisos anteriores.
\item[e) ] Utilice el valor de $p$ pra sacar una conclusi\'on al nivel de significancia $\alpha$.
\item[f) ] Determine el valor de $\beta$ cuando $\mu=2.5$
\item[g) ] Graficar la curva de potencia para la prueba.

\end{itemize}
\end{itemize}

%\end{frame}
\subsection{2.2.2 Prueba de hip\'otesis para la diferencia entre dos medias poblacionales}

%\begin{frame}\frametitle{Diferencia entre dos medias poblacionales}
El estad\'istico que resume la informaci\'on muestral respecto a la diferencia en medias poblacionales $\left(\mu_{1}-\mu_{2}\right)$ es la diferencia de las medias muestrales $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, por tanto al probar la difencia entre las medias muestrales se verifica que la diferencia real entre las medias poblacionales difiere de un valor especificado, $\left(\mu_{1}-\mu_{2}\right)=D_{0}$, se puede usar el error est\'andar de $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, es decir
$$\sqrt{\frac{\sigma^{2}_{1}}{n_{1}}+\frac{\sigma^{2}_{2}}{n_{2}}}$$
cuyo estimador est\'a dado por
$$SE=\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}$$
%\end{frame}


%\begin{frame}\frametitle{Diferencia entre dos medias poblacionales}
El procedimiento para muestras grandes es:
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}

\end{itemize}

%\end{frame}
%\begin{frame}\frametitle{Diferencia entre dos medias poblacionales}
\begin{itemize}
\item[3) ] Estad\'istico de prueba:
$$z=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}$$
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}


\end{itemize}

%\end{frame}
%\begin{frame}\frametitle{Diferencia entre dos medias poblacionales:Ejemplo}
\begin{Ejem}
Para determinar si ser propietario de un autom\'ovil afecta el rendimiento acad\'emico de un estudiante, se tomaron dos muestras aleatorias de 100 estudiantes varones. El promedio de calificaciones para los $n_{1}=100$ no propietarios de un auto tuvieron un promedio y varianza de $\overline{x}_{1}=2.7$ y $s_{1}^{2}=0.36$, respectivamente, mientras que para para la segunda muestra con $n_{2}=100$ propietarios de un auto, se tiene $\overline{x}_{2}=2.54$ y $s_{2}^{2}=0.4$. Los datos presentan suficiente evidencia para indicar una diferencia en la media en el rendimiento acad\'emico entre propietarios y no propietarios de un autom\'ovil? Hacer pruebas para $\alpha=0.01,0.05$ y $\alpha=0.1$.
\end{Ejem}
%\end{frame}

%\begin{frame}\frametitle{None}
\begin{Sol}
\begin{itemize}
\item Soluci\'on utilizando la t\'ecnica de regiones de rechazo:\medskip
realizando las operaciones
$z=1.84$, determinar si excede los valores de $z_{\alpha/2}$.
\item Soluci\'on utilizando el $p$-value:\medskip
Calcular el valor de $p$, la probabilidad de que $z$ sea mayor que $z=1.84$ o menor que $z=-1.84$, se tiene que $p=0.0658$. Concluir.
\end{itemize}
\end{Sol}
%\end{frame}

%\begin{frame}\frametitle{Pruebas de hip\'otesis e Intervalos de Confianza}
\begin{itemize}
\item Si el intervalo de confianza que se construye contiene el valor del par\'ametro especificado por $H_{0}$, entonces ese valor es uno de los posibles valores del par\'ametro y $H_{0}$ no debe ser rechazada.

\item Si el valor hipot\'etico se encuentra fuera de los l\'imites de confianza, la hip\'otesis nula es rechazada al nivel de significancia $\alpha$.
\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Lista de Ejercicios}
\begin{enumerate}
\item Del libro Mendenhall resolver los ejercicios 9.18, 9.19 y 9.20(\href{https://cu.uacm.edu.mx/nextcloud/index.php/f/202873}{Mendenhall}).

\item Del libro \href{https://cu.uacm.edu.mx/nextcloud/index.php/f/202873}{Mendenhall} resolver los ejercicios: 9.23, 9.26 y 9.28.
\end{enumerate}

%\end{frame}

\subsection{2.2.3 Prueba de Hip\'otesis para una Proporci\'on Binomial}
%\begin{frame}\frametitle{Una proporci\'on Binomial}
Para una muestra aleatoria de $n$ intentos id\'enticos, de una poblaci\'on binomial, la proporci\'on muesrtal $\hat{p}$ tiene una distribuci\'on aproximadamente normal cuando $n$ es grande, con media $p$ y error est\'andar
$$SE=\sqrt{\frac{pq}{n}}.$$
La prueba de hip\'otesis de la forma
\begin{eqnarray*}
H_{0}&:&p=p_{0}\\
H_{1}&:&p>p_{0}\textrm{, o }p<p_{0}\textrm{ o }p\neq p_{0}
\end{eqnarray*}
El estad\'istico de prueba se construye con el mejor estimador de la proporci\'on verdadera, $\hat{p}$, con el estad\'istico de prueba $z$, que se distribuye normal est\'andar.
%\end{frame}
%\begin{frame}\frametitle{Una proporci\'on Binomial}
El procedimiento es
\begin{itemize}
\item[1) ] Hip\'otesis nula: $H_{0}:p=p_{0}$
\item[2) ] Hip\'otesis alternativa
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:p>p_{0}$ & $p\neq p_{0}$\\
$H_{1}:p<p_{0}$ & \\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
z=\frac{\hat{p}-p_{0}}{\sqrt{\frac{pq}{n}}},\hat{p}=\frac{x}{n}
\end{eqnarray*}
donde $x$ es el n\'umero de \'exitos en $n$ intentos binomiales.

\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Una proporci\'on Binomial}
\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Una proporci\'on Binomial}
\begin{Ejem}
A cualquier edad, alrededor del $20\%$ de los adultos de cierto pa\'is realiza actividades de acondicionamiento f\'isico al menos dos veces por semana. En una encuesta local de $n=100$ adultos de m\'as de $40$ a\ ~nos, un total de 15 personas indicaron que realizaron actividad f\'isica al menos dos veces por semana. Estos datos indican que el porcentaje de participaci\'on para adultos de m\'as de 40 a\ ~nos de edad es  considerablemente menor a la cifra del $20\%$? Calcule el valor de $p$ y \'uselo para sacar las conclusiones apropiadas.
\end{Ejem}
%\end{frame}

%\begin{frame}\frametitle{Una proporci\'on Binomial}
\begin{enumerate}
\item Resolver los ejercicios: 9.30, 9.32, 9.33, 9.35 y 9.39.
\end{enumerate}

%\end{frame}


\subsection{2.2.4 Prueba de Hip\'otesis diferencia entre dos Proporciones Binomiales}
%\begin{frame}\frametitle{Dos proporciones binomiales}
\begin{Note}
Cuando se tienen dos muestras aleatorias independientes de dos poblaciones binomiales, el objetivo del experimento puede ser la diferencia $\left(p_{1}-p_{2}\right)$ en las proporciones de individuos u objetos que poseen una caracter\'istica especifica en las dos poblaciones. En este caso se pueden utilizar los estimadores de las dos proporciones $\left(\hat{p}_{1}-\hat{p}_{2}\right)$ con error est\'andar dado por
$$SE=\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}$$
considerando el estad\'istico $z$ con un nivel de significancia $\left(1-\alpha\right)100\%$

\end{Note}

%\end{frame}

%\begin{frame}\frametitle{Dos proporciones binomiales}
\begin{Note}
La hip\'otesis nula a probarse es de la forma
\begin{itemize}
\item[$H_{0}$: ] $p_{1}=p_{2}$ o equivalentemente $\left(p_{1}-p_{2}\right)=0$, contra una hip\'otesis alternativa $H_{1}$ de una o dos colas.
\end{itemize}
\end{Note}

\begin{Note}
Para estimar el error est\'andar del estad\'istico $z$, se debe de utilizar el hecho de que suponiendo que $H_{0}$ es verdadera, las dos proporciones son iguales a alg\'un valor com\'un, $p$. Para obtener el mejor estimador de $p$ es
$$p=\frac{\textrm{n\'umero total de \'exitos}}{\textrm{N\'umero total de pruebas}}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$$
\end{Note}

%\end{frame}



%\begin{frame}\frametitle{Prueba de Hip\'otesis para $\left(p_{1}-p_{2}\right)$}
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\left(p_{1}-p_{2}\right)=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(p_{1}-p_{2}\right)>0$ & $H_{1}:\left(p_{1}-p_{2}\right)\neq 0$\\ 
$H_{1}:\left(p_{1}-p_{2}\right)<0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}}=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{pq}{n_{1}}+\frac{pq}{n_{2}}}}
\end{eqnarray*}
donde $\hat{p_{1}}=x_{1}/n_{1}$ y $\hat{p_{2}}=x_{2}/n_{2}$ , dado que el valor com\'un para $p_{1}$ y $p_{2}$ es $p$, entonces $\hat{p}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$ y por tanto el estad\'istico de prueba es
\end{itemize}
%\end{frame}


%\begin{frame}\frametitle{Prueba de Hip\'otesis para $\left(p_{1}-p_{2}\right)$}
\begin{eqnarray*}
z=\frac{\hat{p}_{1}-\hat{p}_{2}}{\sqrt{\hat{p}\hat{q}}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}
\end{eqnarray*}
\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{\alpha}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}

\end{itemize}

%\end{frame}


%\begin{frame}\frametitle{Dos proporciones binomiales: ejercicio}
\begin{Ejem}
Los registros de un hospital, indican que 52 hombres de una muestra de 1000 contra 23 mujeres de una muestra de 1000 fueron ingresados por enfermedad del coraz\'on. Estos datos presentan suficiente evidencia para indicar un porcentaje m\'as alto de enfermedades del coraz\'on entre hombres ingresados al hospital?, utilizar distintos niveles de confianza de $\alpha$.

\end{Ejem}
\begin{enumerate}
\item Resolver los ejercicios 9.42

\item Resolver los ejercicios: 9.45, 9.48, 9.50
\end{enumerate}

%\end{frame}


\section{2.3 Muestras Peque\~nas}

\subsection{2.3.1 Una media poblacional}
%\begin{frame}\frametitle{Una media poblacional}
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu=\mu_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu>\mu_{0}$ & $H_{1}:\mu\neq \mu_{0}$\\ 
$H_{1}:\mu<\mu0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
t=\frac{\overline{x}-\mu_{0}}{\sqrt{\frac{s^{2}}{n}}}
\end{eqnarray*}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Ejercicio}
\begin{Ejem}
Las etiquetas en latas de un gal'on de pintura por lo general indican el tiempo de secado y el \'area puede cubrir una capa. Casi todas las marcas de pintura indican que, en una capa, un gal\'on cubrir\'a entre 250 y 500 pies cuadrados, dependiento de la textura de la superficie a pintarse, un fabricante, sin embargo afirma que un gal\'on de su pintura cubrir\'a 400 pies cuadrados de \'area superficial. Para probar su afirmaci\'on, una muestra aleatoria de 10 latas de un gal\'on de pintura blanca se emple\'o para pintar 10 \'areas id\'enticas usando la misma clase de equipo. Las \'areas reales en pies cuadrados cubiertas por estos 10 galones de pintura se dan a continuac\'on:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
310 & 311 & 412 & 368 & 447 \\ 
\hline 
376 & 303 &410 &365 & 350 \\ 
\hline 
\end{tabular} 
\end{center}
\end{Ejem}
%\end{frame}

%\begin{frame}\frametitle{Ejercicio}
\begin{Ejem}
Los datos presentan suficiente evidencia para indicar que el promedio de la cobertura difiere de 400 pies cuadrados? encuentre el valor de $p$ para la prueba y \'uselo para evaluar la significancia de los resultados.
\end{Ejem}
\begin{enumerate}
\item Resolver los ejercicios: 10.2, 10.3,10.5, 10.7, 10.9, 10.13 y 10.16
\end{enumerate}
%\end{frame}



\subsection{2.3.2 Diferencia entre dos medias poblacionales: M.A.I.}
%\begin{frame}\frametitle{Diferencia entre dos medias: M.A.I.}
\begin{Note}
Cuando los tama\ ~nos de muestra son peque\ ~nos, no se puede asegurar que las medias muestrales sean normales, pero si las poblaciones originales son normales, entonces la distribuci\'on muestral de la diferencia de las medias muestales, $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, ser\'a normal con media $\left(\mu_{1}-\mu_{2}\right)$ y error est\'andar $$ES=\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}$$

\end{Note}
%\end{frame}

%\begin{frame}\frametitle{Diferencia entre dos medias: M.A.I.}
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}

\item[3) ] Estad\'istico de prueba:
$$t=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}$$
\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Diferencia entre dos medias: M.A.I.}

donde $$s^{2}=\frac{\left(n_{1}-1\right)s_{1}^{2}+\left(n_{2}-1\right)s_{2}^{2}}{n_{1}+n_{2}-2}$$
\begin{itemize}

\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.


\end{itemize}


%\end{frame}

\subsection{2.3.3 Diferencia entre dos medias poblacionales: Diferencias Pareadas}
%\begin{frame}\frametitle{Diferencias pareadas}
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu_{d}=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:\mu_{d}$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu_{d}>0$ & $H_{1}:\mu_{d}\neq 0$\\ 
$H_{1}:\mu_{d}<0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
t=\frac{\overline{d}}{\sqrt{\frac{s_{d}^{2}}{n}}}
\end{eqnarray*}
donde $n$ es el n\'umero de diferencias pareadas, $\overline{d}$ es la media de las diferencias muestrales, y $s_{d}$ es la desviaci\'on est\'andar de las diferencias muestrales.



\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Diferencias pareadas}
\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}

Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.

\end{itemize}
%\end{frame}

\subsection{2.3.4 Inferencias con respecto a la Varianza Poblacional}
%\begin{frame}\frametitle{Varianza poblacional}
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\sigma^{2}=\sigma^{2}_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\sigma^{2}>\sigma^{2}_{0}$ & $H_{1}:\sigma^{2}\neq \sigma^{2}_{0}$\\ 
$H_{1}:\sigma^{2}<\sigma^{2}_{0}$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
\chi^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}_{0}}
\end{eqnarray*}

\end{itemize}
%\end{frame}

%\begin{frame}\frametitle{Varianza Poblacional}
\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$\chi^{2}>\chi^{2}_{\alpha}$ & \\
$\chi^{2}<\chi^{2}_{\left(1-\alpha\right)}$ cuando $H_{1}:\chi^{2}<\chi^{2}_{0}$&$\chi^{2}>\chi^{2}_{\alpha/2}$ o $\chi^{2}<\chi^{2}_{\left(1-\alpha/2\right)}$\\
 cuando $p<\alpha$&\\
\end{tabular}

Los valores cr\'iticos de $\chi^{2}$,est\'an basados en $\left(n_{1}+\right)$ grados de libertad.

\end{itemize}

%\end{frame}
\subsection{2.3.5 Comparaci\'on de dos varianzas poblacionales}
%\begin{frame}\frametitle{Igualdad de dos varianzas}
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)>D_{0}$ & $H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)<D_{0}$&\\
\end{tabular}

\end{itemize}

%\end{frame}
%\begin{frame}\frametitle{Diferencia entre dos medias poblacionales}
\begin{itemize}
\item[3) ] Estad\'istico de prueba:
$$F=\frac{s_{1}^{2}}{s_{2}^{2}}$$
donde $s_{1}^{2}$ es la varianza muestral m\'as grande.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$F>F_{\alpha}$ & $F>F_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}


\end{itemize}

%\end{frame}

%---------------------------------------------------------
\section{3. An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------
%\begin{frame}\frametitle{Descripci\'on}
\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\end{itemize}

\end{Note}
%\end{frame}

\subsection{3.1 Regresi\'on Lineal Simple (RLS)}
%\begin{frame}\frametitle{RLS}
\begin{itemize}

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

%\end{frame}

\subsection{3.2 M\'etodo de M\'inimos Cuadrados}
%\begin{frame}\frametitle{M\'inimos Cuadrados}
Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}
donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$, estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.
%\end{frame}


%\begin{frame}\frametitle{M\'inimos Cuadrados}
Entonces la ecuaci\'on \ref{Modelo.Regresion} se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}
%\end{frame}



%\begin{frame}\frametitle{M\'inimos Cuadrados}
Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$, igualar a cero y resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene
%\end{frame}
%\begin{frame}\frametitle{M\'inimos Cuadrados}
\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
%\end{frame}

%\begin{frame}\frametitle{M\'inimos Cuadrados}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ecs.Estimadores.Regresion}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}
%\end{frame}
%\begin{frame}\frametitle{M\'inimos Cuadrados}
Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}
%\end{frame}
\subsection{3.3 Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}
%\begin{frame}\frametitle{Propiedades de los estimadores}
\begin{Note}
\begin{itemize}
\item Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo.

\item Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.
\end{itemize}
\end{Note}
A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)
\end{eqnarray*}
%\end{frame}


%\begin{frame}\frametitle{Propiedades de los estimadores}
\begin{eqnarray*}
&=&\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)\right.\\
&+&\left.E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]=\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}
\end{eqnarray*}
por lo tanto 
\begin{equation}\label{Esperanza.Beta.1}
E\left(\hat{\beta}_{1}\right)=\beta_{1}
\end{equation}
%\end{frame}

%\begin{frame}\frametitle{Propiedades de los estimadores}
\begin{Note}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado.
\end{Note}
Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}\\
&=&\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}}
\end{eqnarray*}

%\end{frame}

%\begin{frame}\frametitle{Propiedades de los estimadores}
por lo tanto
\begin{equation}\label{Varianza.Beta.1}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}
\end{equation}
\begin{Prop}
\begin{eqnarray*}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray*}
\end{Prop}
%\end{frame}

%\begin{frame}\frametitle{Propiedades de los estimadores}
Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray}
%\end{frame}

%\begin{frame}\frametitle{Propiedades de los estimadores}
sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=MC_{E}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray*}
%\end{frame}

%\end{document}
\subsection{3.4 Prueba de Hip\'otesis en RLS}
%\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{itemize}
\item Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza.

\item Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$.

\end{itemize}

%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:
\begin{centering}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,

\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.

\end{itemize}
donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. 
\end{centering}
De las ecuaciones (\ref{Ecs.Estimadores.Regresion}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1}) y (\ref{Varianza.Beta.1}).
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

%\end{frame}


%\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}=\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}

%\end{frame}


%\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}
%\end{frame}


%\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
\end{eqnarray*}
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{eqnarray*}
&=&\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados}) se puede reescribir como 
\begin{equation}\label{Suma.Total.Cuadrados.Dos}
S_{yy}=SC_{R}+SC_{E}
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right)\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}													
\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx}
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}
Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$.
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}													
El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente\medskip

\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}													
La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1}), con $\hat{\beta}_{1,0}=0$, es decir
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.

%\end{frame}

%\end{document}
\subsection{Estimaci\'on de Intervalos en RLS}
%\begin{frame}\frametitle{Intervalos de Confianza}
\begin{itemize}
\item Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros.

\item El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on.

\end{itemize}


%\end{frame}

%\begin{frame}\frametitle{Intervalos de Confianza}
Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por
%\end{frame}
%\begin{frame}\frametitle{Intervalos de Confianza}
\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\small{
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}}
%\end{frame}
\subsection{Predicci\'on}
%\begin{frame}\frametitle{Predicci\'on}
Supongamos que se tiene un valor $x_{0}$ de inter\'es, entonces la estimaci\'on puntual de este nuevo valor
\begin{equation}
\hat{y}_{0}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}
\end{equation}
\begin{Note}
Esta nueva observaci\'on es independiente de las utilizadas para obtener el modelo de regresi\'on, por tanto, el intervalo en torno a la recta de regresi\'on es inapropiado, puesto que se basa \'unicamente en los datos empleados para ajustar el modelo de regresi\'on.\\

El intervalo de confianza en torno a la recta de regresi\'on se refiere a la respuesta media verdadera $x=x_{0}$, no a observaciones futuras.
\end{Note}

%\end{frame}


%\begin{frame}\frametitle{Predicci\'on}
Sea $y_{0}$ la observaci\'on futura en $x=x_{0}$, y sea $\hat{y}_{0}$ dada en la ecuaci\'on anterior, el estimador de $y_{0}$. Si se define la variable aleatoria $$w=y_{0}-\hat{y}_{0},$$ esta se distribuye normalmente con media cero y varianza $$V\left(w\right)=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]$$
dado que $y_{0}$ es independiente de $\hat{y}_{0}$, por lo tanto el intervalo de predicci\'on al nivel $\alpha$ para futuras observaciones $x_{0}$ es

%\end{frame}

%\begin{frame}\frametitle{Predicci\'on}
\begin{eqnarray*}
\hat{y}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}\leq y_{0}\\
\leq \hat{y}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}.
\end{eqnarray*}
%\end{frame}


%\subsection{Prueba de falta de ajuste}
%%\begin{frame}\frametitle{Falta de ajuste}
%Es com\'un encontrar que el modelo ajustado no satisface totalmente el modelo necesario para los datos, en este caso es preciso saber qu\'e tan bueno es el modelo propuesto. Para esto se propone la siguiente prueba de hip\'otesis:
%\begin{itemize}
%\item[$H_{0}:$ ]El modelo propuesto se ajusta adecuademente a los datos.
%\item[$H_{1}:$ ]El modelo NO se ajusta a los datos.
%\end{itemize}
%La prueba implica dividir la suma de cuadrados del eror o del residuo en las siguientes dos componentes:
%\begin{eqnarray*}
%SC_{E}=SC_{EP}+SC_{FDA}
%\end{eqnarray*}

%%\end{frame}

%%\begin{frame}\frametitle{Falta de ajuste}
%donde $SC_{EP}$ es la suma de cuadrados atribuibles al error puro, y $SC_{FDA}$ es la suma de cuadrados atribuible a la falta de ajuste del modelo.
%%\end{frame}

%%\begin{frame}\frametitle{Falta de ajuste}
%%\end{frame}


\subsection{Coeficiente de Determinaci\'on}
%\begin{frame}\frametitle{Coeficiente de Determinaci\'on}
La cantidad
\begin{equation}
R^{2}=\frac{SC_{R}}{S_{yy}}=1-\frac{SC_{E}}{S_{yy}}
\end{equation}
se denomina coeficiente de determinaci\'on y se utiliza para saber si el modelo de regresi\'on es suficiente o no. Se puede demostrar que $0\leq R^{2}\leq1$, una manera de interpretar este valor es que si $R^{2}=k$, entonces el modelo de regresi\'on explica el $k*100\%$ de la variabilidad en los datos.
%\end{frame}

%\begin{frame}\frametitle{Coeficiente de Determinaci\'on}
$R^{2}$ 
\begin{itemize}
\item No mide la magnitud de la pendiente de la recta de regresi\'on
\item Un valor grande de $R^{2}$ no implica una pendiente empinada.
\item No mide la suficiencia del modelo.
\item Valores grandes de $R^{2}$ no implican necesariamente que el modelo de regresi\'on proporcionar\'a predicciones precisas para futuras observaciones.
\end{itemize}
%\end{frame}


\section{An\'alisis de Varianza}

%\begin{frame}\frametitle{An\'alisis de Varianza}
Para analizar el ajuste de regresi\'on se utiliza el m\'etodo de  \textbf{An\'alisis de Varianza} (\textbf{ANOVA}), el en cu\'al  se estudia la variaci\'on de la variable dependiente, subidividiendola en dos componentes significativos.
Recordemos las ecuaciones \ref{Suma.Total.Cuadrados} y \ref{Suma.Total.Cuadrados.Dos}:
\begin{eqnarray*}
S_{yy}=SC_{R}+SC_{E}.
\end{eqnarray*}

\begin{itemize}
\item[$SC_{R}$] Se le denomina \textbf{suma de cuadrados de la regresi\'on} y refleja la cantidad de variaci\'on de los valores de $y$ que es explicada por el modelo, para nuestro caso: la recta propuesta.

\item[$SC_{E}$] Se le denomina suma de cuadrados del error, que es la variaci\'on o diferencia que hay entre los valores originales y los obtenidos mediante el ajuste. 
\end{itemize}

%\end{frame}




%\begin{frame}\frametitle{An\'alisis de Varianza}
De lo anterior se desprende que estamos interesados en validar nuestro modelo dado en la ecuaci\'on (\ref{Modelo.Regresion}), es decir, 

\begin{eqnarray*}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray*}
que en realidad el par\'ametro $\beta_{1}$ ha sido bien estimado:

Supongamos que se desea probar la hip\'otesis
\begin{itemize}
\item[$H_{0}:$] $\beta_{1}=0$
\item[$H_{1}:$] $\beta_{1}\neq0$
\end{itemize}
%\end{frame}



%\begin{frame}\frametitle{An\'alisis de Varianza}
Donde la hip\'otesis nula nos dice que el modelo en realidad debe de ser: $\mu_{Y|x}=\beta_{0}$, es decir, las variaciones en los valores de $Y$ son independientes de los valores de $x$. Se puede demostrar que bajo la hip\'otesis nula los t\'erminos
\begin{itemize}
\item $SC_{R}/\sigma^{2}$ se distribuye $\chi^{2}$ con 1 grado de libertad
\item $SC_{E}/\sigma^{2}$ se distribuye $\chi^{2}$ con $n-2$ grado de libertad.
\end{itemize}
e independientes, y por tanto $S_{yy}$,  tambien llamada \textbf{suma total de cuadrados corregida: STCC}, se distribuye $\chi^{2}$ con $n-1$ grados de libertad.

%\end{frame}


%\begin{frame}\frametitle{An\'alisis de Varianza}

Para realizar esta prueba de hip\'otesis se calcula el cociente

\begin{eqnarray*}
f=\frac{SC_{R}/1}{SC_{E}/\left(n-2\right)}=\frac{SC_{R}}{s^{2}}
\end{eqnarray*}
y se rechaza $H_{0}$ a un nivel de significancia $\alpha$ si $f>f_{\alpha}\left(1,\left(n-2\right)\right)$, esto se puede realizar mediante una tabla, llamada tabla de an\'alisis de varianza, cuando a las distintas sumas de cuadrados se les divide por sus grados de libertad, se les denomina \textbf{cuadrados medios}.


%\end{frame}

%---------------------------------------------------------
\section{3. An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------
%\begin{frame}\frametitle{Descripci\'on}
\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\end{itemize}

\end{Note}
%\end{frame}

\subsection{3.1 Regresi\'on Lineal Simple (RLS)}
%\begin{frame}\frametitle{RLS}
\begin{itemize}

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

%\end{frame}

\subsection{3.2 M\'etodo de M\'inimos Cuadrados}
%\begin{frame}\frametitle{M\'inimos Cuadrados}
Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}
donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$, estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.
%\end{frame}


%\begin{frame}\frametitle{M\'inimos Cuadrados}
Entonces la ecuaci\'on \ref{Modelo.Regresion} se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}
%\end{frame}



%\begin{frame}\frametitle{M\'inimos Cuadrados}
Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$, igualar a cero y resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene
%\end{frame}
%\begin{frame}\frametitle{M\'inimos Cuadrados}
\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
%\end{frame}

%\begin{frame}\frametitle{M\'inimos Cuadrados}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ecs.Estimadores.Regresion}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}
%\end{frame}
%\begin{frame}\frametitle{M\'inimos Cuadrados}
Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}
%\end{frame}
\subsection{3.3 Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}
%\begin{frame}\frametitle{Propiedades de los estimadores}
\begin{Note}
\begin{itemize}
\item Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo.

\item Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.
\end{itemize}
\end{Note}
A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)
\end{eqnarray*}
%\end{frame}


%\begin{frame}\frametitle{Propiedades de los estimadores}
\begin{eqnarray*}
&=&\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)\right.\\
&+&\left.E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]=\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}
\end{eqnarray*}
por lo tanto 
\begin{equation}\label{Esperanza.Beta.1}
E\left(\hat{\beta}_{1}\right)=\beta_{1}
\end{equation}
%\end{frame}

%\begin{frame}\frametitle{Propiedades de los estimadores}
\begin{Note}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado.
\end{Note}
Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}\\
&=&\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}}
\end{eqnarray*}

%\end{frame}

%\begin{frame}\frametitle{Propiedades de los estimadores}
por lo tanto
\begin{equation}\label{Varianza.Beta.1}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}
\end{equation}
\begin{Prop}
\begin{eqnarray*}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray*}
\end{Prop}
%\end{frame}

%\begin{frame}\frametitle{Propiedades de los estimadores}
Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray}
%\end{frame}

%\begin{frame}\frametitle{Propiedades de los estimadores}
sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=MC_{E}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray*}
%\end{frame}

%\end{document}
\subsection{3.4 Prueba de Hip\'otesis en RLS}
%\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{itemize}
\item Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza.

\item Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$.

\end{itemize}

%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:
\begin{centering}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,

\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.

\end{itemize}
donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. 
\end{centering}
De las ecuaciones (\ref{Ecs.Estimadores.Regresion}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1}) y (\ref{Varianza.Beta.1}).
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

%\end{frame}


%\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}=\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}

%\end{frame}


%\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}
%\end{frame}


%\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
\end{eqnarray*}
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{eqnarray*}
&=&\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}
Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados}) se puede reescribir como 
\begin{equation}\label{Suma.Total.Cuadrados.Dos}
S_{yy}=SC_{R}+SC_{E}
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right)\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}													
\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx}
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}
Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$.
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}													
El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente\medskip

\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 
%\end{frame}

%\begin{frame}\frametitle{Prueba de Hip\'otesis}													
La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1}), con $\hat{\beta}_{1,0}=0$, es decir
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.

%\end{frame}

%\end{document}
\subsection{Estimaci\'on de Intervalos en RLS}
%\begin{frame}\frametitle{Intervalos de Confianza}
\begin{itemize}
\item Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros.

\item El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on.

\end{itemize}


%\end{frame}

%\begin{frame}\frametitle{Intervalos de Confianza}
Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por
%\end{frame}
%\begin{frame}\frametitle{Intervalos de Confianza}
\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\small{
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}}
%\end{frame}
\subsection{Predicci\'on}
%\begin{frame}\frametitle{Predicci\'on}
Supongamos que se tiene un valor $x_{0}$ de inter\'es, entonces la estimaci\'on puntual de este nuevo valor
\begin{equation}
\hat{y}_{0}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}
\end{equation}
\begin{Note}
Esta nueva observaci\'on es independiente de las utilizadas para obtener el modelo de regresi\'on, por tanto, el intervalo en torno a la recta de regresi\'on es inapropiado, puesto que se basa \'unicamente en los datos empleados para ajustar el modelo de regresi\'on.\\

El intervalo de confianza en torno a la recta de regresi\'on se refiere a la respuesta media verdadera $x=x_{0}$, no a observaciones futuras.
\end{Note}

%\end{frame}


%\begin{frame}\frametitle{Predicci\'on}
Sea $y_{0}$ la observaci\'on futura en $x=x_{0}$, y sea $\hat{y}_{0}$ dada en la ecuaci\'on anterior, el estimador de $y_{0}$. Si se define la variable aleatoria $$w=y_{0}-\hat{y}_{0},$$ esta se distribuye normalmente con media cero y varianza $$V\left(w\right)=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]$$
dado que $y_{0}$ es independiente de $\hat{y}_{0}$, por lo tanto el intervalo de predicci\'on al nivel $\alpha$ para futuras observaciones $x_{0}$ es

%\end{frame}

%\begin{frame}\frametitle{Predicci\'on}
\begin{eqnarray*}
\hat{y}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}\leq y_{0}\\
\leq \hat{y}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}.
\end{eqnarray*}
%\end{frame}


%\subsection{Prueba de falta de ajuste}
%%\begin{frame}\frametitle{Falta de ajuste}
%Es com\'un encontrar que el modelo ajustado no satisface totalmente el modelo necesario para los datos, en este caso es preciso saber qu\'e tan bueno es el modelo propuesto. Para esto se propone la siguiente prueba de hip\'otesis:
%\begin{itemize}
%\item[$H_{0}:$ ]El modelo propuesto se ajusta adecuademente a los datos.
%\item[$H_{1}:$ ]El modelo NO se ajusta a los datos.
%\end{itemize}
%La prueba implica dividir la suma de cuadrados del eror o del residuo en las siguientes dos componentes:
%\begin{eqnarray*}
%SC_{E}=SC_{EP}+SC_{FDA}
%\end{eqnarray*}

%%\end{frame}

%%\begin{frame}\frametitle{Falta de ajuste}
%donde $SC_{EP}$ es la suma de cuadrados atribuibles al error puro, y $SC_{FDA}$ es la suma de cuadrados atribuible a la falta de ajuste del modelo.
%%\end{frame}

%%\begin{frame}\frametitle{Falta de ajuste}
%%\end{frame}


\subsection{Coeficiente de Determinaci\'on}
%\begin{frame}\frametitle{Coeficiente de Determinaci\'on}
La cantidad
\begin{equation}
R^{2}=\frac{SC_{R}}{S_{yy}}=1-\frac{SC_{E}}{S_{yy}}
\end{equation}
se denomina coeficiente de determinaci\'on y se utiliza para saber si el modelo de regresi\'on es suficiente o no. Se puede demostrar que $0\leq R^{2}\leq1$, una manera de interpretar este valor es que si $R^{2}=k$, entonces el modelo de regresi\'on explica el $k*100\%$ de la variabilidad en los datos.
%\end{frame}

%\begin{frame}\frametitle{Coeficiente de Determinaci\'on}
$R^{2}$ 
\begin{itemize}
\item No mide la magnitud de la pendiente de la recta de regresi\'on
\item Un valor grande de $R^{2}$ no implica una pendiente empinada.
\item No mide la suficiencia del modelo.
\item Valores grandes de $R^{2}$ no implican necesariamente que el modelo de regresi\'on proporcionar\'a predicciones precisas para futuras observaciones.
\end{itemize}
%\end{frame}


\section{An\'alisis de Varianza}

%\begin{frame}\frametitle{An\'alisis de Varianza}
Para analizar el ajuste de regresi\'on se utiliza el m\'etodo de  \textbf{An\'alisis de Varianza} (\textbf{ANOVA}), el en cu\'al  se estudia la variaci\'on de la variable dependiente, subidividiendola en dos componentes significativos.
Recordemos las ecuaciones \ref{Suma.Total.Cuadrados} y \ref{Suma.Total.Cuadrados.Dos}:
\begin{eqnarray*}
S_{yy}=SC_{R}+SC_{E}.
\end{eqnarray*}

\begin{itemize}
\item[$SC_{R}$] Se le denomina \textbf{suma de cuadrados de la regresi\'on} y refleja la cantidad de variaci\'on de los valores de $y$ que es explicada por el modelo, para nuestro caso: la recta propuesta.

\item[$SC_{E}$] Se le denomina suma de cuadrados del error, que es la variaci\'on o diferencia que hay entre los valores originales y los obtenidos mediante el ajuste. 
\end{itemize}

%\end{frame}




%\begin{frame}\frametitle{An\'alisis de Varianza}
De lo anterior se desprende que estamos interesados en validar nuestro modelo dado en la ecuaci\'on (\ref{Modelo.Regresion}), es decir, 

\begin{eqnarray*}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray*}
que en realidad el par\'ametro $\beta_{1}$ ha sido bien estimado:

Supongamos que se desea probar la hip\'otesis
\begin{itemize}
\item[$H_{0}:$] $\beta_{1}=0$
\item[$H_{1}:$] $\beta_{1}\neq0$
\end{itemize}
%\end{frame}



%\begin{frame}\frametitle{An\'alisis de Varianza}
Donde la hip\'otesis nula nos dice que el modelo en realidad debe de ser: $\mu_{Y|x}=\beta_{0}$, es decir, las variaciones en los valores de $Y$ son independientes de los valores de $x$. Se puede demostrar que bajo la hip\'otesis nula los t\'erminos
\begin{itemize}
\item $SC_{R}/\sigma^{2}$ se distribuye $\chi^{2}$ con 1 grado de libertad
\item $SC_{E}/\sigma^{2}$ se distribuye $\chi^{2}$ con $n-2$ grado de libertad.
\end{itemize}
e independientes, y por tanto $S_{yy}$,  tambien llamada \textbf{suma total de cuadrados corregida: STCC}, se distribuye $\chi^{2}$ con $n-1$ grados de libertad.

%\end{frame}


%\begin{frame}\frametitle{An\'alisis de Varianza}

Para realizar esta prueba de hip\'otesis se calcula el cociente

\begin{eqnarray*}
f=\frac{SC_{R}/1}{SC_{E}/\left(n-2\right)}=\frac{SC_{R}}{s^{2}}
\end{eqnarray*}
y se rechaza $H_{0}$ a un nivel de significancia $\alpha$ si $f>f_{\alpha}\left(1,\left(n-2\right)\right)$, esto se puede realizar mediante una tabla, llamada tabla de an\'alisis de varianza, cuando a las distintas sumas de cuadrados se les divide por sus grados de libertad, se les denomina \textbf{cuadrados medios}.


%\end{frame}

%\begin{frame}\frametitle{An\'alisis de Varianza}
\begin{table}[t!]
\begin{center}
\scalebox{0.65}{\begin{tabular}{| c | c | c | c| c | }
\hline 
\textbf{Fuente de Variaci\'on}&\textbf{Suma de Cuadrados}&\textbf{ Grados de Libertad }&\textbf{ Cuadrado Medio }&\textbf{ $f$ calculada }\\ 
\hline 
Regresi\'on & $SC_{R}$ & 1 & $SC_{R}$ & $\frac{SC_{R}}{s^{2}}$ \\ 
Error & $SC_{E}$ & $n-2$ & $s^{2}=\frac{SC_{E}}{n-2}$ &  \\\hline
Total & $STCC$ & $n-1$ &  &  \\ 
\hline 
\end{tabular}}
\caption{An\'alisis de Varianza para la prueba $\beta_{1}=0$}
\label{tab:ANOVA}
\end{center}
\end{table}
Se rechaza la hip\'otesis nula, cuando el estad\'istico $F$ calculado excede al valor cr\'itico $f_{\alpha}\left(1-n-2\right)$, y entonces se concluye que existe evidencia sobre la variaci\'on respecto al modelo ajustado. Si el estad\'istico $F$ est\'a en la regi\'on de no rechazo, se concluye que los datos no reflejan evidencia suficiente para sostener que el modelo ajustado.
%\end{frame}


%\begin{frame}\frametitle{An\'alisis de Varianza}
Para hacer la prueba de hip\'otesis 
\begin{itemize}
\item[$H_{0}$ :]$\beta_{1}=\beta_{10}$
\item[$H_{1}$ :]$\beta_{1}\neq\beta_{10}$
\end{itemize}
se utiliza el estad\'istico:
\begin{eqnarray*}
T=\frac{B_{1}-\beta_{10}}{S/\sqrt{S_{xx}}}
\end{eqnarray*}
donde $T$ se distribuye $t$ con $n-2$ grados de libertad. La hip\'otesis se rechaza si $|t|>t_{\alpha/2}$ con un nivel de confianza $\alpha$.
%\end{frame}
%\begin{frame}\frametitle{An\'alisis de Varianza}
\begin{Note}
Para el caso en que $\beta_{10}=0$, se tiene que el valor del estad\'istico se convierte en 
\begin{eqnarray*}
T=\frac{b_{1}-\beta_{10}}{s/\sqrt{S_{xx}}}
\end{eqnarray*}
y entonces el an\'alisis es similar al dado en la tabla \ref{tab:ANOVA}, y lo que se est\'a diciendo es que la variaci\'on depende totalmente del azar.
\end{Note}
\begin{Note}
El An\'alisis de Varianza utiliza la distribuci\'on $F$ en lugar de la distribuci\'on $t$.
\end{Note}

%\end{frame}

%\begin{frame}\frametitle{An\'alisis de Varianza}
Supongamos que se tienen observaciones repetidas de las respuestas para $k$ valores distintos de $x$, es decir: para $x_{1},x_{2},\ldots,x_{k}$ se tienen $y_{1,1},y_{1,2},\ldots,y_{1,n_{1}}$ valores observados para la variable aleatoria $Y_{1}$, $y_{2,1},y_{2,2},\ldots,y_{2,n_{2}}$ valores observados para la variable aleatoria $Y_{2}$, y as\'i sucesivamente para $y_{k,1},y_{k,2},\ldots,y_{1,n_{k}}$ valores observados para la variable aleatoria $Y_{k}$, de tal manera que 
\begin{eqnarray*}
n=\sum_{i=1}^{k}n_{i}
\end{eqnarray*}
%\end{frame}
%\begin{frame}\frametitle{An\'alisis de Varianza}
\begin{eqnarray*}
Y = \left[
\begin{matrix}
y_{1,1} & y_{2,1} & \cdots & Y_{k,1} \\ 
y_{1,2} & y_{2,2} & \cdots & Y_{k,2} \\ 
\vdots & \vdots &  \vdots & \vdots \\ 
 y_{1,j} &  y_{2,j} & y_{i,j} &  y_{k,j} \\ 
\vdots & \vdots & \vdots & \vdots \\ 
y_{1,n_{1}} & y_{2,n_{2}} & \cdots & Y_{k,n_{k}} 
\end{matrix} 
\right]
\end{eqnarray*}
entonces, si definimos $y_{i}=T_{i}=\sum_{j=1}^{n_{i}}y_{i,j}$, se tiene que $\overline{y}_{i}=\frac{T_{i}}{n_{i}}$
%\end{frame}
%\begin{frame}\frametitle{An\'alisis de Varianza}
C\'omo se ve la matriz para el caso en que:
\begin{itemize}
\item $n_{4}=3$ mediciones de $Y$
\item Simular en R, para los casos en que $n_{1}=4$, $n_{2}=6$, $n_{3}=5$, y $n_{4}=8$,
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{An\'alisis de Varianza: Falta de ajuste}
La suma de cuadrados del error se divide en dos partes: la cantidad debida a la variaci|'on entre los valores de $Y$ para los valores dados de $x$, y lo que se denomina \textbf{falta de ajuste} que es una medida de la variaci\'on sistem\'atica introducida por  los t\'erminos de orden superior. Para nuestro caso en espec\'ifico, estos son t\'erminos de $x$ distintos de la contribuci\'on lineal de primer orden.\medskip

Hasta el momento, dado que hemos considerado un modelo lineal, se asume que este segundo componente no existe, y por tanto la suma de cuadrados de error depende totalmente de los errores aleatorios. En consecuencia tenemos que $s^{2}=\frac{SCE}{\left(n-2\right)}$ es un estimador insesgado para $\sigma^{2}$.
%\end{frame}
%\begin{frame}\frametitle{An\'alisis de Varianza}
Sin embargo, si el modelo no ajusta correctamente a los datos, lo que tenemos es una sobre estimaci\'on del valor de $\sigma^{2}$ y por tanto ser\'a un estimador sesgado del mismo.\medskip

Para obtener un estimador insesgado se calcula
\begin{eqnarray*}
s^{2}=\frac{\sum_{j=1}^{n_{i}}\left(y_{ij}-\overline{y}_{i}\right)^{2}}{n_{i}-1},\textrm{ para } i=1,2,\ldots,k
\end{eqnarray*}
despu\'es de hacer unas operaciones se puede obtener:

%\end{frame}

%\begin{frame}\frametitle{An\'alisis de Varianza}

\begin{eqnarray*}
s^{2}=\frac{\sum_{i=1}^{k}\left(n_{i}-1\right)s_{i}^{2}}{n_{i}-k}=\frac{\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}\left(y_{ij}-\overline{y}_{i}\right)^{2}}{n-k}.
\end{eqnarray*}
El numerador de $s^{2}$ es una medida del \textbf{error experimental puro} o \textbf{falta de ajuste}\medskip

Para determinar el cuadrado del error en: error puro y la falta de ajuste:
\begin{itemize}
\item Se calcula la suma de cuadrados del error puro:
\begin{eqnarray*}
\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}\left(y_{ij}-\overline{y}_{i}\right)^{2}
\end{eqnarray*}

\end{itemize}


%\end{frame}
%\begin{frame}\frametitle{An\'alisis de Varianza}
esta suma de cuadrados tiene $n-k$ grados de libertad, y el cuadrado medio resultante es el estimador insesgado $s^{2}$ de $\sigma^{2}$.
\begin{itemize}
\item Restar la suma de cuadrados del error puro de la suma de cuadrados del error, SCE, resultando la suma de cuadrados por ajuste. Los grados de libertad de la  falta de ajuste se obtienen por la resta: $\left(n-2\right)-\left(n-k\right)=k-2$.
\end{itemize}
%\end{frame}
%\begin{frame}\frametitle{An\'alisis de Varianza}
La prueba de hip\'otesis en un problema de regresi\'on con mediciones repetidas de la respuesta se ilustra en la tabla \ref{tab:ANOVA}:
\begin{table}[t!]
\begin{center}
\scalebox{0.65}{\begin{tabular}{| c | c | c | c| c | }
\hline 
\textbf{Fuente de Variaci\'on}&\textbf{Suma de Cuadrados}&\textbf{ Grados de Libertad }&\textbf{ Cuadrado Medio }&\textbf{ $f$ calculada }\\ 
\hline 
Regresi\'on & $SC_{R}$ & 1 & $SC_{R}$ & $\frac{SC_{R}}{s^{2}}$ \\ 
Error & $SC_{E}$ & $n-2$ &  &  \\\hline
Falta de ajuste &$SCE-SCE (puro)$&$k-2$&$\frac{SCE-SCE (puro)}{k-2}$&$\frac{SCE-SCE(puro)}{s^{2}\left(k-2\right)}$\\\hline
Error Puro &$SCE(puro)$&$n-k$&$s^{2}=\frac{SCE(puro)}{n-k}$&\\\hline
Total & $STCC$ & $n-1$ &  &  \\ 
\hline 
\end{tabular}}
\caption{An\'alisis de Varianza para la prueba $\beta_{1}=0$}
\label{tab:ANOVA}
\end{center}
\end{table}
%\end{frame}


%---------------------------------------------------------
\section{Pruebas de Hip\'otesis}
%---------------------------------------------------------
\subsection{Tipos de errores}



%\begin{frame}

%%\begin{frame}\frametitle{Prueba de Hip\'otesis}

\begin{itemize}
\item Una hip\'otesis estad\'istica es una afirmaci\'on  acerca de la distribuci\'on de probabilidad de una variable aleatoria, a menudo involucran uno o m\'as par\'ametros de la distribuci\'on.

\item Las hip\'otesis son afirmaciones respecto a la poblaci\'on o distribuci\'on bajo estudio, no en torno a la muestra.

\item La mayor\'ia de las veces, la prueba de hip\'otesis consiste en determinar si la situaci \'on experimental ha cambiado

\item el inter\'es principal es decidir sobre la veracidad o falsedad de una hip\'otesis, a este procedimiento se le llama \textit{prueba de hip\'otesis}.

\item Si la informaci\'on es consistente con la hip\'otesis, se concluye que esta es verdadera, de lo contrario que con base en la informaci\'on, es falsa.

\end{itemize}

%\end{frame}



%\begin{frame}

Una prueba de hip\'otesis est\'a formada por cinco partes
\begin{itemize}
\item La hip\'otesis nula, denotada por $H_{0}$.
\item La hip\'otesis alterativa, denorada por $H_{1}$.
\item El estad\'sitico de prueba y su valor $p$.
\item La regi\'on de rechazo.
\item La conclusi\'on.

\end{itemize}

\begin{Def}
Las dos hip\'otesis en competencias son la \textbf{hip\'otesis alternativa $H_{1}$}, usualmente la que se desea apoyar, y la \textbf{hip\'otesis nula $H_{0}$}, opuesta a $H_{1}$.
\end{Def}

%\end{frame}




%\begin{frame}


En general, es m\'as f\'acil presentar evidencia de que $H_{1}$ es cierta, que demostrar 	que $H_{0}$ es falsa, es por eso que por lo regular se comienza suponiendo que $H_{0}$ es cierta, luego se utilizan los datos de la muestra para decidir si existe evidencia a favor de $H_{1}$, m\'as que a favor de $H_{0}$, as\'i se tienen dos conclusiones
\begin{itemize}
\item Rechazar $H_{0}$ y concluir que $H_{1}$ es verdadera.
\item Aceptar, no rechazar, $H_{0}$ como verdadera.

\end{itemize}

\begin{Ejem}
Se desea demostrar que el salario promedio  por hora en cierto lugar es distinto de $19$usd, que es el promedio nacional. Entonces $H_{1}:\mu\neq19$, y $H_{0}:\mu=19$.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de dos colas}.


%\end{frame}




%\begin{frame}


\begin{Ejem}
Un determinado proceso produce un promedio de $5\%$ de piezas defectuosas. Se est\'a interesado en demostrar que un simple ajuste en una m\'aquina reducir\'a $p$, la proporci\'on de piezas defectuosas producidas en este proceso. Entonces se tiene $H_{0}:p<0.3$ y $H_{1}:p=0.03$. Si se puede rechazar $H_{0}$, se concluye que el proceso ajustado produce menos del $5\%$ de piezas defectuosas.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de una cola}.

La decisi\'on de rechazar o aceptar la hip\'otesis nula est\'a basada en la informaci\'on contenida en una muestra proveniente de la poblaci\'on de inter\'es. Esta informaci\'on tiene estas formas
%\end{frame}

%\begin{frame}

\begin{itemize}
\item \textbf{Estad\'sitico de prueba:} un s\'olo n\'umero calculado a partir de la muestra.

\item \textbf{$p$-value:} probabilidad calculada a partir del estad\'stico de prueba.
\begin{Def}
El $p$-value es la probabilidad de observar un estad\'istico de prueba tanto o m\'as alejado del valor obervado, si en realidad $H_{0}$ es verdadera.\medskip
Valores grandes del estad\'stica de prueba  y valores peque\~nos de $p$ significan que se ha observado un evento muy poco probable, si $H_{0}$ en realidad es verdadera.
\end{Def}

\end{itemize}
%\end{frame}



%\begin{frame}

Todo el conjunto de valores que puede tomar el estad\'istico de prueba se divide en dos regiones. Un conjunto, formado de valores que apoyan la hip\'otesis alternativa y llevan a rechazar $H_{0}$, se denomina \textbf{regi\'on de rechazo}. El otro, conformado por los valores que sustentatn la hip\'otesis nula, se le denomina \textbf{regi\'on de aceptaci\'on}.\medskip

Cuando la regi\'on de rechazo est\'a en la cola izquierda de la distribuci\'on, la  prueba se denomina \textbf{prueba lateral izquierda}. Una prueba con regi\'on de rechazo en la cola derecha se le llama \textbf{prueba lateral derecha}.


Si el estad\'stico de prueba cae en la regi\'on de rechazo, entonces se rechaza $H_{0}$. Si el estad\'stico de prueba cae en la regi\'on de aceptaci\'on, entonces la hip\'otesis nula se acepta o la prueba se juzga como no concluyente.\medskip
%\end{frame}



%\begin{frame}

Dependiendo del nivel de confianza que se desea agregar a las conclusiones de la prueba, y el \textbf{nivel de significancia $\alpha$}, el riesgo que est\'a dispuesto a correr si se toma una decisi\'on incorrecta.

%\end{frame}


%\begin{frame}

\begin{Def}
Un \textbf{error de tipo I} para una prueba estad\'istica es el error que se tiene al rechazar la hip\'otesis nula cuando es verdadera. El \textbf{nivel de significancia} para una prueba estad\'istica de hip\'otesis es
\begin{eqnarray*}
\alpha&=&P\left\{\textrm{error tipo I}\right\}=P\left\{\textrm{rechazar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es verdadera}\right\}
\end{eqnarray*}

\end{Def}
Este valor $\alpha$ representa el valor m\'aximo de riesgo tolerable de rechazar incorrectamente $H_{0}$. Una vez establecido el nivel de significancia, la regi\'on de rechazo se define para poder determinar si se rechaza $H_{0}$ con un cierto nivel de confianza.

%\end{frame}

\section{Muestras grandes: una media poblacional}
\subsection{C\'alculo de valor $p$}



%\begin{frame}

\begin{Def}
El \textbf{valor de $p$} (\textbf{$p$-value}) o nivel de significancia observado de un estad\'istico de prueba es el valor m\'as peque\~ no de $\alpha$ para el cual $H_{0}$ se puede rechazar. El riesgo de cometer un error tipo $I$, si $H_{0}$ es rechazada con base en la informaci\'on que proporciona la muestra.
\end{Def}

\begin{Note}
Valores peque\~ nos de $p$ indican 	que el valor observado del estad\'stico de prueba se encuentra alejado del valor hipot\'etico de $\mu$, es decir se tiene evidencia de que $H_{0}$ es falsa y por tanto debe de rechazarse.
\end{Note}

\begin{Note}
Valores grandes de $p$ indican que el estad\'istico de prueba observado no est\'a alejado de la medi hipot\'etica y no apoya el rechazo de $H_{0}$.
\end{Note}


%\end{frame}


%\begin{frame}

\begin{Def}
Si el valor de $p$ es menor o igual que el nivel de significancia $\alpha$, determinado previamente, entonces $H_{0}$ es rechazada y se puede concluir que los resultados son estad\'isticamente significativos con un nivel de confianza del $100\left(1-\alpha\right)\%$.
\end{Def}
Es usual utilizar la siguiente clasificaci\'on de resultados


\begin{tabular}{|c||c|l|}\hline
$p$& $H_{0}$&Significativa\\\hline\hline
$p<0.01$&Rechazar &Altamente\\\hline
$0.01\leq p<0.05$ & Rechazar&Estad\'isticamente\\\hline
$0.05\leq p <0.1$ & No rechazar & Tendencia estad\'istica\\\hline
$0.01\leq p$ & No rechazar & No son estad\'isticamente\\\hline
\end{tabular}

%\end{frame}

%\begin{frame}

\begin{Note}
Para determinar el valor de $p$, encontrar el \'area en la cola despu\'es del estad\'istico de prueba. Si la prueba es de una cola, este es el valor de $p$. Si es de dos colas, \'este valor encontrado es la mitad del valor de $p$. Rechazar $H_{0}$ cuando el valor de $p<\alpha$.
\end{Note}

Hay dos tipos de errores al realizar una prueba de hip\'otesis
\begin{center}
\begin{tabular}{c|cc}
& $H_{0}$ es Verdadera & $H_{0}$ es Falsa\\\hline\hline
Rechazar $H_{0}$ & Error tipo I & $\surd$\\
Aceptar $H_{0}$ & $\surd$ & Error tipo II
\end{tabular}
\end{center}

%\end{frame}


%\begin{frame}

\begin{Def}
La probabilidad de cometer el error tipo II se define por $\beta$ donde
\begin{eqnarray*}
\beta&=&P\left\{\textrm{error tipo II}\right\}=P\left\{\textrm{Aceptar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{Aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}
\end{eqnarray*}
\end{Def}

\begin{Note}
Cuando $H_{0}$ es falsa y $H_{1}$ es verdadera, no siempre es posible especificar un valor exacto de $\mu$, sino m\'as bien un rango de posibles valores.\medskip
En lugar de arriesgarse a tomar una decisi\'on incorrecta, es mejor conlcuir que \textit{no hay evidencia suficiente para rechazar $H_{0}$}, es decir en lugar de aceptar $H_{0}$, \textit{no rechazar $H_{0}$}.

\end{Note}
%\end{frame}



%\begin{frame}

La bondad de una prueba estad\'istica se mide por el tama\~ no de $\alpha$ y $\beta$, ambas deben de ser peque\~ nas. Una manera muy efectiva de medir la potencia de la prueba es calculando el complemento del error tipo $II$:
\begin{eqnarray*}
1-\beta&= &P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{1}\textrm{ es verdadera}\right\}
\end{eqnarray*}
\begin{Def}
La \textbf{potencia de la prueba}, $1-\beta$, mide la capacidad de que la prueba funciona como se necesita.
\end{Def}


%\end{frame}


%\begin{frame}

\begin{Ejem}
La producci\'on diariade una planta qu\'imica local ha promediado 880 toneladas en los \'ultimos a\~nos. A la gerente de control de calidad le gustar\'ia saber si este promedio ha cambiado en meses recientes. Ella selecciona al azar 50 d\'ias de la base de datos computarizada y calcula el promedio y la desviaci\'on est\'andar de las $n=50$  producciones como $\overline{x}=871$ toneladas y $s=21$ toneladas, respectivamente. Pruebe la hip\'otesis  apropiada usando $\alpha=0.05$.

\end{Ejem}

%\end{frame}


%\begin{frame}

\begin{Sol}
La hip\'otesis nula apropiada es:

\begin{eqnarray*}
H_{0}&:& \mu=880\\
&&\textrm{ y la hip\'otesis alternativa }H_{1}\textrm{ es }\\
H_{1}&:& \mu\neq880
\end{eqnarray*}
el estimador puntual para $\mu$ es $\overline{x}$, entonces el estad\'istico de prueba es\medskip
\begin{eqnarray*}
z&=&\frac{\overline{x}-\mu_{0}}{s/\sqrt{n}}\\
&=&\frac{871-880}{21/\sqrt{50}}=-3.03
\end{eqnarray*}
\end{Sol}



\begin{Sol}
Para esta prueba de  dos colas, hay que determinar los dos valores de $z_{\alpha/2}$, es decir, $z_{\alpha/2}=\pm1.96$

\end{Sol}


%\end{frame}


%---------------------------------------------------------
\section{Pruebas de Hip\'otesis}
%---------------------------------------------------------
\subsection{Tipos de errores}



%\begin{frame}

\begin{itemize}
\item Una hip\'otesis estad\'istica es una afirmaci\'on  acerca de la distribuci\'on de probabilidad de una variable aleatoria, a menudo involucran uno o m\'as par\'ametros de la distribuci\'on.

\item Las hip\'otesis son afirmaciones respecto a la poblaci\'on o distribuci\'on bajo estudio, no en torno a la muestra.

\item La mayor\'ia de las veces, la prueba de hip\'otesis consiste en determinar si la situaci \'on experimental ha cambiado

\item el inter\'es principal es decidir sobre la veracidad o falsedad de una hip\'otesis, a este procedimiento se le llama \textit{prueba de hip\'otesis}.

\item Si la informaci\'on es consistente con la hip\'otesis, se concluye que esta es verdadera, de lo contrario que con base en la informaci\'on, es falsa.

\end{itemize}

%\end{frame}





%\begin{frame}


\begin{itemize}
\item La descisi\'on de aceptar o rechazar la hip\'otesis nula se basa en un estad\'istico calculado a partir de la muestra. Esto necesariamente implica la existencia de un error.


\end{itemize}

%\end{frame}

%---------------------------------------------------------
\section{Pruebas de Hip\'otesis}
%---------------------------------------------------------
\subsection{Tipos de errores}


%\begin{frame}

\begin{itemize}
\item Una hip\'otesis estad\'istica es una afirmaci\'on  acerca de la distribuci\'on de probabilidad de una variable aleatoria, a menudo involucran uno o m\'as par\'ametros de la distribuci\'on.

\item Las hip\'otesis son afirmaciones respecto a la poblaci\'on o distribuci\'on bajo estudio, no en torno a la muestra.

\item La mayor\'ia de las veces, la prueba de hip\'otesis consiste en determinar si la situaci \'on experimental ha cambiado

\item el inter\'es principal es decidir sobre la veracidad o falsedad de una hip\'otesis, a este procedimiento se le llama \textit{prueba de hip\'otesis}.

\item Si la informaci\'on es consistente con la hip\'otesis, se concluye que esta es verdadera, de lo contrario que con base en la informaci\'on, es falsa.

\end{itemize}


%\end{frame}


%\begin{frame}

Una prueba de hip\'otesis est\'a formada por cinco partes
\begin{itemize}
\item La hip\'otesis nula, denotada por $H_{0}$.
\item La hip\'otesis alterativa, denorada por $H_{1}$.
\item El estad\'sitico de prueba y su valor $p$.
\item La regi\'on de rechazo.
\item La conclusi\'on.

\end{itemize}

\begin{Def}
Las dos hip\'otesis en competencias son la \textbf{hip\'otesis alternativa $H_{1}$}, usualmente la que se desea apoyar, y la \textbf{hip\'otesis nula $H_{0}$}, opuesta a $H_{1}$.
\end{Def}


%\end{frame}



%\begin{frame}

En general, es m\'as f\'acil presentar evidencia de que $H_{1}$ es cierta, que demostrar 	que $H_{0}$ es falsa, es por eso que por lo regular se comienza suponiendo que $H_{0}$ es cierta, luego se utilizan los datos de la muestra para decidir si existe evidencia a favor de $H_{1}$, m\'as que a favor de $H_{0}$, as\'i se tienen dos conclusiones
\begin{itemize}
\item Rechazar $H_{0}$ y concluir que $H_{1}$ es verdadera.
\item Aceptar, no rechazar, $H_{0}$ como verdadera.

\end{itemize}

\begin{Ejem}
Se desea demostrar que el salario promedio  por hora en cierto lugar es distinto de $19$usd, que es el promedio nacional. Entonces $H_{1}:\mu\neq19$, y $H_{0}:\mu=19$.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de dos colas}.

%\end{frame}


%\begin{frame}

\begin{Ejem}
Un determinado proceso produce un promedio de $5\%$ de piezas defectuosas. Se est\'a interesado en demostrar que un simple ajuste en una m\'aquina reducir\'a $p$, la proporci\'on de piezas defectuosas producidas en este proceso. Entonces se tiene $H_{0}:p<0.3$ y $H_{1}:p=0.03$. Si se puede rechazar $H_{0}$, se concluye que el proceso ajustado produce menos del $5\%$ de piezas defectuosas.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de una cola}.


La decisi\'on de rechazar o aceptar la hip\'otesis nula est\'a basada en la informaci\'on contenida en una muestra proveniente de la poblaci\'on de inter\'es. Esta informaci\'on tiene estas formas

\begin{itemize}
\item \textbf{Estad\'sitico de prueba:} un s\'olo n\'umero calculado a partir de la muestra.

\item \textbf{$p$-value:} probabilidad calculada a partir del estad\'stico de prueba.
\begin{Def}
El $p$-value es la probabilidad de observar un estad\'istico de prueba tanto o m\'as alejado del valor obervado, si en realidad $H_{0}$ es verdadera.\medskip
Valores grandes del estad\'stica de prueba  y valores peque\~nos de $p$ significan que se ha observado un evento muy poco probable, si $H_{0}$ en realidad es verdadera.
\end{Def}

\end{itemize}

%\end{frame}




%\begin{frame}

Todo el conjunto de valores que puede tomar el estad\'istico de prueba se divide en dos regiones. Un conjunto, formado de valores que apoyan la hip\'otesis alternativa y llevan a rechazar $H_{0}$, se denomina \textbf{regi\'on de rechazo}. El otro, conformado por los valores que sustentatn la hip\'otesis nula, se le denomina \textbf{regi\'on de aceptaci\'on}.\medskip

Cuando la regi\'on de rechazo est\'a en la cola izquierda de la distribuci\'on, la  prueba se denomina \textbf{prueba lateral izquierda}. Una prueba con regi\'on de rechazo en la cola derecha se le llama \textbf{prueba lateral derecha}.



Si el estad\'stico de prueba cae en la regi\'on de rechazo, entonces se rechaza $H_{0}$. Si el estad\'stico de prueba cae en la regi\'on de aceptaci\'on, entonces la hip\'otesis nula se acepta o la prueba se juzga como no concluyente.\medskip

%\end{frame}




%\begin{frame}

Dependiendo del nivel de confianza que se desea agregar a las conclusiones de la prueba, y el \textbf{nivel de significancia $\alpha$}, el riesgo que est\'a dispuesto a correr si se toma una decisi\'on incorrecta.
%\end{frame}




%\begin{frame}


\begin{Def}
Un \textbf{error de tipo I} para una prueba estad\'istica es el error que se tiene al rechazar la hip\'otesis nula cuando es verdadera. El \textbf{nivel de significancia} para una prueba estad\'istica de hip\'otesis es
\begin{eqnarray*}
\alpha&=&P\left\{\textrm{error tipo I}\right\}=P\left\{\textrm{rechazar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es verdadera}\right\}
\end{eqnarray*}

\end{Def}
Este valor $\alpha$ representa el valor m\'aximo de riesgo tolerable de rechazar incorrectamente $H_{0}$. Una vez establecido el nivel de significancia, la regi\'on de rechazo se define para poder determinar si se rechaza $H_{0}$ con un cierto nivel de confianza.

%\end{frame}




\section{Muestras grandes: una media poblacional}
\subsection{C\'alculo de valor $p$}




%\begin{frame}

\begin{Def}
El \textbf{valor de $p$} (\textbf{$p$-value}) o nivel de significancia observado de un estad\'istico de prueba es el valor m\'as peque\~ no de $\alpha$ para el cual $H_{0}$ se puede rechazar. El riesgo de cometer un error tipo $I$, si $H_{0}$ es rechazada con base en la informaci\'on que proporciona la muestra.
\end{Def}

\begin{Note}
Valores peque\~ nos de $p$ indican 	que el valor observado del estad\'stico de prueba se encuentra alejado del valor hipot\'etico de $\mu$, es decir se tiene evidencia de que $H_{0}$ es falsa y por tanto debe de rechazarse.
\end{Note}

\begin{Note}
Valores grandes de $p$ indican que el estad\'istico de prueba observado no est\'a alejado de la medi hipot\'etica y no apoya el rechazo de $H_{0}$.
\end{Note}

%\end{frame}



%\begin{frame}


\begin{Def}
Si el valor de $p$ es menor o igual que el nivel de significancia $\alpha$, determinado previamente, entonces $H_{0}$ es rechazada y se puede concluir que los resultados son estad\'isticamente significativos con un nivel de confianza del $100\left(1-\alpha\right)\%$.
\end{Def}
Es usual utilizar la siguiente clasificaci\'on de resultados


\begin{tabular}{|c||c|l|}\hline
$p$& $H_{0}$&Significativa\\\hline\hline
$p<0.01$&Rechazar &Altamente\\\hline
$0.01\leq p<0.05$ & Rechazar&Estad\'isticamente\\\hline
$0.05\leq p <0.1$ & No rechazar & Tendencia estad\'istica\\\hline
$0.01\leq p$ & No rechazar & No son estad\'isticamente\\\hline
\end{tabular}

%\end{frame}


%\begin{frame}

\begin{Note}
Para determinar el valor de $p$, encontrar el \'area en la cola despu\'es del estad\'istico de prueba. Si la prueba es de una cola, este es el valor de $p$. Si es de dos colas, \'este valor encontrado es la mitad del valor de $p$. Rechazar $H_{0}$ cuando el valor de $p<\alpha$.
\end{Note}

Hay dos tipos de errores al realizar una prueba de hip\'otesis
\begin{center}
\begin{tabular}{c|cc}
& $H_{0}$ es Verdadera & $H_{0}$ es Falsa\\\hline\hline
Rechazar $H_{0}$ & Error tipo I & $\surd$\\
Aceptar $H_{0}$ & $\surd$ & Error tipo II
\end{tabular}
\end{center}
%\end{frame}

%\begin{frame}



\begin{Def}
La probabilidad de cometer el error tipo II se define por $\beta$ donde
\begin{eqnarray*}
\beta&=&P\left\{\textrm{error tipo II}\right\}=P\left\{\textrm{Aceptar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{Aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}
\end{eqnarray*}
\end{Def}

\begin{Note}
Cuando $H_{0}$ es falsa y $H_{1}$ es verdadera, no siempre es posible especificar un valor exacto de $\mu$, sino m\'as bien un rango de posibles valores.\medskip
En lugar de arriesgarse a tomar una decisi\'on incorrecta, es mejor conlcuir que \textit{no hay evidencia suficiente para rechazar $H_{0}$}, es decir en lugar de aceptar $H_{0}$, \textit{no rechazar $H_{0}$}.

\end{Note}
%\end{frame}


%\begin{frame}


La bondad de una prueba estad\'istica se mide por el tama\~ no de $\alpha$ y $\beta$, ambas deben de ser peque\~ nas. Una manera muy efectiva de medir la potencia de la prueba es calculando el complemento del error tipo $II$:
\begin{eqnarray*}
1-\beta&= &P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{1}\textrm{ es verdadera}\right\}
\end{eqnarray*}
\begin{Def}
La \textbf{potencia de la prueba}, $1-\beta$, mide la capacidad de que la prueba funciones como se necesita.
\end{Def}
%\end{frame}



%---------------------------------------------------------
\section{Estimaci\'on por intervalos}
%---------------------------------------------------------
\subsection*{Para la media}

%\begin{frame}



Recordemos que $S^{2}$ es un estimador insesgado de $\sigma^{2}$
\begin{Def}
Sean $\hat{\theta}_{1}$ y $\hat{\theta}_{2}$ dos estimadores insesgados de $\theta$, par\'ametro poblacional. Si $\sigma_{\hat{\theta}_{1}}^{2}<\sigma_{\hat{\theta}_{2}}^{2}$, decimos que $\hat{\theta}_{1}$ un estimador m\'as eficaz de $\theta$ que $\hat{\theta}_{2}$.
\end{Def}

Algunas observaciones que es preciso realizar

\begin{enumerate}
\item[a) ]Para poblaciones normales, $\overline{X}$ y $\tilde{X}$ son estimadores insesgados de $\mu$, pero con $\sigma_{\overline{X}}^{2}<\sigma_{\tilde{X}_{2}}^{2}$.
%\end{Note}

%\begin{Note}
\item[b) ]Para las estimaciones por intervalos de $\theta$, un intervalo de la forma $\hat{\theta}_{L}<\theta<\hat{\theta}_{U}$,  $\hat{\theta}_{L}$ y $\hat{\theta}_{U}$ dependen del valor de $\hat{\theta}$.
\item[c) ]Para $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, si $n\rightarrow\infty$, entonces $\hat{\theta}\rightarrow\mu$.
%\end{Note}
\end{enumerate}

%\end{frame}



%\begin{frame}



%\begin{Note}
%Para $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, si $n\rightarrow\infty$, %entonces $\hat{\theta}\rightarrow\mu$.
%\end{Note}

%\begin{Note}
\begin{enumerate}
\item[d) ]Para $\hat{\theta}$ se determinan $\hat{\theta}_{L}$ y $\hat{\theta}_{U}$ de modo tal que 
\begin{eqnarray}
P\left\{\hat{\theta}_{L}<\hat{\theta}<\hat{\theta}_{U}\right\}=1-\alpha,
\end{eqnarray}
con $\alpha\in\left(0,1\right)$. Es decir, $\theta\in\left(\hat{\theta}_{L},\hat{\theta}_{U}\right)$ es un intervalo de confianza del $100\left(1-\alpha\right)\%$.

\item[e) ] De acuerdo con el TLC se espera que la distribuci\'on muestral de $\overline{X}$ se distribuye aproximadamente normal con media $\mu_{X}=\mu$ y desviaci\'on est\'andar $\sigma_{\overline{X}}=\frac{\sigma}{\sqrt{n}}$.

\end{enumerate}
%\end{frame}


%\begin{frame}




Para $Z_{\alpha/2}$ se tiene $P\left\{-Z_{\alpha/2}<Z<Z_{\alpha/2}\right\}=1-\alpha$, donde $Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}$. Entonces
$P\left\{-Z_{\alpha/2}<\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}<Z_{\alpha/2}\right\}=1-\alpha$ es equivalente a 
$P\left\{\overline{X}-Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}<\mu<\overline{X}+Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right\}=1-\alpha$ 

\begin{enumerate}
\item[f) ]Si $\overline{X}$ es la media muestral de una muestra de tama\~no $n$ de una poblaci\'on con varianza conocida $\sigma^{2}$, el intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\mu$ es $\mu\in\left(\overline{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}},\overline{x}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right)$.

\item[g) ] Para muestras peque\~nas de poblaciones no normales, no se puede esperar que el grado de confianza sea preciso.
\item[h) ] Para $n\geq30$, con distribuci\'on de forma no muy sesgada, se pueden tener buenos resultados.
\end{enumerate}

%\end{frame}


%\begin{frame}



\begin{Teo}
Si $\overline{X}$ es un estimador de $\mu$, podemos tener $100\left(1-\alpha\right)\%$  de confianza en que el error no exceder\'a a $z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$, error entre $\overline{X}$ y $\mu$.
\end{Teo}

\begin{Teo}
Si $\overline{X}$ es un estimador de $\mu$, podemos tener $100\left(1-\alpha\right)\%$  de confianza en que el error no exceder\'a una cantidad $e$ cuando el tama\~no de la muestra es $$n=\left(\frac{z_{\alpha/2}\sigma}{e}\right)^{2}.$$
\end{Teo}
\begin{Note}
Para intervalos unilaterales
$$P\left\{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}<Z_{\alpha}\right\}=1-\alpha$$
\end{Note}

%\end{frame}


%\begin{frame}



equivalentemente
$$P\left\{\mu<\overline{X}+Z_{\alpha}\frac{\sigma}{\sqrt{n}}\right\}=1-\alpha.$$
Si $\overline{X}$ es la media de una muestra aleatoria de tama\~no $n$  a partir de una poblaci\'on con varianza $\sigma^{2}$, los l\'imites de confianza unilaterales del   $100\left(1-\alpha\right)\%$  de confianza para $\mu$ est\'an dados por
\begin{itemize}
\item L\'imite unilateral superior: $\overline{x}+z_{\alpha}\frac{\sigma}{\sqrt{n}}$
\item L\'imite unilateral inferior: $\overline{x}-z_{\alpha}\frac{\sigma}{\sqrt{n}}$
\end{itemize}


\begin{itemize}
\item Para $\sigma$ desconocida recordar que $T=\frac{\overline{x}-\mu}{s/\sqrt{n}}\sim t_{n-1}$, donde $s$ es la desviaci\'on est\'andar de la muestra. Entonces
\begin{eqnarray*}
P\left\{-t_{\alpha/2}<T<t_{\alpha/2}\right\}=1-\alpha,\textrm{equivalentemente}\\
P\left\{\overline{X}-t_{\alpha/2}\frac{s}{\sqrt{n}}<\mu<\overline{X}+t_{\alpha/2}\frac{s}{\sqrt{n}}\right\}=1-\alpha.
\end{eqnarray*}

\item Un intervalo de confianza del $100\left(1-\alpha\right)\%$  de confianza para $\mu$, $\sigma^{2}$ desconocida y poblaci\'on normal es $\mu\in\left(\overline{x}-t_{\alpha/2}\frac{s}{\sqrt{n}},\overline{x}+t_{\alpha/2}\frac{s}{\sqrt{n}}\right)$, donde $t_{\alpha/2}$ es una $t$-student con $\nu=n-1$ grados de libertad.
\item Los l\'imites unilaterales para $\mu$ con $\sigma$ desconocida son $\overline{X}-t_{\alpha/2}\frac{s}{\sqrt{n}}$ y $\overline{X}+t_{\alpha/2}\frac{s}{\sqrt{n}}$.
\end{itemize}

%\end{frame}




%\begin{frame}



\begin{itemize}
\item Cuando la poblaci\'on no es normal, $\sigma$ desconocida y $n\geq30$, $\sigma$ se puede reemplazar por $s$ para obtener el intervalo de confianza para muestras grandes:
$$\overline{X}\pm t_{\alpha/2}\frac{s}{\sqrt{n}}.$$

\item El estimador de $\overline{X}$ de $\mu$,  $\sigma$ desconocida, la varianza de $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, el error est\'andar de $\overline{X}$ es $\sigma/\sqrt{n}$.

\item Si $\sigma$ es desconocida y la poblaci\'on es normal, $s\rightarrow\sigma$ y se incluye el error est\'andar $s/\sqrt{n}$, entonces $$\overline{x}\pm t_{\alpha/2}\frac{s}{\sqrt{n}}.$$
\end{itemize}


%\end{frame}

%---------------------------------------------------------
\subsection*{Intervalos de confianza sobre la varianza}
%---------------------------------------------------------

%\begin{frame}



Supongamos que  $X$ se distribuye normal $\left(\mu,\sigma^{2}\right)$, desconocidas. Sea $X_{1},X_{2},\ldots,X_{n}$ muestra aleatoria de tama\~no $n$ , $s^{2}$ la varianza muestral.

Se sabe que $X^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}}$ se distribuye $\chi^{2}_{n-1}$ grados de libertad. Su intervalo de confianza es
\begin{eqnarray}
\begin{array}{l}
P\left\{\chi^{2}_{1-\frac{\alpha}{2},n-1}\leq\chi^{2}\leq\chi^{2}_{\frac{\alpha}{2},n-1}\right\}=1-\alpha\\
P\left\{\chi^{2}_{1-\frac{\alpha}{2},n-1}\leq\frac{\left(n-1\right)s^{2}}{\sigma^{2}}\leq\chi^{2}_{\frac{\alpha}{2},n-1}\right\}=1-\alpha\\
P\left\{\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}}\leq\sigma^{2}\leq\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right\}=1-\alpha
\end{array}
\end{eqnarray}
es decir
%\end{frame}


%\begin{frame}




\begin{eqnarray}
\sigma^{2}\in\left[\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}},\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right]
\end{eqnarray}
los intervalos unilaterales son
\begin{eqnarray}
\sigma^{2}\in\left[\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}},\infty\right]-
\end{eqnarray}
\begin{eqnarray}
\sigma^{2}\in\left[-\infty,\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right]
\end{eqnarray}
%\end{frame}



%---------------------------------------------------------
\subsection*{Intervalos de confianza para proporciones}
%---------------------------------------------------------
%\begin{frame}



Supongamos que se tienen una muestra de tama\~no $n$ de una poblaci\'on grande pero finita, y supongamos que $X$, $X\leq n$, pertenecen a la clase de inter\'es, entonces $$\hat{p}=\frac{\overline{X}}{n}$$ es el estimador puntual de la proporci\'on de la poblaci\'on que pertenece a dicha clase.

$n$ y $p$ son los par\'ametros de la distribuci\'on binomial, entonces $\hat{p}\sim N\left(p,\frac{p\left(1-p\right)}{n}\right)$ aproximadamente si $p$ es distinto de $0$ y $1$; o si $n$ es suficientemente grande. Entonces
\begin{eqnarray*}
Z=\frac{\hat{p}-p}{\sqrt{\frac{p\left(1-p\right)}{n}}}\sim N\left(0,1\right),\textrm{aproximadamente.}
\end{eqnarray*}
 
 
entonces
\begin{eqnarray*}
1-\alpha&=&P\left\{-z_{\alpha/2}\leq\frac{\hat{p}-p}{\sqrt{\frac{p\left(1-p\right)}{n}}}\leq z_{\alpha/2}\right\}\\
&=&P\left\{\hat{p}-z_{\alpha/2}\sqrt{\frac{p\left(1-p\right)}{n}}\leq p\leq \hat{p}+z_{\alpha/2}\sqrt{\frac{p\left(1-p\right)}{n}}\right\}
\end{eqnarray*}
con $\sqrt{\frac{p\left(1-p\right)}{n}}$ error est\'andar del estimador puntual $p$. Una soluci\'on para determinar el intervalo de confianza del par\'ametro $p$ (desconocido) es
%\end{frame}


%\begin{frame}



\begin{eqnarray*}
1-\alpha=P\left\{\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\leq p\leq \hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right\}
\end{eqnarray*}
entonces los intervalos de confianza, tanto unilaterales como de dos colas son: 
\begin{itemize}
\item $p\in \left(\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}},\hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right)$

\item $p\in \left(-\infty,\hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right)$

\item $p\in \left(\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}},\infty\right)$

\end{itemize}
para minimizar el error est\'andar, se propone que el tama\~no de la muestra sea $n= \left(\frac{z_{\alpha/2}}{E}\right)^{2}p\left(1-p\right)$, donde $E=\mid p-\hat{p}\mid$.
%\end{frame}


%---------------------------------------------------------
\section{Intervalos de confianza para dos muestras}
%---------------------------------------------------------
\subsection*{Varianzas conocidas}
%---------------------------------------------------------
%\begin{frame}



Sean $X_{1}$ y $X_{2}$ variables aleatorias independientes. $X_{1}$ con media desconocida $\mu_{1}$ y varianza conocida $\sigma_{1}^{2}$; y $X_{2}$ con media desconocida $\mu_{2}$ y varianza conocida $\sigma_{2}^{2}$. Se busca encontrar un intervalo de confianza de $100\left(1-\alpha\right)\%$ de la diferencia entre medias $\mu_{1}$ y $\mu_{2}$.\medskip

Sean $X_{11},X_{12},\ldots,X_{1n_{1}}$ muestra aleatoria de $n_{1}$ observaciones de $X_{1}$, y sean $X_{21},X_{22},\ldots,X_{2n_{2}}$ muestra aleatoria de $n_{2}$ observaciones de $X_{2}$.\medskip

Sean $\overline{X}_{1}$ y $\overline{X}_{2}$, medias muestrales, entonces el estad\'sitico 
\begin{eqnarray}
Z=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}}\sim N\left(0,1\right),\end{eqnarray}
si $X_{1}$ y $X_{2}$ son normales o aproximadamente normales si se aplican las condiciones del Teorema de L\'imite Central respectivamente. 
%\end{frame}


%\begin{frame}



Entonces se tiene
\begin{eqnarray*}
1-\alpha&=& P\left\{-Z_{\alpha/2}\leq Z\leq Z_{\alpha/2}\right\}\\
&=&P\left\{-Z_{\alpha/2}\leq \frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}}\leq Z_{\alpha/2}\right\}\\
&=&P\left\{\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\leq \mu_{1}-\mu_{2}\leq\right.\\
&&\left. \left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right\}
\end{eqnarray*}

Entonces los intervalos de confianza unilaterales y de dos colas al $\left(1-\alpha\right)\%$ de confianza son 
%\end{frame}


%\begin{frame}



\begin{itemize}
\item $\mu_{1}-\mu_{2}\in \left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right]$

\item $\mu_{1}-\mu_{2}\in \left[-\infty,\left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right]$

\item $\mu_{1}-\mu_{2}\in \left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}},\infty\right]$

\end{itemize}
%\end{frame}


%\begin{frame}




\begin{Note}
Si $\sigma_{1}$ y $\sigma_{2}$ son conocidas, o por lo menos se conoce una aproximaci\'on, y los tama\~nos de las muestras $n_{1}$ y $n_{2}$ son iguales, $n_{1}=n_{2}=n$, se puede determinar el tama\~no de la muestra para que el error al estimar $\mu_{1}-\mu_{2}$ usando $\overline{X}_{1}-\overline{X}_{2}$ sea menor que $E$ (valor del error deseado) al $\left(1-\alpha\right)\%$ de confianza. El tama\~no $n$ de la muestra requerido para cada muestra es
\begin{eqnarray*}
n=\left(\frac{Z_{\alpha/2}}{E}\right)^{2}\left(\sigma_{1}^{2}+\sigma_{2}^{2}\right).
\end{eqnarray*}

\end{Note}
%\end{frame}




\subsection*{Varianzas desconocidas}


%\begin{frame}



\begin{itemize}
\item Si $n_{1},n_{2}\geq30$ se pueden utilizar los intervalos de la distribuci\'on normal para varianza conocida


\item Si $n_{1},n_{2}$ son muestras peque\~nas, supongase que las poblaciones para $X_{1}$ y $X_{2}$ son normales con varianzas desconocidas y con base en el intervalo de confianza para distribuciones $t$-student
\end{itemize}

%\end{frame}


\subsubsection*{$\sigma_{1}^{2}=\sigma_{2}^{2}=\sigma$}

%\begin{frame}
Supongamos que $X_{1}$ es una variable aleatoria con media $\mu_{1}$ y varianza $\sigma_{1}^{2}$, $X_{2}$ es una variable aleatoria con media $\mu_{2}$ y varianza $\sigma_{2}^{2}$. Todos los par\'ametros son desconocidos. Sin embargo sup\'ongase que es razonable considerar que $\sigma_{1}^{2}=\sigma_{2}^{2}=\sigma^{2}$.\medskip

Nuevamente sean $X_{1}$ y $X_{2}$ variables aleatorias independientes. $X_{1}$ con media desconocida $\mu_{1}$ y varianza muestral $S_{1}^{2}$; y $X_{2}$ con media desconocida $\mu_{2}$ y varianza muestral $S_{2}^{2}$. Dado que $S_{1}^{2}$ y $S_{2}^{2}$ son estimadores de $\sigma_{1}^{2}$, se propone el estimador $S$ de $\sigma^{2}$ como 

\begin{eqnarray*}
S_{p}^{2}=\frac{\left(n_{1}-1\right)S_{1}^{2}+\left(n_{2}-1\right)S_{2}^{2}}{n_{1}+n_{2}-2},
\end{eqnarray*}
entonces, el estad\'istico para $\mu_{1}-\mu_{2}$ es

\begin{eqnarray*}
t_{\nu}=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}}
\end{eqnarray*}
donde $t_{\nu}$ es una $t$ de student con $\nu=n_{1}+n_{2}-2$ grados de libertad.\medskip

Por lo tanto
%\end{frame}


%\begin{frame}



\begin{eqnarray*}
1-\alpha=P\left\{-t_{\alpha/2,\nu}\leq t\leq t_{\alpha/2,\nu}\right\}\\
=P\left\{\left(\overline{X}_{1}-\overline{X}_{2}\right)-t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\leq \right.\\
\left.t\leq\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right\}
\end{eqnarray*}

luego, los intervalos de confianza del $\left(1-\alpha\right)\%$ para $\mu_{1}-|mu_{2}$ son 
\begin{itemize}
\item $\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)- t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right]$


\item $\mu_{1}-\mu_{2}\in\left[-\infty,\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right]$

\item $\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)- t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}},\infty\right]$


\end{itemize}

%\end{frame}



\subsubsection*{$\sigma_{1}^{2}\neq\sigma_{2}^{2}$}
%\begin{frame}




Si no se tiene certeza de que $\sigma_{1}^{2}=\sigma_{2}^{2}$, se propone el estad\'istico
\begin{eqnarray}
t^{*}=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}}}
\end{eqnarray}
que se distribuye $t$-student con $\nu$ grados de libertad, donde

\begin{eqnarray*}
\nu=\frac{\left(\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}\right)^{2}}{\frac{S_{1}^{2}/n_{1}}{n_{1}+1}+\frac{S_{2}^{2}/n_{2}}{n_{2}+1}}-2
\end{eqnarray*}


Entonces el intervalo de confianza de aproximadamente el $100\left(1-\alpha\right)\%$ para $\mu_{1}-\mu_{2}$ con $\sigma_{1}^{2}\neq\sigma_{2}^{2}$ es
\begin{eqnarray*}
\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-t_{\alpha/2,\nu}\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}},\right.\\
\left.\left(\overline{X}_{1}-\overline{X}_{2}\right)+t_{\alpha/2,\nu}\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}}\right]
\end{eqnarray*}

%\end{frame}




\section{Intervalos de confianza para raz\'on de Varianzas}

%\begin{frame}



Supongamos que se toman dos muestras aleatorias independientes de las dos poblaciones de inter\'es.\medskip

Sean $X_{1}$ y $X_{2}$ variables normales independientes con medias desconocidas $\mu_{1}$ y $\mu_{2}$ y varianzas desconocidas $\sigma_{1}^{2}$ y $\sigma_{2}^{2}$ respectivamente. Se busca un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\sigma_{1}^{2}/\sigma_{2}^{2}$.\medskip
Supongamos $n_{1}$ y $n_{2}$ muestras aleatorias de $X_{1}$ y $X_{2}$ y sean $S_{1}^{2}$ y $S_{2}^{2}$ varianzas muestralres. Se sabe que 
$$F=\frac{S_{2}^{2}/\sigma_{2}^{2}}{S_{1}^{2}/\sigma_{1}^{2}}$$
se distribuye $F$ con $n_{2}-1$ y $n_{1}-1$ grados de libertad.


Por lo tanto
\begin{eqnarray*}
P\left\{F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq F\leq F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha\\
P\left\{F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq \frac{S_{2}^{2}/\sigma_{2}^{2}}{S_{1}^{2}/\sigma_{1}^{2}}\leq F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha
\end{eqnarray*}
por lo tanto
\begin{eqnarray*}
P\left\{\frac{S_{1}^{2}}{S_{2}^{2}}F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq \frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\leq \frac{S_{1}^{2}}{S_{2}^{2}}F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha\\
\end{eqnarray*}
entonces

%\end{frame}


%\begin{frame}



\begin{eqnarray*}
\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\in \left[\frac{S_{1}^{2}}{S_{2}^{2}}F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}, \frac{S_{1}^{2}}{S_{2}^{2}}F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right]
\end{eqnarray*}
donde
\begin{eqnarray*}
F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}=\frac{1}{F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}}
\end{eqnarray*}

%\end{frame}



\section{Intervalos de confianza para diferencia de proporciones}

%\begin{frame}



Sean dos proporciones de inter\'es $p_{1}$ y $p_{2}$. Se busca un intervalo para $p_{1}-p_{2}$ al $100\left(1-\alpha\right)\%$.\medskip

Sean dos muestras independientes de tama\~no $n_{1}$ y $n_{2}$ de poblaciones infinitas de modo que $X_{1}$ y $X_{2}$ variables aleatorias binomiales independientes con par\'ametros $\left(n_{1},p_{1}\right)$ y $\left(n_{2},p_{2}\right)$.\medskip

$X_{1}$ y $X_{2}$ son  el n\'umero de observaciones que pertenecen a la clase de inter\'es correspondientes. Entonces $\hat{p}_{1}=\frac{X_{1}}{n_{1}}$ y $\hat{p}_{2}=\frac{X_{2}}{n_{2}}$ son estimadores de $p_{1}$ y $p_{2}$ respectivamente. Supongamos que se cumple la aproximaci\'on  normal a la binomial, entonces




\begin{eqnarray*}
Z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)-\left(p_{1}-p_{2}\right)}{\sqrt{\frac{p_{1}\left(1-p_{1}\right)}{n_{1}}-\frac{p_{2}\left(1-p_{2}\right)}{n_{2}}}}\sim N\left(0,1\right)\textrm{aproximadamente}
\end{eqnarray*}
entonces

\begin{eqnarray*}
\left(\hat{p}_{1}-\hat{p}_{2}\right)-Z_{\frac{\alpha}{2}}\sqrt{\frac{\hat{p}_{1}\left(1-\hat{p}_{1}\right)}{n_{1}}+\frac{\hat{p}_{2}\left(1-\hat{p}_{2}\right)}{n_{2}}}\leq p_{1}-p_{2}\\
\leq\left(\hat{p}_{1}-\hat{p}_{2}\right)+Z_{\frac{\alpha}{2}}\sqrt{\frac{\hat{p}_{1}\left(1-\hat{p}_{1}\right)}{n_{1}}-\frac{\hat{p}_{2}\left(1-\hat{p}_{2}\right)}{n_{2}}}
\end{eqnarray*}
%\end{frame}


%\begin{frame}
\begin{itemize}
\item Una hip\'otesis estad\'istica es una afirmaci\'on  acerca de la distribuci\'on de probabilidad de una variable aleatoria, a menudo involucran uno o m\'as par\'ametros de la distribuci\'on.

\item Las hip\'otesis son afirmaciones respecto a la poblaci\'on o distribuci\'on bajo estudio, no en torno a la muestra.

\item La mayor\'ia de las veces, la prueba de hip\'otesis consiste en determinar si la situaci \'on experimental ha cambiado

\item el inter\'es principal es decidir sobre la veracidad o falsedad de una hip\'otesis, a este procedimiento se le llama \textit{prueba de hip\'otesis}.

\item Si la informaci\'on es consistente con la hip\'otesis, se concluye que esta es verdadera, de lo contrario que con base en la informaci\'on, es falsa.

\end{itemize}
%\end{frame}


%---------------------------------------------------------
\section{2. Pruebas de Hip\'otesis}
%---------------------------------------------------------
\subsection{2.1 Tipos de errores}

%\begin{frame}
Una prueba de hip\'otesis est\'a formada por cinco partes
\begin{itemize}
\item La hip\'otesis nula, denotada por $H_{0}$.
\item La hip\'otesis alterativa, denorada por $H_{1}$.
\item El estad\'sitico de prueba y su valor $p$.
\item La regi\'on de rechazo.
\item La conclusi\'on.

\end{itemize}

\begin{Def}
Las dos hip\'otesis en competencias son la \textbf{hip\'otesis alternativa $H_{1}$}, usualmente la que se desea apoyar, y la \textbf{hip\'otesis nula $H_{0}$}, opuesta a $H_{1}$.
\end{Def}
%\end{frame}



%\begin{frame}
En general, es m\'as f\'acil presentar evidencia de que $H_{1}$ es cierta, que demostrar 	que $H_{0}$ es falsa, es por eso que por lo regular se comienza suponiendo que $H_{0}$ es cierta, luego se utilizan los datos de la muestra para decidir si existe evidencia a favor de $H_{1}$, m\'as que a favor de $H_{0}$, as\'i se tienen dos conclusiones:
\begin{itemize}
\item Rechazar $H_{0}$ y concluir que $H_{1}$ es verdadera.
\item Aceptar, no rechazar, $H_{0}$ como verdadera.

\end{itemize}
%\end{frame}



%\begin{frame}
\begin{Ejem}
Se desea demostrar que el salario promedio  por hora en cierto lugar es distinto de $19$usd, que es el promedio nacional. Entonces $H_{1}:\mu\neq19$, y $H_{0}:\mu=19$.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de dos colas}.


\begin{Ejem}
Un determinado proceso produce un promedio de $5\%$ de piezas defectuosas. Se est\'a interesado en demostrar que un simple ajuste en una m\'aquina reducir\'a $p$, la proporci\'on de piezas defectuosas producidas en este proceso. Entonces se tiene $H_{0}:p<0.3$ y $H_{1}:p=0.03$. Si se puede rechazar $H_{0}$, se concluye que el proceso ajustado produce menos del $5\%$ de piezas defectuosas.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de una cola}.

La decisi\'on de rechazar o aceptar la hip\'otesis nula est\'a basada en la informaci\'on contenida en una muestra proveniente de la poblaci\'on de inter\'es. Esta informaci\'on tiene estas formas
%\end{frame}



%\begin{frame}
\begin{itemize}
\item \textbf{Estad\'sitico de prueba:} un s\'olo n\'umero calculado a partir de la muestra.

\item \textbf{$p$-value:} probabilidad calculada a partir del estad\'stico de prueba.

\end{itemize}

\begin{Def}
El $p$-value es la probabilidad de observar un estad\'istico de prueba tanto o m\'as alejado del valor obervado, si en realidad $H_{0}$ es verdadera.\medskip
Valores grandes del estad\'stica de prueba  y valores peque\~nos de $p$ significan que se ha observado un evento muy poco probable, si $H_{0}$ en realidad es verdadera.
\end{Def}
%\end{frame}



%\begin{frame}
Todo el conjunto de valores que puede tomar el estad\'istico de prueba se divide en dos regiones. Un conjunto, formado de valores que apoyan la hip\'otesis alternativa y llevan a rechazar $H_{0}$, se denomina \textbf{regi\'on de rechazo}. El otro, conformado por los valores que sustentatn la hip\'otesis nula, se le denomina \textbf{regi\'on de aceptaci\'on}.\medskip


Cuando la regi\'on de rechazo est\'a en la cola izquierda de la distribuci\'on, la  prueba se denomina \textbf{prueba lateral izquierda}. Una prueba con regi\'on de rechazo en la cola derecha se le llama \textbf{prueba lateral derecha}.\medskip

Si el estad\'stico de prueba cae en la regi\'on de rechazo, entonces se rechaza $H_{0}$. Si el estad\'stico de prueba cae en la regi\'on de aceptaci\'on, entonces la hip\'otesis nula se acepta o la prueba se juzga como no concluyente.\medskip
%\end{frame}



%\begin{frame}
Dependiendo del nivel de confianza que se desea agregar a las conclusiones de la prueba, y el \textbf{nivel de significancia $\alpha$}, el riesgo que est\'a dispuesto a correr si se toma una decisi\'on incorrecta.

\begin{Def}
Un \textbf{error de tipo I} para una prueba estad\'istica es el error que se tiene al rechazar la hip\'otesis nula cuando es verdadera. El \textbf{nivel de significancia} para una prueba estad\'istica de hip\'otesis es
\begin{eqnarray*}
\alpha&=&P\left\{\textrm{error tipo I}\right\}=P\left\{\textrm{rechazar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es verdadera}\right\}
\end{eqnarray*}

\end{Def}
%\end{frame}



%\begin{frame}
Este valor $\alpha$ representa el valor m\'aximo de riesgo tolerable de rechazar incorrectamente $H_{0}$. Una vez establecido el nivel de significancia, la regi\'on de rechazo se define para poder determinar si se rechaza $H_{0}$ con un cierto nivel de confianza.

%\end{frame}



\section{2.2 Muestras grandes: una media poblacional}
\subsection{2.2.1 C\'alculo de valor $p$}
%\begin{frame}

\begin{Def}
El \textbf{valor de $p$} (\textbf{$p$-value}) o nivel de significancia observado de un estad\'istico de prueba es el valor m\'as peque\~ no de $\alpha$ para el cual $H_{0}$ se puede rechazar. El riesgo de cometer un error tipo $I$, si $H_{0}$ es rechazada con base en la informaci\'on que proporciona la muestra.
\end{Def}

\begin{Note}
Valores peque\~ nos de $p$ indican 	que el valor observado del estad\'stico de prueba se encuentra alejado del valor hipot\'etico de $\mu$, es decir se tiene evidencia de que $H_{0}$ es falsa y por tanto debe de rechazarse.
\end{Note}
%\end{frame}


%\begin{frame}
\begin{Note}
Valores grandes de $p$ indican que el estad\'istico de prueba observado no est\'a alejado de la medi hipot\'etica y no apoya el rechazo de $H_{0}$.
\end{Note}

\begin{Def}
Si el valor de $p$ es menor o igual que el nivel de significancia $\alpha$, determinado previamente, entonces $H_{0}$ es rechazada y se puede concluir que los resultados son estad\'isticamente significativos con un nivel de confianza del $100\left(1-\alpha\right)\%$.
\end{Def}
Es usual utilizar la siguiente clasificaci\'on de resultados
%\end{frame}


%\begin{frame}
\begin{tabular}{|c||c|l|}\hline
$p$& $H_{0}$&Significativa\\\hline\hline
$p<0.01$&Rechazar &Altamente\\\hline
$0.01\leq p<0.05$ & Rechazar&Estad\'isticamente\\\hline
$0.05\leq p <0.1$ & No rechazar & Tendencia estad\'istica\\\hline
$0.01\leq p$ & No rechazar & No son estad\'isticamente\\\hline
\end{tabular}

\begin{Note}
Para determinar el valor de $p$, encontrar el \'area en la cola despu\'es del estad\'istico de prueba. Si la prueba es de una cola, este es el valor de $p$. Si es de dos colas, \'este valor encontrado es la mitad del valor de $p$. Rechazar $H_{0}$ cuando el valor de $p<\alpha$.
\end{Note}
%\end{frame}


%\begin{frame}
Hay dos tipos de errores al realizar una prueba de hip\'otesis
\begin{center}
\begin{tabular}{c|cc}
& $H_{0}$ es Verdadera & $H_{0}$ es Falsa\\\hline\hline
Rechazar $H_{0}$ & Error tipo I & $\surd$\\
Aceptar $H_{0}$ & $\surd$ & Error tipo II
\end{tabular}
\end{center}
\begin{Def}
La probabilidad de cometer el error tipo II se define por $\beta$ donde
\begin{eqnarray*}
\beta&=&P\left\{\textrm{error tipo II}\right\}=P\left\{\textrm{Aceptar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{Aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}
\end{eqnarray*}
\end{Def}
%\end{frame}


%\begin{frame}

\begin{Note}
Cuando $H_{0}$ es falsa y $H_{1}$ es verdadera, no siempre es posible especificar un valor exacto de $\mu$, sino m\'as bien un rango de posibles valores.\medskip
En lugar de arriesgarse a tomar una decisi\'on incorrecta, es mejor conlcuir que \textit{no hay evidencia suficiente para rechazar $H_{0}$}, es decir en lugar de aceptar $H_{0}$, \textit{no rechazar $H_{0}$}.

\end{Note}
La bondad de una prueba estad\'istica se mide por el tama\~ no de $\alpha$ y $\beta$, ambas deben de ser peque\~ nas. Una manera muy efectiva de medir la potencia de la prueba es calculando el complemento del error tipo $II$:
\begin{eqnarray*}
1-\beta&= &P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{1}\textrm{ es verdadera}\right\}
\end{eqnarray*}
\begin{Def}
La \textbf{potencia de la prueba}, $1-\beta$, mide la capacidad de que la prueba funciona como se necesita.
\end{Def}
%\end{frame}


%\begin{frame}

\begin{Ejem}
La producci\'on diariade una planta qu\'imica local ha promediado 880 toneladas en los \'ultimos a\~nos. A la gerente de control de calidad le gustar\'ia saber si este promedio ha cambiado en meses recientes. Ella selecciona al azar 50 d\'ias de la base de datos computarizada y calcula el promedio y la desviaci\'on est\'andar de las $n=50$  producciones como $\overline{x}=871$ toneladas y $s=21$ toneladas, respectivamente. Pruebe la hip\'otesis  apropiada usando $\alpha=0.05$.

\end{Ejem}

\begin{Sol}
La hip\'otesis nula apropiada es:
\begin{eqnarray*}
H_{0}&:& \mu=880\\
&&\textrm{ y la hip\'otesis alternativa }H_{1}\textrm{ es }\\
H_{1}&:& \mu\neq880
\end{eqnarray*}
el estimador puntual para $\mu$ es $\overline{x}$, entonces el estad\'istico de prueba es\medskip
\begin{eqnarray*}
z&=&\frac{\overline{x}-\mu_{0}}{s/\sqrt{n}}\\
&=&\frac{871-880}{21/\sqrt{50}}=-3.03
\end{eqnarray*}
\end{Sol}
%\end{frame}


%\begin{frame}
\begin{Sol}
Para esta prueba de  dos colas, hay que determinar los dos valores de $z_{\alpha/2}$, es decir,  $z_{\alpha/2}=\pm1.96$, como $z>z_{\alpha/2}$, $z$ cae en la zona de rechazo, por lo tanto  la gerente puede rechazar la hip\'otesis nula y concluir que el promedio efectivamente ha cambiado.\medskip
La probabilidad de rechazar $H_{0}$ cuando esta es verdadera es de  $0.05$.


Recordemos que el valor observado del estad\'istico de prueba es $z=-3.03$, la regi\'on de rechazo m\'as peque\~na que puede usarse y todav\'ia seguir rechazando $H_{0}$ es $|z|>3.03$, \\
entonces $p=2(0.012)=0.0024$, que a su vez es menor que el nivel de significancia $\alpha$ asignado inicialmente, y adem\'as los resultados son  \textbf{altamente significativos}.


\end{Sol}
%\end{frame}


%\begin{frame}

Finalmente determinemos la potencia de la prueba cuando $\mu$ en realidad es igual a $870$ toneladas.

Recordar que la regi\'on de aceptaci\'on est\'a entre $-1.96$ y $1.96$, para $\mu=880$, equivalentemente $$874.18<\overline{x}<885.82$$
$\beta$ es la probabilidad de aceptar $H_{0}$ cuando $\mu=870$, calculemos los valores de $z$ correspondientes a $874.18$ y $885.82$ \medskip
Entonces
\begin{eqnarray*}
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{874.18-870}{21/\sqrt{50}}=1.41\\
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{885.82-870}{21/\sqrt{50}}=5.33
\end{eqnarray*}
por lo tanto
\begin{eqnarray*}
\beta&=&P\left\{\textrm{aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{874.18<\mu<885.82\textrm{ cuando }\mu=870\right\}\\
&=&P\left\{1.41<z<5.33\right\}=P\left\{1.41<z\right\}\\
&=&1-0.9207=0.0793
\end{eqnarray*}
entonces, la potencia de la prueba es
$$1-\beta=1-0.0793=0.9207$$ que es la probabilidad de rechazar correctamente $H_{0}$ cuando $H_{0}$ es falsa.
%\end{frame}


%\begin{frame}
Determinar la potencia de la prueba para distintos valores de $H_{1}$ y graficarlos, \textit{curva de potencia}
\begin{center}
\begin{tabular}{c||c}
$H_{1}$ & $\left(1-\beta\right)$ \\\hline 
\hline 
865 &  \\ \hline 
870 &  \\ \hline 
872 &  \\ \hline 
875 &  \\ \hline 
877 &  \\ \hline 
880 &  \\ \hline 
883 &  \\ \hline 
885 &  \\ \hline 
888 &  \\ \hline 
890 &  \\ \hline 
895 &  \\ \hline 
\end{tabular} 

\end{center}
%\end{frame}


%\begin{frame}
\begin{enumerate}
\item Encontrar las regiones de rechazo para el estad\'istico $z$, para una prueba de
\begin{itemize}
\item[a) ]  dos colas para $\alpha=0.01,0.05,0.1$
\item[b) ]  una cola superior para $\alpha=0.01,0.05,0.1$
\item[c) ] una cola inferior para $\alpha=0.01,0.05,0.1$

\end{itemize}


\item Suponga que el valor del estad\'istico de prueba es 
\begin{itemize}
\item[a) ]$z=-2.41$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[b) ] $z=2.16$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[c) ] $z=1.15$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[d) ] $z=-2.78$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[e) ] $z=-1.81$, sacar las conclusiones correspondientes para los incisos anteriores.

\end{itemize}
\end{enumerate}
%\end{frame}



%\begin{frame}
\begin{itemize}
\item[3. ] Encuentre el valor de $p$ para las pruebas de hip\'otesis correspondientes a los valores de $z$ del ejercicio anterior.

\item[4. ] Para las pruebas dadas en el ejercicio 2, utilice el valor de $p$, determinado en el ejercicio 3,  para determinar la significancia de los resultados.


\end{itemize}


\begin{itemize}
\item[5. ] Una muestra aleatoria de $n=45$ observaciones de una poblaci\'on con media $\overline{x}=2.4$, y desviaci\'on est\'andar $s=0.29$. Suponga que el objetivo es demostrar que la media poblacional $\mu$ excede $2.3$.
\begin{itemize}
\item[a) ] Defina la hip\'otesis nula y alternativa para la prueba.
\item[b) ] Determine la regi\'on de rechazo para un nivel de significancia de: $\alpha=0.1,0.05,0.01$.
\item[c) ] Determine el error est\'andar de la media muestral.
\item[d) ] Calcule el valor de $p$ para los estad\'isticos de prueba definidos en los incisos anteriores.
\item[e) ] Utilice el valor de $p$ pra sacar una conclusi\'on al nivel de significancia $\alpha$.
\item[f) ] Determine el valor de $\beta$ cuando $\mu=2.5$
\item[g) ] Graficar la curva de potencia para la prueba.

\end{itemize}
\end{itemize}
%\end{frame}

\subsection{2.2.2 Prueba de hip\'otesis para la diferencia entre dos medias poblacionales}

%\begin{frame}

El estad\'istico que resume la informaci\'on muestral respecto a la diferencia en medias poblacionales $\left(\mu_{1}-\mu_{2}\right)$ es la diferencia de las medias muestrales $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, por tanto al probar la difencia entre las medias muestrales se verifica que la diferencia real entre las medias poblacionales difiere de un valor especificado, $\left(\mu_{1}-\mu_{2}\right)=D_{0}$, se puede usar el error est\'andar de $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, es decir
$$\sqrt{\frac{\sigma^{2}_{1}}{n_{1}}+\frac{\sigma^{2}_{2}}{n_{2}}}$$
cuyo estimador est\'a dado por
$$SE=\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}$$
El procedimiento para muestras grandes es:
%\end{frame}

%\begin{frame}
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}

\end{itemize}

%\end{frame}

%\begin{frame}
\begin{itemize}
\item[3) ] Estad\'istico de prueba:
$$z=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}$$
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}


\end{itemize}

%\end{frame}

%\begin{frame}

\begin{Ejem}
Para determinar si ser propietario de un autom\'ovil afecta el rendimiento acad\'emico de un estudiante, se tomaron dos muestras aleatorias de 100 estudiantes varones. El promedio de calificaciones para los $n_{1}=100$ no propietarios de un auto tuvieron un promedio y varianza de $\overline{x}_{1}=2.7$ y $s_{1}^{2}=0.36$, respectivamente, mientras que para para la segunda muestra con $n_{2}=100$ propietarios de un auto, se tiene $\overline{x}_{2}=2.54$ y $s_{2}^{2}=0.4$. Los datos presentan suficiente evidencia para indicar una diferencia en la media en el rendimiento acad\'emico entre propietarios y no propietarios de un autom\'ovil? Hacer pruebas para $\alpha=0.01,0.05$ y $\alpha=0.1$.
\end{Ejem}

\begin{Sol}
\begin{itemize}
\item Soluci\'on utilizando la t\'ecnica de regiones de rechazo:\medskip
realizando las operaciones
$z=1.84$, determinar si excede los valores de $z_{\alpha/2}$.
\item Soluci\'on utilizando el $p$-value:\medskip
Calcular el valor de $p$, la probabilidad de que $z$ sea mayor que $z=1.84$ o menor que $z=-1.84$, se tiene que $p=0.0658$. Concluir.
\end{itemize}
\end{Sol}
%\end{frame}

%\begin{frame}
\begin{itemize}
\item Si el intervalo de confianza que se construye contiene el valor del par\'ametro especificado por $H_{0}$, entonces ese valor es uno de los posibles valores del par\'ametro y $H_{0}$ no debe ser rechazada.

\item Si el valor hipot\'etico se encuentra fuera de los l\'imites de confianza, la hip\'otesis nula es rechazada al nivel de significancia $\alpha$.
\end{itemize}

\begin{enumerate}
\item Del libro Mendenhall resolver los ejercicios 9.18, 9.19 y 9.20(\href{https://cu.uacm.edu.mx/nextcloud/index.php/f/202873}{Mendenhall}).

\item Del libro \href{https://cu.uacm.edu.mx/nextcloud/index.php/f/202873}{Mendenhall} resolver los ejercicios: 9.23, 9.26 y 9.28.
\end{enumerate}

%\end{frame}

\subsection{2.2.3 Prueba de Hip\'otesis para una Proporci\'on Binomial}

%\begin{frame}
Para una muestra aleatoria de $n$ intentos id\'enticos, de una poblaci\'on binomial, la proporci\'on muesrtal $\hat{p}$ tiene una distribuci\'on aproximadamente normal cuando $n$ es grande, con media $p$ y error est\'andar
$$SE=\sqrt{\frac{pq}{n}}.$$
La prueba de hip\'otesis de la forma
\begin{eqnarray*}
H_{0}&:&p=p_{0}\\
H_{1}&:&p>p_{0}\textrm{, o }p<p_{0}\textrm{ o }p\neq p_{0}
\end{eqnarray*}
El estad\'istico de prueba se construye con el mejor estimador de la proporci\'on verdadera, $\hat{p}$, con el estad\'istico de prueba $z$, que se distribuye normal est\'andar.
%\end{frame}

%\begin{frame}
El procedimiento es
\begin{itemize}
\item[1) ] Hip\'otesis nula: $H_{0}:p=p_{0}$
\item[2) ] Hip\'otesis alternativa
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:p>p_{0}$ & $p\neq p_{0}$\\
$H_{1}:p<p_{0}$ & \\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
z=\frac{\hat{p}-p_{0}}{\sqrt{\frac{pq}{n}}},\hat{p}=\frac{x}{n}
\end{eqnarray*}
donde $x$ es el n\'umero de \'exitos en $n$ intentos binomiales.

\end{itemize}
%\end{frame}

%\begin{frame}
\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}
%\end{frame}

%\begin{frame}
\begin{Ejem}
A cualquier edad, alrededor del $20\%$ de los adultos de cierto pa\'is realiza actividades de acondicionamiento f\'isico al menos dos veces por semana. En una encuesta local de $n=100$ adultos de m\'as de $40$ a\ ~nos, un total de 15 personas indicaron que realizaron actividad f\'isica al menos dos veces por semana. Estos datos indican que el porcentaje de participaci\'on para adultos de m\'as de 40 a\ ~nos de edad es  considerablemente menor a la cifra del $20\%$? Calcule el valor de $p$ y \'uselo para sacar las conclusiones apropiadas.
\end{Ejem}

\begin{enumerate}
\item Resolver los ejercicios: 9.30, 9.32, 9.33, 9.35 y 9.39.
\end{enumerate}
%\end{frame}


\subsection{2.2.4 Prueba de Hip\'otesis diferencia entre dos Proporciones Binomiales}


%\begin{frame}

\begin{Note}
Cuando se tienen dos muestras aleatorias independientes de dos poblaciones binomiales, el objetivo del experimento puede ser la diferencia $\left(p_{1}-p_{2}\right)$ en las proporciones de individuos u objetos que poseen una caracter\'istica especifica en las dos poblaciones. En este caso se pueden utilizar los estimadores de las dos proporciones $\left(\hat{p}_{1}-\hat{p}_{2}\right)$ con error est\'andar dado por
$$SE=\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}$$
considerando el estad\'istico $z$ con un nivel de significancia $\left(1-\alpha\right)100\%$

\end{Note}


\begin{Note}
La hip\'otesis nula a probarse es de la forma
\begin{itemize}
\item[$H_{0}$: ] $p_{1}=p_{2}$ o equivalentemente $\left(p_{1}-p_{2}\right)=0$, contra una hip\'otesis alternativa $H_{1}$ de una o dos colas.
\end{itemize}
\end{Note}
%\end{frame}


%\begin{frame}

\begin{Note}
Para estimar el error est\'andar del estad\'istico $z$, se debe de utilizar el hecho de que suponiendo que $H_{0}$ es verdadera, las dos proporciones son iguales a alg\'un valor com\'un, $p$. Para obtener el mejor estimador de $p$ es
$$p=\frac{\textrm{n\'umero total de \'exitos}}{\textrm{N\'umero total de pruebas}}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$$
\end{Note}



\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\left(p_{1}-p_{2}\right)=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(p_{1}-p_{2}\right)>0$ & $H_{1}:\left(p_{1}-p_{2}\right)\neq 0$\\ 
$H_{1}:\left(p_{1}-p_{2}\right)<0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}}=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{pq}{n_{1}}+\frac{pq}{n_{2}}}}
\end{eqnarray*}
donde $\hat{p_{1}}=x_{1}/n_{1}$ y $\hat{p_{2}}=x_{2}/n_{2}$ , dado que el valor com\'un para $p_{1}$ y $p_{2}$ es $p$, entonces $\hat{p}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$ y por tanto el estad\'istico de prueba es
\end{itemize}
%\end{frame}


%\begin{frame}


\begin{eqnarray*}
z=\frac{\hat{p}_{1}-\hat{p}_{2}}{\sqrt{\hat{p}\hat{q}}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}
\end{eqnarray*}
\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{\alpha}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}

\end{itemize}
%\end{frame}


%\begin{frame}

\begin{Ejem}
Los registros de un hospital, indican que 52 hombres de una muestra de 1000 contra 23 mujeres de una muestra de 1000 fueron ingresados por enfermedad del coraz\'on. Estos datos presentan suficiente evidencia para indicar un porcentaje m\'as alto de enfermedades del coraz\'on entre hombres ingresados al hospital?, utilizar distintos niveles de confianza de $\alpha$.

\end{Ejem}
\begin{enumerate}
\item Resolver los ejercicios 9.42

\item Resolver los ejercicios: 9.45, 9.48, 9.50
\end{enumerate}

%\end{frame}



\section{2.3 Muestras Peque\~nas}

\subsection{2.3.1 Una media poblacional}


%\begin{frame}

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu=\mu_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu>\mu_{0}$ & $H_{1}:\mu\neq \mu_{0}$\\ 
$H_{1}:\mu<\mu0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
t=\frac{\overline{x}-\mu_{0}}{\sqrt{\frac{s^{2}}{n}}}
\end{eqnarray*}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}
%\end{frame}


%\begin{frame}

\begin{Ejem}
Las etiquetas en latas de un gal'on de pintura por lo general indican el tiempo de secado y el \'area puede cubrir una capa. Casi todas las marcas de pintura indican que, en una capa, un gal\'on cubrir\'a entre 250 y 500 pies cuadrados, dependiento de la textura de la superficie a pintarse, un fabricante, sin embargo afirma que un gal\'on de su pintura cubrir\'a 400 pies cuadrados de \'area superficial. Para probar su afirmaci\'on, una muestra aleatoria de 10 latas de un gal\'on de pintura blanca se emple\'o para pintar 10 \'areas id\'enticas usando la misma clase de equipo. Las \'areas reales en pies cuadrados cubiertas por estos 10 galones de pintura se dan a continuac\'on:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
310 & 311 & 412 & 368 & 447 \\ 
\hline 
376 & 303 &410 &365 & 350 \\ 
\hline 
\end{tabular} 
\end{center}
\end{Ejem}
%\end{frame}


%\begin{frame}

\begin{Ejem}
Los datos presentan suficiente evidencia para indicar que el promedio de la cobertura difiere de 400 pies cuadrados? encuentre el valor de $p$ para la prueba y \'uselo para evaluar la significancia de los resultados.
\end{Ejem}
\begin{enumerate}
\item Resolver los ejercicios: 10.2, 10.3,10.5, 10.7, 10.9, 10.13 y 10.16
\end{enumerate}

%\end{frame}


\subsection{2.3.2 Diferencia entre dos medias poblacionales: M.A.I.}


%\begin{frame}

\begin{Note}
Cuando los tama\ ~nos de muestra son peque\ ~nos, no se puede asegurar que las medias muestrales sean normales, pero si las poblaciones originales son normales, entonces la distribuci\'on muestral de la diferencia de las medias muestales, $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, ser\'a normal con media $\left(\mu_{1}-\mu_{2}\right)$ y error est\'andar $$ES=\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}$$

\end{Note}

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}

\item[3) ] Estad\'istico de prueba:
$$t=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}$$
\end{itemize}
%\end{frame}


%\begin{frame}


donde $$s^{2}=\frac{\left(n_{1}-1\right)s_{1}^{2}+\left(n_{2}-1\right)s_{2}^{2}}{n_{1}+n_{2}-2}$$
\begin{itemize}

\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.


\end{itemize}

%\end{frame}


\subsection{2.3.3 Diferencia entre dos medias poblacionales: Diferencias Pareadas}



%\begin{frame}


\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu_{d}=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:\mu_{d}$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu_{d}>0$ & $H_{1}:\mu_{d}\neq 0$\\ 
$H_{1}:\mu_{d}<0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
t=\frac{\overline{d}}{\sqrt{\frac{s_{d}^{2}}{n}}}
\end{eqnarray*}
donde $n$ es el n\'umero de diferencias pareadas, $\overline{d}$ es la media de las diferencias muestrales, y $s_{d}$ es la desviaci\'on est\'andar de las diferencias muestrales.

\end{itemize}

%\end{frame}


%\begin{frame}

\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}

Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.

\end{itemize}


%\end{frame}



\subsection{2.3.4 Inferencias con respecto a la Varianza Poblacional}


%\begin{frame}

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\sigma^{2}=\sigma^{2}_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\sigma^{2}>\sigma^{2}_{0}$ & $H_{1}:\sigma^{2}\neq \sigma^{2}_{0}$\\ 
$H_{1}:\sigma^{2}<\sigma^{2}_{0}$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
\chi^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}_{0}}
\end{eqnarray*}

\end{itemize}

%\end{frame}


%\begin{frame}

\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$\chi^{2}>\chi^{2}_{\alpha}$ & \\
$\chi^{2}<\chi^{2}_{\left(1-\alpha\right)}$ cuando $H_{1}:\chi^{2}<\chi^{2}_{0}$&$\chi^{2}>\chi^{2}_{\alpha/2}$ o $\chi^{2}<\chi^{2}_{\left(1-\alpha/2\right)}$\\
 cuando $p<\alpha$&\\
\end{tabular}

Los valores cr\'iticos de $\chi^{2}$,est\'an basados en $\left(n_{1}+\right)$ grados de libertad.

\end{itemize}


%\end{frame}


\subsection{2.3.5 Comparaci\'on de dos varianzas poblacionales}



%\begin{frame}

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)>D_{0}$ & $H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)<D_{0}$&\\
\end{tabular}

\end{itemize}

%\end{frame}


%\begin{frame}


\begin{itemize}
\item[3) ] Estad\'istico de prueba:
$$F=\frac{s_{1}^{2}}{s_{2}^{2}}$$
donde $s_{1}^{2}$ es la varianza muestral m\'as grande.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$F>F_{\alpha}$ & $F>F_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}


\end{itemize}

%\end{frame}


%---------------------------------------------------------
\section{2. Pruebas de Hip\'otesis}
%---------------------------------------------------------
\subsection{2.1 Tipos de errores}


%\begin{frame}


\begin{itemize}
\item Una hip\'otesis estad\'istica es una afirmaci\'on  acerca de la distribuci\'on de probabilidad de una variable aleatoria, a menudo involucran uno o m\'as par\'ametros de la distribuci\'on.

\item Las hip\'otesis son afirmaciones respecto a la poblaci\'on o distribuci\'on bajo estudio, no en torno a la muestra.

\item La mayor\'ia de las veces, la prueba de hip\'otesis consiste en determinar si la situaci \'on experimental ha cambiado

\item el inter\'es principal es decidir sobre la veracidad o falsedad de una hip\'otesis, a este procedimiento se le llama \textit{prueba de hip\'otesis}.

\item Si la informaci\'on es consistente con la hip\'otesis, se concluye que esta es verdadera, de lo contrario que con base en la informaci\'on, es falsa.

\end{itemize}


%\end{frame}


%\begin{frame}

Una prueba de hip\'otesis est\'a formada por cinco partes
\begin{itemize}
\item La hip\'otesis nula, denotada por $H_{0}$.
\item La hip\'otesis alterativa, denorada por $H_{1}$.
\item El estad\'sitico de prueba y su valor $p$.
\item La regi\'on de rechazo.
\item La conclusi\'on.

\end{itemize}


\begin{Def}
Las dos hip\'otesis en competencias son la \textbf{hip\'otesis alternativa $H_{1}$}, usualmente la que se desea apoyar, y la \textbf{hip\'otesis nula $H_{0}$}, opuesta a $H_{1}$.
\end{Def}


%\end{frame}


%\begin{frame}

En general, es m\'as f\'acil presentar evidencia de que $H_{1}$ es cierta, que demostrar 	que $H_{0}$ es falsa, es por eso que por lo regular se comienza suponiendo que $H_{0}$ es cierta, luego se utilizan los datos de la muestra para decidir si existe evidencia a favor de $H_{1}$, m\'as que a favor de $H_{0}$, as\'i se tienen dos conclusiones:
\begin{itemize}
\item Rechazar $H_{0}$ y concluir que $H_{1}$ es verdadera.
\item Aceptar, no rechazar, $H_{0}$ como verdadera.

\end{itemize}


%\end{frame}


%\begin{frame}

\begin{Ejem}
Se desea demostrar que el salario promedio  por hora en cierto lugar es distinto de $19$usd, que es el promedio nacional. Entonces $H_{1}:\mu\neq19$, y $H_{0}:\mu=19$.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de dos colas}.


\begin{Ejem}
Un determinado proceso produce un promedio de $5\%$ de piezas defectuosas. Se est\'a interesado en demostrar que un simple ajuste en una m\'aquina reducir\'a $p$, la proporci\'on de piezas defectuosas producidas en este proceso. Entonces se tiene $H_{0}:p<0.3$ y $H_{1}:p=0.03$. Si se puede rechazar $H_{0}$, se concluye que el proceso ajustado produce menos del $5\%$ de piezas defectuosas.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de una cola}.

%\end{frame}


%\begin{frame}

La decisi\'on de rechazar o aceptar la hip\'otesis nula est\'a basada en la informaci\'on contenida en una muestra proveniente de la poblaci\'on de inter\'es. Esta informaci\'on tiene estas formas

\begin{itemize}
\item \textbf{Estad\'sitico de prueba:} un s\'olo n\'umero calculado a partir de la muestra.

\item \textbf{$p$-value:} probabilidad calculada a partir del estad\'stico de prueba.

\end{itemize}

%\end{frame}


%\begin{frame}

\begin{Def}
El $p$-value es la probabilidad de observar un estad\'istico de prueba tanto o m\'as alejado del valor obervado, si en realidad $H_{0}$ es verdadera.\medskip
Valores grandes del estad\'stica de prueba  y valores peque\~nos de $p$ significan que se ha observado un evento muy poco probable, si $H_{0}$ en realidad es verdadera.
\end{Def}

Todo el conjunto de valores que puede tomar el estad\'istico de prueba se divide en dos regiones. Un conjunto, formado de valores que apoyan la hip\'otesis alternativa y llevan a rechazar $H_{0}$, se denomina \textbf{regi\'on de rechazo}. El otro, conformado por los valores que sustentatn la hip\'otesis nula, se le denomina \textbf{regi\'on de aceptaci\'on}.\medskip


%\end{frame}


%\begin{frame}

Cuando la regi\'on de rechazo est\'a en la cola izquierda de la distribuci\'on, la  prueba se denomina \textbf{prueba lateral izquierda}. Una prueba con regi\'on de rechazo en la cola derecha se le llama \textbf{prueba lateral derecha}.\medskip

Si el estad\'stico de prueba cae en la regi\'on de rechazo, entonces se rechaza $H_{0}$. Si el estad\'stico de prueba cae en la regi\'on de aceptaci\'on, entonces la hip\'otesis nula se acepta o la prueba se juzga como no concluyente.\medskip

Dependiendo del nivel de confianza que se desea agregar a las conclusiones de la prueba, y el \textbf{nivel de significancia $\alpha$}, el riesgo que est\'a dispuesto a correr si se toma una decisi\'on incorrecta.

%\end{frame}


%\begin{frame}

\begin{Def}
Un \textbf{error de tipo I} para una prueba estad\'istica es el error que se tiene al rechazar la hip\'otesis nula cuando es verdadera. El \textbf{nivel de significancia} para una prueba estad\'istica de hip\'otesis es
\begin{eqnarray*}
\alpha&=&P\left\{\textrm{error tipo I}\right\}=P\left\{\textrm{rechazar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es verdadera}\right\}
\end{eqnarray*}

\end{Def}
Este valor $\alpha$ representa el valor m\'aximo de riesgo tolerable de rechazar incorrectamente $H_{0}$. Una vez establecido el nivel de significancia, la regi\'on de rechazo se define para poder determinar si se rechaza $H_{0}$ con un cierto nivel de confianza.


%\end{frame}

\section{2.2 Muestras grandes: una media poblacional}
\subsection{2.2.1 C\'alculo de valor $p$}



%\begin{frame}

\begin{Def}
El \textbf{valor de $p$} (\textbf{$p$-value}) o nivel de significancia observado de un estad\'istico de prueba es el valor m\'as peque\~ no de $\alpha$ para el cual $H_{0}$ se puede rechazar. El riesgo de cometer un error tipo $I$, si $H_{0}$ es rechazada con base en la informaci\'on que proporciona la muestra.
\end{Def}

\begin{Note}
Valores peque\~ nos de $p$ indican 	que el valor observado del estad\'stico de prueba se encuentra alejado del valor hipot\'etico de $\mu$, es decir se tiene evidencia de que $H_{0}$ es falsa y por tanto debe de rechazarse.
\end{Note}


%\end{frame}


%\begin{frame}



\begin{Note}
Valores grandes de $p$ indican que el estad\'istico de prueba observado no est\'a alejado de la medi hipot\'etica y no apoya el rechazo de $H_{0}$.
\end{Note}

\begin{Def}
Si el valor de $p$ es menor o igual que el nivel de significancia $\alpha$, determinado previamente, entonces $H_{0}$ es rechazada y se puede concluir que los resultados son estad\'isticamente significativos con un nivel de confianza del $100\left(1-\alpha\right)\%$.
\end{Def}
Es usual utilizar la siguiente clasificaci\'on de resultados



%\end{frame}


%\begin{frame}


\begin{tabular}{|c||c|l|}\hline
$p$& $H_{0}$&Significativa\\\hline\hline
$p<0.01$&Rechazar &Altamente\\\hline
$0.01\leq p<0.05$ & Rechazar&Estad\'isticamente\\\hline
$0.05\leq p <0.1$ & No rechazar & Tendencia estad\'istica\\\hline
$0.01\leq p$ & No rechazar & No son estad\'isticamente\\\hline
\end{tabular}

\begin{Note}
Para determinar el valor de $p$, encontrar el \'area en la cola despu\'es del estad\'istico de prueba. Si la prueba es de una cola, este es el valor de $p$. Si es de dos colas, \'este valor encontrado es la mitad del valor de $p$. Rechazar $H_{0}$ cuando el valor de $p<\alpha$.
\end{Note}


%\end{frame}


%\begin{frame}


Hay dos tipos de errores al realizar una prueba de hip\'otesis
\begin{center}
\begin{tabular}{c|cc}
& $H_{0}$ es Verdadera & $H_{0}$ es Falsa\\\hline\hline
Rechazar $H_{0}$ & Error tipo I & $\surd$\\
Aceptar $H_{0}$ & $\surd$ & Error tipo II
\end{tabular}
\end{center}
\begin{Def}
La probabilidad de cometer el error tipo II se define por $\beta$ donde
\begin{eqnarray*}
\beta&=&P\left\{\textrm{error tipo II}\right\}=P\left\{\textrm{Aceptar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{Aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}
\end{eqnarray*}
\end{Def}

%\end{frame}


%\begin{frame}


\begin{Note}
Cuando $H_{0}$ es falsa y $H_{1}$ es verdadera, no siempre es posible especificar un valor exacto de $\mu$, sino m\'as bien un rango de posibles valores.\medskip
En lugar de arriesgarse a tomar una decisi\'on incorrecta, es mejor conlcuir que \textit{no hay evidencia suficiente para rechazar $H_{0}$}, es decir en lugar de aceptar $H_{0}$, \textit{no rechazar $H_{0}$}.

\end{Note}

%\end{frame}


%\begin{frame}

La bondad de una prueba estad\'istica se mide por el tama\~ no de $\alpha$ y $\beta$, ambas deben de ser peque\~ nas. Una manera muy efectiva de medir la potencia de la prueba es calculando el complemento del error tipo $II$:
\begin{eqnarray*}
1-\beta&= &P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{1}\textrm{ es verdadera}\right\}
\end{eqnarray*}
\begin{Def}
La \textbf{potencia de la prueba}, $1-\beta$, mide la capacidad de que la prueba funciona como se necesita.
\end{Def}

%\end{frame}


%\begin{frame}


\begin{Ejem}
La producci\'on diariade una planta qu\'imica local ha promediado 880 toneladas en los \'ultimos a\~nos. A la gerente de control de calidad le gustar\'ia saber si este promedio ha cambiado en meses recientes. Ella selecciona al azar 50 d\'ias de la base de datos computarizada y calcula el promedio y la desviaci\'on est\'andar de las $n=50$  producciones como $\overline{x}=871$ toneladas y $s=21$ toneladas, respectivamente. Pruebe la hip\'otesis  apropiada usando $\alpha=0.05$.

\end{Ejem}

\begin{Sol}
La hip\'otesis nula apropiada es:
\begin{eqnarray*}
H_{0}&:& \mu=880\\
&&\textrm{ y la hip\'otesis alternativa }H_{1}\textrm{ es }\\
H_{1}&:& \mu\neq880
\end{eqnarray*}
el estimador puntual para $\mu$ es $\overline{x}$, entonces el estad\'istico de prueba es\medskip
\begin{eqnarray*}
z&=&\frac{\overline{x}-\mu_{0}}{s/\sqrt{n}}\\
&=&\frac{871-880}{21/\sqrt{50}}=-3.03
\end{eqnarray*}
\end{Sol}


%\end{frame}


%\begin{frame}

\begin{Sol}
Para esta prueba de  dos colas, hay que determinar los dos valores de $z_{\alpha/2}$, es decir,  $z_{\alpha/2}=\pm1.96$, como $z>z_{\alpha/2}$, $z$  cae en la zona de rechazo, por lo tanto  la gerente puede rechazar la hip\'otesis nula y concluir que el promedio efectivamente ha cambiado.\medskip
La probabilidad de rechazar $H_{0}$ cuando esta es verdadera es de $0.05$.


Recordemos que el valor observado del estad\'istico de prueba es $z=-3.03$, la regi\'on de rechazo m\'as peque\~na que puede usarse y todav\'ia seguir rechazando $H_{0}$ es $|z|>3.03$, \\
entonces $p=2(0.012)=0.0024$, que a su vez es menor que el nivel de significancia $\alpha$ asignado inicialmente, y adem\'as los resultados son  \textbf{altamente significativos}.


\end{Sol}

%\end{frame}


%\begin{frame}

Finalmente determinemos la potencia de la prueba cuando $\mu$ en realidad es igual a $870$ toneladas.

Recordar que la regi\'on de aceptaci\'on est\'a entre $-1.96$ y $1.96$, para $\mu=880$, equivalentemente $$874.18<\overline{x}<885.82$$
$\beta$ es la probabilidad de aceptar $H_{0}$ cuando $\mu=870$, calculemos los valores de $z$ correspondientes a $874.18$ y $885.82$ \medskip
Entonces
\begin{eqnarray*}
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{874.18-870}{21/\sqrt{50}}=1.41\\
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{885.82-870}{21/\sqrt{50}}=5.33
\end{eqnarray*}


por lo tanto
\begin{eqnarray*}
\beta&=&P\left\{\textrm{aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{874.18<\mu<885.82\textrm{ cuando }\mu=870\right\}\\
&=&P\left\{1.41<z<5.33\right\}=P\left\{1.41<z\right\}\\
&=&1-0.9207=0.0793
\end{eqnarray*}
entonces, la potencia de la prueba es
$$1-\beta=1-0.0793=0.9207$$ que es la probabilidad de rechazar correctamente $H_{0}$ cuando $H_{0}$ es falsa.

%\end{frame}


%\begin{frame}

Determinar la potencia de la prueba para distintos valores de $H_{1}$ y graficarlos, \textit{curva de potencia}
\begin{center}
\begin{tabular}{c||c}
$H_{1}$ & $\left(1-\beta\right)$ \\\hline 
\hline 
865 &  \\ \hline 
870 &  \\ \hline 
872 &  \\ \hline 
875 &  \\ \hline 
877 &  \\ \hline 
880 &  \\ \hline 
883 &  \\ \hline 
885 &  \\ \hline 
888 &  \\ \hline 
890 &  \\ \hline 
895 &  \\ \hline 
\end{tabular} 

\end{center}

%\end{frame}


%\begin{frame}

\begin{enumerate}
\item Encontrar las regiones de rechazo para el estad\'istico $z$, para una prueba de
\begin{itemize}
\item[a) ]  dos colas para $\alpha=0.01,0.05,0.1$
\item[b) ]  una cola superior para $\alpha=0.01,0.05,0.1$
\item[c) ] una cola inferior para $\alpha=0.01,0.05,0.1$

\end{itemize}


\item Suponga que el valor del estad\'istico de prueba es 
\begin{itemize}
\item[a) ]$z=-2.41$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[b) ] $z=2.16$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[c) ] $z=1.15$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[d) ] $z=-2.78$, sacar las conclusiones correspondientes para los incisos anteriores.
\item[e) ] $z=-1.81$, sacar las conclusiones correspondientes para los incisos anteriores.

\end{itemize}
\end{enumerate}
%\end{frame}


%\begin{frame}

\begin{itemize}
\item[3. ] Encuentre el valor de $p$ para las pruebas de hip\'otesis correspondientes a los valores de $z$ del ejercicio anterior.

\item[4. ] Para las pruebas dadas en el ejercicio 2, utilice el valor de $p$, determinado en el ejercicio 3,  para determinar la significancia de los resultados.


\end{itemize}

%\end{frame}


%\begin{frame}

\begin{itemize}
\item[5. ] Una muestra aleatoria de $n=45$ observaciones de una poblaci\'on con media $\overline{x}=2.4$, y desviaci\'on est\'andar $s=0.29$. Suponga que el objetivo es demostrar que la media poblacional $\mu$ excede $2.3$.
\begin{itemize}
\item[a) ] Defina la hip\'otesis nula y alternativa para la prueba.
\item[b) ] Determine la regi\'on de rechazo para un nivel de significancia de: $\alpha=0.1,0.05,0.01$.
\item[c) ] Determine el error est\'andar de la media muestral.
\item[d) ] Calcule el valor de $p$ para los estad\'isticos de prueba definidos en los incisos anteriores.
\item[e) ] Utilice el valor de $p$ pra sacar una conclusi\'on al nivel de significancia $\alpha$.
\item[f) ] Determine el valor de $\beta$ cuando $\mu=2.5$
\item[g) ] Graficar la curva de potencia para la prueba.

\end{itemize}
\end{itemize}
%\end{frame}




\subsection{2.2.2 Prueba de hip\'otesis para la diferencia entre dos medias poblacionales}

%\begin{frame}



El estad\'istico que resume la informaci\'on muestral respecto a la diferencia en medias poblacionales $\left(\mu_{1}-\mu_{2}\right)$ es la diferencia de las medias muestrales $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, por tanto al probar la difencia entre las medias muestrales se verifica que la diferencia real entre las medias poblacionales difiere de un valor especificado, $\left(\mu_{1}-\mu_{2}\right)=D_{0}$, se puede usar el error est\'andar de $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, es decir
$$\sqrt{\frac{\sigma^{2}_{1}}{n_{1}}+\frac{\sigma^{2}_{2}}{n_{2}}}$$
cuyo estimador est\'a dado por
$$SE=\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}$$

El procedimiento para muestras grandes es:
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}

\end{itemize}

%\end{frame}


%\begin{frame}



\begin{itemize}
\item[3) ] Estad\'istico de prueba:
$$z=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}$$
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}


\end{itemize}


%\end{frame}


%\begin{frame}



\begin{Ejem}
Para determinar si ser propietario de un autom\'ovil afecta el rendimiento acad\'emico de un estudiante, se tomaron dos muestras aleatorias de 100 estudiantes varones. El promedio de calificaciones para los $n_{1}=100$ no propietarios de un auto tuvieron un promedio y varianza de $\overline{x}_{1}=2.7$ y $s_{1}^{2}=0.36$, respectivamente, mientras que para para la segunda muestra con $n_{2}=100$ propietarios de un auto, se tiene $\overline{x}_{2}=2.54$ y $s_{2}^{2}=0.4$. Los datos presentan suficiente evidencia para indicar una diferencia en la media en el rendimiento acad\'emico entre propietarios y no propietarios de un autom\'ovil? Hacer pruebas para $\alpha=0.01,0.05$ y $\alpha=0.1$.
\end{Ejem}

%\end{frame}


%\begin{frame}


\begin{Sol}
\begin{itemize}
\item Soluci\'on utilizando la t\'ecnica de regiones de rechazo:\medskip
realizando las operaciones
$z=1.84$, determinar si excede los valores de $z_{\alpha/2}$.
\item Soluci\'on utilizando el $p$-value:\medskip
Calcular el valor de $p$, la probabilidad de que $z$ sea mayor que $z=1.84$ o menor que $z=-1.84$, se tiene que $p=0.0658$. Concluir.
\end{itemize}
\end{Sol}

%\end{frame}


%\begin{frame}


\begin{itemize}
\item Si el intervalo de confianza que se construye contiene el valor del par\'ametro especificado por $H_{0}$, entonces ese valor es uno de los posibles valores del par\'ametro y $H_{0}$ no debe ser rechazada.

\item Si el valor hipot\'etico se encuentra fuera de los l\'imites de confianza, la hip\'otesis nula es rechazada al nivel de significancia $\alpha$.
\end{itemize}

\begin{enumerate}
\item Del libro Mendenhall resolver los ejercicios 9.18, 9.19 y 9.20(\href{https://cu.uacm.edu.mx/nextcloud/index.php/f/202873}{Mendenhall}).

\item Del libro \href{https://cu.uacm.edu.mx/nextcloud/index.php/f/202873}{Mendenhall} resolver los ejercicios: 9.23, 9.26 y 9.28.
\end{enumerate}



%\end{frame}



\subsection{2.2.3 Prueba de Hip\'otesis para una Proporci\'on Binomial}

%\begin{frame}


Para una muestra aleatoria de $n$ intentos id\'enticos, de una poblaci\'on binomial, la proporci\'on muesrtal $\hat{p}$ tiene una distribuci\'on aproximadamente normal cuando $n$ es grande, con media $p$ y error est\'andar
$$SE=\sqrt{\frac{pq}{n}}.$$
La prueba de hip\'otesis de la forma
\begin{eqnarray*}
H_{0}&:&p=p_{0}\\
H_{1}&:&p>p_{0}\textrm{, o }p<p_{0}\textrm{ o }p\neq p_{0}
\end{eqnarray*}
El estad\'istico de prueba se construye con el mejor estimador de la proporci\'on verdadera, $\hat{p}$, con el estad\'istico de prueba $z$, que se distribuye normal est\'andar.

El procedimiento es
\begin{itemize}
\item[1) ] Hip\'otesis nula: $H_{0}:p=p_{0}$
\item[2) ] Hip\'otesis alternativa
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:p>p_{0}$ & $p\neq p_{0}$\\
$H_{1}:p<p_{0}$ & \\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
z=\frac{\hat{p}-p_{0}}{\sqrt{\frac{pq}{n}}},\hat{p}=\frac{x}{n}
\end{eqnarray*}
donde $x$ es el n\'umero de \'exitos en $n$ intentos binomiales.

\end{itemize}

%\end{frame}


%\begin{frame}


\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}

%\end{frame}


%\begin{frame}


\begin{Ejem}
A cualquier edad, alrededor del $20\%$ de los adultos de cierto pa\'is realiza actividades de acondicionamiento f\'isico al menos dos veces por semana. En una encuesta local de $n=100$ adultos de m\'as de $40$ a\ ~nos, un total de 15 personas indicaron que realizaron actividad f\'isica al menos dos veces por semana. Estos datos indican que el porcentaje de participaci\'on para adultos de m\'as de 40 a\ ~nos de edad es  considerablemente menor a la cifra del $20\%$? Calcule el valor de $p$ y \'uselo para sacar las conclusiones apropiadas.
\end{Ejem}

\begin{enumerate}
\item Resolver los ejercicios: 9.30, 9.32, 9.33, 9.35 y 9.39.
\end{enumerate}



%\end{frame}


\subsection{2.2.4 Prueba de Hip\'otesis diferencia entre dos Proporciones Binomiales}



%\begin{frame}


\begin{Note}
Cuando se tienen dos muestras aleatorias independientes de dos poblaciones binomiales, el objetivo del experimento puede ser la diferencia $\left(p_{1}-p_{2}\right)$ en las proporciones de individuos u objetos que poseen una caracter\'istica especifica en las dos poblaciones. En este caso se pueden utilizar los estimadores de las dos proporciones $\left(\hat{p}_{1}-\hat{p}_{2}\right)$ con error est\'andar dado por
$$SE=\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}$$
considerando el estad\'istico $z$ con un nivel de significancia $\left(1-\alpha\right)100\%$

\end{Note}


\begin{Note}
La hip\'otesis nula a probarse es de la forma
\begin{itemize}
\item[$H_{0}$: ] $p_{1}=p_{2}$ o equivalentemente $\left(p_{1}-p_{2}\right)=0$, contra una hip\'otesis alternativa $H_{1}$ de una o dos colas.
\end{itemize}
\end{Note}

%\end{frame}


%\begin{frame}


\begin{Note}
Para estimar el error est\'andar del estad\'istico $z$, se debe de utilizar el hecho de que suponiendo que $H_{0}$ es verdadera, las dos proporciones son iguales a alg\'un valor com\'un, $p$. Para obtener el mejor estimador de $p$ es
$$p=\frac{\textrm{n\'umero total de \'exitos}}{\textrm{N\'umero total de pruebas}}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$$
\end{Note}



\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\left(p_{1}-p_{2}\right)=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(p_{1}-p_{2}\right)>0$ & $H_{1}:\left(p_{1}-p_{2}\right)\neq 0$\\ 
$H_{1}:\left(p_{1}-p_{2}\right)<0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}}=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{pq}{n_{1}}+\frac{pq}{n_{2}}}}
\end{eqnarray*}
donde $\hat{p_{1}}=x_{1}/n_{1}$ y $\hat{p_{2}}=x_{2}/n_{2}$ , dado que el valor com\'un para $p_{1}$ y $p_{2}$ es $p$, entonces $\hat{p}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$ y por tanto el estad\'istico de prueba es
\end{itemize}

%\end{frame}


%\begin{frame}



\begin{eqnarray*}
z=\frac{\hat{p}_{1}-\hat{p}_{2}}{\sqrt{\hat{p}\hat{q}}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}
\end{eqnarray*}
\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{\alpha}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}

\end{itemize}

%\end{frame}


%\begin{frame}



\begin{Ejem}
Los registros de un hospital, indican que 52 hombres de una muestra de 1000 contra 23 mujeres de una muestra de 1000 fueron ingresados por enfermedad del coraz\'on. Estos datos presentan suficiente evidencia para indicar un porcentaje m\'as alto de enfermedades del coraz\'on entre hombres ingresados al hospital?, utilizar distintos niveles de confianza de $\alpha$.

\end{Ejem}
\begin{enumerate}
\item Resolver los ejercicios 9.42

\item Resolver los ejercicios: 9.45, 9.48, 9.50
\end{enumerate}



%\end{frame}



\section{2.3 Muestras Peque\~nas}

\subsection{2.3.1 Una media poblacional}


%\begin{frame}

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu=\mu_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu>\mu_{0}$ & $H_{1}:\mu\neq \mu_{0}$\\ 
$H_{1}:\mu<\mu0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
t=\frac{\overline{x}-\mu_{0}}{\sqrt{\frac{s^{2}}{n}}}
\end{eqnarray*}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{itemize}

%\end{frame}


%\begin{frame}

\begin{Ejem}
Las etiquetas en latas de un gal'on de pintura por lo general indican el tiempo de secado y el \'area puede cubrir una capa. Casi todas las marcas de pintura indican que, en una capa, un gal\'on cubrir\'a entre 250 y 500 pies cuadrados, dependiento de la textura de la superficie a pintarse, un fabricante, sin embargo afirma que un gal\'on de su pintura cubrir\'a 400 pies cuadrados de \'area superficial. Para probar su afirmaci\'on, una muestra aleatoria de 10 latas de un gal\'on de pintura blanca se emple\'o para pintar 10 \'areas id\'enticas usando la misma clase de equipo. Las \'areas reales en pies cuadrados cubiertas por estos 10 galones de pintura se dan a continuac\'on:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
310 & 311 & 412 & 368 & 447 \\ 
\hline 
376 & 303 &410 &365 & 350 \\ 
\hline 
\end{tabular} 
\end{center}
\end{Ejem}

%\end{frame}


%\begin{frame}

\begin{Ejem}
Los datos presentan suficiente evidencia para indicar que el promedio de la cobertura difiere de 400 pies cuadrados? encuentre el valor de $p$ para la prueba y \'uselo para evaluar la significancia de los resultados.
\end{Ejem}
\begin{enumerate}
\item Resolver los ejercicios: 10.2, 10.3,10.5, 10.7, 10.9, 10.13 y 10.16
\end{enumerate}


%\end{frame}


\subsection{2.3.2 Diferencia entre dos medias poblacionales: M.A.I.}

%\begin{frame}

\begin{Note}
Cuando los tama\ ~nos de muestra son peque\ ~nos, no se puede asegurar que las medias muestrales sean normales, pero si las poblaciones originales son normales, entonces la distribuci\'on muestral de la diferencia de las medias muestales, $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, ser\'a normal con media $\left(\mu_{1}-\mu_{2}\right)$ y error est\'andar $$ES=\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}$$

\end{Note}

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}

\item[3) ] Estad\'istico de prueba:
$$t=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}$$
\end{itemize}

%\end{frame}


%\begin{frame}


donde $$s^{2}=\frac{\left(n_{1}-1\right)s_{1}^{2}+\left(n_{2}-1\right)s_{2}^{2}}{n_{1}+n_{2}-2}$$
\begin{itemize}

\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.


\end{itemize}


%\end{frame}



\subsection{2.3.3 Diferencia entre dos medias poblacionales: Diferencias Pareadas}


%\begin{frame}

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu_{d}=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:\mu_{d}$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu_{d}>0$ & $H_{1}:\mu_{d}\neq 0$\\ 
$H_{1}:\mu_{d}<0$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
t=\frac{\overline{d}}{\sqrt{\frac{s_{d}^{2}}{n}}}
\end{eqnarray*}
donde $n$ es el n\'umero de diferencias pareadas, $\overline{d}$ es la media de las diferencias muestrales, y $s_{d}$ es la desviaci\'on est\'andar de las diferencias muestrales.



\end{itemize}

%\end{frame}


%\begin{frame}

\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}

Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.

\end{itemize}

%\end{frame}



\subsection{2.3.4 Inferencias con respecto a la Varianza Poblacional}


%\begin{frame}

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\sigma^{2}=\sigma^{2}_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}$
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\sigma^{2}>\sigma^{2}_{0}$ & $H_{1}:\sigma^{2}\neq \sigma^{2}_{0}$\\ 
$H_{1}:\sigma^{2}<\sigma^{2}_{0}$&\\
\end{tabular}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray*}
\chi^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}_{0}}
\end{eqnarray*}

\end{itemize}


%\end{frame}


%\begin{frame}

\begin{itemize}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$\chi^{2}>\chi^{2}_{\alpha}$ & \\
$\chi^{2}<\chi^{2}_{\left(1-\alpha\right)}$ cuando $H_{1}:\chi^{2}<\chi^{2}_{0}$&$\chi^{2}>\chi^{2}_{\alpha/2}$ o $\chi^{2}<\chi^{2}_{\left(1-\alpha/2\right)}$\\
 cuando $p<\alpha$&\\
\end{tabular}

Los valores cr\'iticos de $\chi^{2}$,est\'an basados en $\left(n_{1}+\right)$ grados de libertad.

\end{itemize}

%\end{frame}


\subsection{2.3.5 Comparaci\'on de dos varianzas poblacionales}



%\begin{frame}
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)>D_{0}$ & $H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)<D_{0}$&\\
\end{tabular}

\end{itemize}


\begin{itemize}
\item[3) ] Estad\'istico de prueba:
$$F=\frac{s_{1}^{2}}{s_{2}^{2}}$$
donde $s_{1}^{2}$ es la varianza muestral m\'as grande.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$F>F_{\alpha}$ & $F>F_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}


\end{itemize}

%\end{frame}


\section{Ejercicios}

%\begin{frame}
\begin{itemize}
\item[1) ] Del libro Probabililidad y Estad\'sitica para Ingenier\'ia de Hines, Montgomery, Goldsman y Borror resolver los siguientes ejercicios: 10-9, 10-10,10-13,10-16 y 10-20.

\item[2) ] Realizar un programa en R para cada una de las secciones y subsecciones revisadas en clase, para determinar intervalos de confianza.

\item[3) ] Aplicar los programas elaborados en el ejercicio anterior a la siguiente lista:  10-39, 10-41, 10-45, 10-47, 10-48, 10-50, 10-52, 10-54, 10-56,10-57, 10-58, 10-65, 10-68, 10-72 y 10-73.

\item[4) ]  Elaborar una rutina en R que grafique las siguientes distribuciones, permitiendo variar los par\'ametros de las distribuciones: Binomial, Uniforme continua, Gamma, Beta, Exponencial, Normal y $t$-Student.

\item[5)] Presentar el primer cap\'itulo del libro del curso en formato \textit{Rnw} con su respectivo archivo \textit{pdf} generado
\end{itemize}


%\end{frame}


%---------------------------------------------------------
\section{An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------


%\begin{frame}
\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

\end{Note}

%\end{frame}

%---------------------------------------------------------
\section{An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------

%\begin{frame}
\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

\end{Note}


%\end{frame}


\subsection{Regresi\'on Lineal Simple (RLS)}


%\begin{frame}

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y  $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}

donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


%\end{frame}


%\begin{frame}
Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$,  estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.

Entonces la ecuaci\'on (\ref{Modelo.Regresion}) se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on 
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}

%\end{frame}


%\begin{frame}
Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$,  $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$,  igualar a cero y  resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene 

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}

%\end{frame}


%\begin{frame}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}

Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto

%\end{frame}


%\begin{frame}
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}

%\end{frame}


\subsection{Regresi\'on Lineal Simple (RLS)}




%\begin{frame}
Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y  $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}

%\end{frame}


%\begin{frame}
donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$,  estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.


Entonces la ecuaci\'on (\ref{Modelo.Regresion}) se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on 
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}

%\end{frame}


%\begin{frame}
Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$,  igualar a cero y  resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}

%\end{frame}


%\begin{frame}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}

Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}

%\end{frame}


%---------------------------------------------------------
\section{3. An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------

%\begin{frame}


\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\end{itemize}

\end{Note}

%\end{frame}


\subsection{3.1 Regresi\'on Lineal Simple (RLS)}



%\begin{frame}

\begin{itemize}

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 



%\end{frame}

\subsection{3.2 M\'etodo de M\'inimos Cuadrados}


%\begin{frame}

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}
donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$, estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.


%\end{frame}


%\begin{frame}

Entonces la ecuaci\'on \ref{Modelo.Regresion} se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}

%\end{frame}


%\begin{frame}

Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$, igualar a cero y resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ecs.Estimadores.Regresion}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}

%\end{frame}


%\begin{frame}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}
Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}

%\end{frame}


\subsection{3.3 Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}

%\begin{frame}
\begin{Note}
\begin{itemize}
\item Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo.

\item Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.
\end{itemize}
\end{Note}
A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)
\end{eqnarray*}
\begin{eqnarray*}
&=&\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)\right.\\
&+&\left.E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]=\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}
\end{eqnarray*}

%\end{frame}


%\begin{frame}

por lo tanto 
\begin{equation}\label{Esperanza.Beta.1}
E\left(\hat{\beta}_{1}\right)=\beta_{1}
\end{equation}
\begin{Note}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado.
\end{Note}
Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}\\
&=&\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}}
\end{eqnarray*}

%\end{frame}


%\begin{frame}

por lo tanto
\begin{equation}\label{Varianza.Beta.1}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}
\end{equation}
\begin{Prop}
\begin{eqnarray*}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray*}
\end{Prop}
Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray}

%\end{frame}


%\begin{frame}

sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=MC_{E}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray*}

%\end{frame}

\subsection{3.4 Prueba de Hip\'otesis en RLS}


%\begin{frame}

\begin{itemize}
\item Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza.

\item Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$.

\end{itemize}

%\end{frame}


%\begin{frame}

Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:
\begin{centering}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,

\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.

\end{itemize}
donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. 
\end{centering}

%\end{frame}


%\begin{frame}
De las ecuaciones (\ref{Ecs.Estimadores.Regresion}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1}) y (\ref{Varianza.Beta.1}).
Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}


%\end{frame}


%\begin{frame}

Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

%\end{frame}


%\begin{frame}

\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}=\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}

\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}

%\end{frame}


%\begin{frame}

\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
\end{eqnarray*}

\begin{eqnarray*}
&=&\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}

%\end{frame}


%\begin{frame}

Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}
Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados}) se puede reescribir como 
\begin{equation}\label{Suma.Total.Cuadrados.Dos}
S_{yy}=SC_{R}+SC_{E}
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right)\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.

%\end{frame}


%\begin{frame}

\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx}
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}
Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$.

El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente\medskip

%\end{frame}


%\begin{frame}

\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 

La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1}), con $\hat{\beta}_{1,0}=0$, es decir
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}

%\end{frame}


%\begin{frame}
Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.

%\end{frame}


\subsection{Estimaci\'on de Intervalos en RLS}


%\begin{frame}

\begin{itemize}
\item Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros.

\item El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on.

\end{itemize}

%\end{frame}


%\begin{frame}

Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por
\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\small{
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}}

%\end{frame}

%---------------------------------------------------------
\section{3. An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------

%\begin{frame}

\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\end{itemize}

\end{Note}

%\end{frame}



\subsection{3.1 Regresi\'on Lineal Simple (RLS)}



%\begin{frame}

\begin{itemize}

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 


%\end{frame}

\subsection{3.2 M\'etodo de M\'inimos Cuadrados}


%\begin{frame}

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}
donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$, estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.



%\end{frame}


%\begin{frame}


Entonces la ecuaci\'on \ref{Modelo.Regresion} se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}
Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$, igualar a cero y resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}

%\end{frame}


%\begin{frame}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene
\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ecs.Estimadores.Regresion}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}

%\end{frame}


%\begin{frame}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}
Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}

%\end{frame}



\subsection{3.3 Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}



%\begin{frame}

\begin{Note}
\begin{itemize}
\item Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo.

\item Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.
\end{itemize}
\end{Note}
A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)
\end{eqnarray*}

%\end{frame}


%\begin{frame}

\begin{eqnarray*}
&=&\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)\right.\\
&+&\left.E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]=\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}
\end{eqnarray*}
por lo tanto 
\begin{equation}\label{Esperanza.Beta.1}
E\left(\hat{\beta}_{1}\right)=\beta_{1}
\end{equation}

%\end{frame}


%\begin{frame}

\begin{Note}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado.
\end{Note}
Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}\\
&=&\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}}
\end{eqnarray*}

%\end{frame}


%\begin{frame}


por lo tanto
\begin{equation}\label{Varianza.Beta.1}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}
\end{equation}
\begin{Prop}
\begin{eqnarray*}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray*}
\end{Prop}

Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray}

%\end{frame}


%\begin{frame}

sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=MC_{E}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray*}


%\end{frame}



%\end{document}
\subsection{3.4 Prueba de Hip\'otesis en RLS}


%\begin{frame}

\begin{itemize}
\item Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza.

\item Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$.

\end{itemize}


Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:
\begin{centering}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,

\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.

\end{itemize}

donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. 
\end{centering}

%\end{frame}

%\begin{frame}
De las ecuaciones (\ref{Ecs.Estimadores.Regresion}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1}) y (\ref{Varianza.Beta.1}).

Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}


%\end{frame}

%\begin{frame}


Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}


%\end{frame}

%\begin{frame}



\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}=\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}


%\end{frame}

%\begin{frame}

\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}

\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
\end{eqnarray*}

%\end{frame}

%\begin{frame}

\begin{eqnarray*}
&=&\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}

Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}

%\end{frame}

%\begin{frame}

Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados}) se puede reescribir como 
\begin{equation}\label{Suma.Total.Cuadrados.Dos}
S_{yy}=SC_{R}+SC_{E}
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right)\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.

\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx}
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}

%\end{frame}

%\begin{frame}
Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$.
									
El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente\medskip

\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 

%\end{frame}

%\begin{frame}

La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1}), con $\hat{\beta}_{1,0}=0$, es decir
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.


%\end{frame}


\subsection{Estimaci\'on de Intervalos en RLS}

%\begin{frame}

\begin{itemize}
\item Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros.

\item El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on.

\end{itemize}

Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por

%\end{frame}

%\begin{frame}
\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\small{
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}}

%\end{frame}
%\end{document}
\subsection{Predicci\'on}
%\begin{frame}

Supongamos que se tiene un valor $x_{0}$ de inter\'es, entonces la estimaci\'on puntual de este nuevo valor
\begin{equation}
\hat{y}_{0}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}
\end{equation}
\begin{Note}
Esta nueva observaci\'on es independiente de las utilizadas para obtener el modelo de regresi\'on, por tanto, el intervalo en torno a la recta de regresi\'on es inapropiado, puesto que se basa \'unicamente en los datos empleados para ajustar el modelo de regresi\'on.\\

El intervalo de confianza en torno a la recta de regresi\'on se refiere a la respuesta media verdadera $x=x_{0}$, no a observaciones futuras.
\end{Note}

%\end{frame}

%\begin{frame}


Sea $y_{0}$ la observaci\'on futura en $x=x_{0}$, y sea $\hat{y}_{0}$ dada en la ecuaci\'on anterior, el estimador de $y_{0}$. Si se define la variable aleatoria $$w=y_{0}-\hat{y}_{0},$$ esta se distribuye normalmente con media cero y varianza $$V\left(w\right)=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]$$
dado que $y_{0}$ es independiente de $\hat{y}_{0}$, por lo tanto el intervalo de predicci\'on al nivel $\alpha$ para futuras observaciones $x_{0}$ es


\begin{eqnarray*}
\hat{y}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}\leq y_{0}\\
\leq \hat{y}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}.
\end{eqnarray*}


%\end{frame}


\subsection{Prueba de falta de ajuste}
%\begin{frame}\frametitle{Falta de ajuste}
Es com\'un encontrar que el modelo ajustado no satisface totalmente el modelo necesario para los datos, en este caso es preciso saber qu\'e tan bueno es el modelo propuesto. Para esto se propone la siguiente prueba de hip\'otesis:
\begin{itemize}
\item[$H_{0}:$ ]El modelo propuesto se ajusta adecuademente a los datos.
\item[$H_{1}:$ ]El modelo NO se ajusta a los datos.
\end{itemize}
La prueba implica dividir la suma de cuadrados del eror o del residuo en las siguientes dos componentes:
\begin{eqnarray*}
SC_{E}=SC_{EP}+SC_{FDA}
\end{eqnarray*}

%\end{frame}

%\begin{frame}\frametitle{Falta de ajuste}
donde $SC_{EP}$ es la suma de cuadrados atribuibles al error puro, y $SC_{FDA}$ es la suma de cuadrados atribuible a la falta de ajuste del modelo.
%\end{frame}

%%\begin{frame}\frametitle{Falta de ajuste}
%%\end{frame}


\subsection{Coeficiente de Determinaci\'on}

%\begin{frame}

La cantidad
\begin{equation}
R^{2}=\frac{SC_{R}}{S_{yy}}=1-\frac{SC_{E}}{S_{yy}}
\end{equation}
se denomina coeficiente de determinaci\'on y se utiliza para saber si el modelo de regresi\'on es suficiente o no. Se puede demostrar que $0\leq R^{2}\leq1$, una manera de interpretar este valor es que si $R^{2}=k$, entonces el modelo de regresi\'on explica el $k*100\%$ de la variabilidad en los datos.
$R^{2}$ 
\begin{itemize}
\item No mide la magnitud de la pendiente de la recta de regresi\'on
\item Un valor grande de $R^{2}$ no implica una pendiente empinada.
\item No mide la suficiencia del modelo.
\item Valores grandes de $R^{2}$ no implican necesariamente que el modelo de regresi\'on proporcionar\'a predicciones precisas para futuras observaciones.
\end{itemize}


%\end{frame}



\section{Introducci\'on}

La preparaci\'on de datos y la selecci\'on de variables son pasos cruciales en el proceso de modelado estad\'istico. Un modelo bien preparado y con las variables adecuadas puede mejorar significativamente la precisi\'on y la interpretabilidad del modelo. Este cap\'itulo proporciona una revisi\'on detallada de las t\'ecnicas de limpieza de datos, tratamiento de datos faltantes, codificaci\'on de variables categ\'oricas y selecci\'on de variables.

\section{Importancia de la Preparaci\'on de Datos}

La calidad de los datos es fundamental para el \'exito de cualquier an\'alisis estad\'istico. Los datos sin limpiar pueden llevar a modelos inexactos y conclusiones err\'oneas. La preparaci\'on de datos incluye varias etapas:
\begin{itemize}
    \item Limpieza de datos
    \item Tratamiento de datos faltantes
    \item Codificaci\'on de variables categ\'oricas
    \item Selecci\'on y transformaci\'on de variables
\end{itemize}

\section{Limpieza de Datos}

La limpieza de datos es el proceso de detectar y corregir (o eliminar) los datos incorrectos, incompletos o irrelevantes. Este proceso incluye:
\begin{itemize}
    \item Eliminaci\'on de duplicados
    \item Correcci\'on de errores tipogr\'aficos
    \item Consistencia de formato
    \item Tratamiento de valores extremos (outliers)
\end{itemize}

\section{Tratamiento de Datos Faltantes}

Los datos faltantes son un problema com\'un en los conjuntos de datos y pueden afectar la calidad de los modelos. Hay varias estrategias para manejar los datos faltantes:
\begin{itemize}
    \item \textbf{Eliminaci\'on de Datos Faltantes}: Se eliminan las filas o columnas con datos faltantes.
    \item \textbf{Imputaci\'on}: Se reemplazan los valores faltantes con estimaciones, como la media, la mediana o la moda.
    \item \textbf{Modelos Predictivos}: Se utilizan modelos predictivos para estimar los valores faltantes.
\end{itemize}

\subsection{Imputaci\'on de la Media}

Una t\'ecnica com\'un es reemplazar los valores faltantes con la media de la variable. Esto se puede hacer de la siguiente manera:
\begin{eqnarray*}
x_i = \begin{cases} 
      x_i & \text{si } x_i \text{ no es faltante} \\
      \bar{x} & \text{si } x_i \text{ es faltante}
   \end{cases}
\end{eqnarray*}
donde $\bar{x}$ es la media de la variable.

\section{Codificaci\'on de Variables Categ\'oricas}

Las variables categ\'oricas deben ser convertidas a un formato num\'erico antes de ser usadas en un modelo de regresi\'on log\'istica. Hay varias t\'ecnicas para codificar variables categ\'oricas:

\subsection{Codificaci\'on One-Hot}

La codificaci\'on one-hot crea una columna binaria para cada categor\'ia. Por ejemplo, si tenemos una variable categ\'orica con tres categor\'ias (A, B, C), se crean tres columnas:
\begin{eqnarray*}
\text{A} &=& [1, 0, 0] \\
\text{B} &=& [0, 1, 0] \\
\text{C} &=& [0, 0, 1]
\end{eqnarray*}

\subsection{Codificaci\'on Ordinal}

La codificaci\'on ordinal asigna un valor entero \'unico a cada categor\'ia, preservando el orden natural de las categor\'ias. Por ejemplo:
\begin{eqnarray*}
\text{Bajo} &=& 1 \\
\text{Medio} &=& 2 \\
\text{Alto} &=& 3
\end{eqnarray*}

\section{Selecci\'on de Variables}

La selecci\'on de variables es el proceso de elegir las variables m\'as relevantes para el modelo. Existen varias t\'ecnicas para la selecci\'on de variables:

\subsection{M\'etodos de Filtrado}

Los m\'etodos de filtrado seleccionan variables basadas en criterios estad\'isticos, como la correlaci\'on o la chi-cuadrado. Algunas t\'ecnicas comunes incluyen:
\begin{itemize}
    \item \textbf{An\'alisis de Correlaci\'on}: Se seleccionan variables con alta correlaci\'on con la variable dependiente y baja correlaci\'on entre ellas.
    \item \textbf{Pruebas de Chi-cuadrado}: Se utilizan para variables categ\'oricas para determinar la asociaci\'on entre la variable independiente y la variable dependiente.
\end{itemize}

\subsection{M\'etodos de Wrapper}

Los m\'etodos de wrapper eval\'uan m\'ultiples combinaciones de variables y seleccionan la combinaci\'on que optimiza el rendimiento del modelo. Ejemplos incluyen:
\begin{itemize}
    \item \textbf{Selecci\'on hacia Adelante}: Comienza con un modelo vac\'io y agrega variables una por una, seleccionando la variable que mejora m\'as el modelo en cada paso.
    \item \textbf{Selecci\'on hacia Atr\'as}: Comienza con todas las variables y elimina una por una, removiendo la variable que tiene el menor impacto en el modelo en cada paso.
    \item \textbf{Selecci\'on Paso a Paso}: Combina la selecci\'on hacia adelante y hacia atr\'as, agregando y eliminando variables seg\'un sea necesario.
\end{itemize}

\subsection{M\'etodos Basados en Modelos}

Los m\'etodos basados en modelos utilizan t\'ecnicas de regularizaci\'on como Lasso y Ridge para seleccionar variables. Estas t\'ecnicas a\~naden un t\'ermino de penalizaci\'on a la funci\'on de costo para evitar el sobreajuste.

\subsubsection{Regresi\'on Lasso}

La regresi\'on Lasso (Least Absolute Shrinkage and Selection Operator) a\~nade una penalizaci\'on $L_1$ a la funci\'on de costo:
\begin{eqnarray*}
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
\end{eqnarray*}
donde $\lambda$ es el par\'ametro de regularizaci\'on que controla la cantidad de penalizaci\'on.

\subsubsection{Regresi\'on Ridge}

La regresi\'on Ridge a\~nade una penalizaci\'on $L_2$ a la funci\'on de costo:
\begin{eqnarray*}
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
\end{eqnarray*}
donde $\lambda$ es el par\'ametro de regularizaci\'on.

\section{Implementaci\'on en R}

\subsection{Limpieza de Datos}

Para ilustrar la limpieza de datos en R, considere el siguiente conjunto de datos:
\begin{verbatim}
data <- data.frame(
  var1 = c(1, 2, 3, NA, 5),
  var2 = c("A", "B", "A", "B", "A"),
  var3 = c(10, 15, 10, 20, 25)
)

# Eliminaci\'on de filas con datos faltantes
data_clean <- na.omit(data)

# Imputaci\'on de la media
data$var1[is.na(data$var1)] <- mean(data$var1, na.rm = TRUE)
\end{verbatim}

\subsection{Codificaci\'on de Variables Categ\'oricas}

Para codificar variables categ\'oricas, utilice la funci\'on `model.matrix`:
\begin{verbatim}
data <- data.frame(
  var1 = c(1, 2, 3, 4, 5),
  var2 = c("A", "B", "A", "B", "A")
)

# Codificaci\'on one-hot
data_onehot <- model.matrix(~ var2 - 1, data = data)
\end{verbatim}

\subsection{Selecci\'on de Variables}

Para la selecci\'on de variables, utilice el paquete `caret`:
\begin{verbatim}
library(caret)

# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)
trainIndex <- createDataPartition(data$var1, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dataTrain <- data[trainIndex,]
dataTest <- data[-trainIndex,]

# Modelo de regresi\'on log\'istica
model <- train(var1 ~ ., data = dataTrain, method = "glm", family = "binomial")

# Selecci\'on de variables
model <- step(model, direction = "both")
summary(model)
\end{verbatim}



\section{Introducción}

Evaluar la calidad y el rendimiento de un modelo de regresión logística es crucial para asegurar que las predicciones sean precisas y útiles. Este capítulo se centra en las técnicas y métricas utilizadas para evaluar modelos de clasificación binaria, así como en la validación cruzada, una técnica para evaluar la generalización del modelo.

\section{Métricas de Evaluación del Modelo}

Las métricas de evaluación permiten cuantificar la precisión y el rendimiento de un modelo. Algunas de las métricas más comunes incluyen:

\subsection{Curva ROC y AUC}

La curva ROC (Receiver Operating Characteristic) es una representación gráfica de la sensibilidad (verdaderos positivos) frente a 1 - especificidad (falsos positivos). El área bajo la curva (AUC) mide la capacidad del modelo para distinguir entre las clases.

\begin{eqnarray*}
\text{Sensibilidad} &=& \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\text{Especificidad} &=& \frac{\text{TN}}{\text{TN} + \text{FP}}
\end{eqnarray*}

\subsection{Matriz de Confusión}

La matriz de confusión es una tabla que muestra el rendimiento del modelo comparando las predicciones con los valores reales. Los términos incluyen:
\begin{itemize}
    \item \textbf{Verdaderos Positivos (TP)}: Predicciones correctas de la clase positiva.
    \item \textbf{Falsos Positivos (FP)}: Predicciones incorrectas de la clase positiva.
    \item \textbf{Verdaderos Negativos (TN)}: Predicciones correctas de la clase negativa.
    \item \textbf{Falsos Negativos (FN)}: Predicciones incorrectas de la clase negativa.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Predicción Positiva} & \textbf{Predicción Negativa} \\
\hline
\textbf{Real Positiva} & TP & FN \\
\hline
\textbf{Real Negativa} & FP & TN \\
\hline
\end{tabular}
\caption{Matriz de Confusión}
\label{tab:confusion_matrix}
\end{table}

\subsection{Precisión, Recall y F1-Score}

\begin{eqnarray*}
\text{Precisión} &=& \frac{\text{TP}}{\text{TP} + \text{FP}} \\
\text{Recall} &=& \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\text{F1-Score} &=& 2 \cdot \frac{\text{Precisión} \cdot \text{Recall}}{\text{Precisión} + \text{Recall}}
\end{eqnarray*}

\subsection{Log-Loss}

La pérdida logarítmica (Log-Loss) mide la precisión de las probabilidades predichas. La fórmula es:
\begin{eqnarray*}
\text{Log-Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{eqnarray*}
donde $y_i$ son los valores reales y $p_i$ son las probabilidades predichas.

\section{Validación Cruzada}

La validación cruzada es una técnica para evaluar la capacidad de generalización de un modelo. Existen varios tipos de validación cruzada:

\subsection{K-Fold Cross-Validation}

En K-Fold Cross-Validation, los datos se dividen en K subconjuntos. El modelo se entrena K veces, cada vez utilizando K-1 subconjuntos para el entrenamiento y el subconjunto restante para la validación.

\begin{eqnarray*}
\text{Error Medio} = \frac{1}{K} \sum_{k=1}^{K} \text{Error}_k
\end{eqnarray*}

\subsection{Leave-One-Out Cross-Validation (LOOCV)}

En LOOCV, cada observación se usa una vez como conjunto de validación y las restantes como conjunto de entrenamiento. Este método es computacionalmente costoso pero útil para conjuntos de datos pequeños.

\section{Ajuste y Sobreajuste del Modelo}

El ajuste adecuado del modelo es crucial para evitar el sobreajuste (overfitting) y el subajuste (underfitting).

\subsection{Sobreajuste}

El sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento, capturando ruido y patrones irrelevantes. Los síntomas incluyen una alta precisión en el entrenamiento y baja precisión en la validación.

\subsection{Subajuste}

El subajuste ocurre cuando un modelo no captura los patrones subyacentes de los datos. Los síntomas incluyen baja precisión tanto en el entrenamiento como en la validación.

\subsection{Regularización}

La regularización es una técnica para prevenir el sobreajuste añadiendo un término de penalización a la función de costo. Las técnicas comunes incluyen:
\begin{itemize}
    \item \textbf{Regresión Lasso (L1)}
    \item \textbf{Regresión Ridge (L2)}
\end{itemize}

\section{Implementación en R}

\subsection{Evaluación del Modelo}

\begin{verbatim}
# Cargar el paquete necesario
library(caret)

# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)
trainIndex <- createDataPartition(data$var1, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dataTrain <- data[trainIndex,]
dataTest <- data[-trainIndex,]

# Entrenar el modelo de regresión logística
model <- train(var1 ~ ., data = dataTrain, method = "glm", family = "binomial")

# Predicciones en el conjunto de prueba
predictions <- predict(model, dataTest)

# Matriz de confusión
confusionMatrix(predictions, dataTest$var1)
\end{verbatim}

\subsection{Validación Cruzada}

\begin{verbatim}
# K-Fold Cross-Validation
control <- trainControl(method = "cv", number = 10)
model_cv <- train(var1 ~ ., data = dataTrain, method = "glm", 
                  family = "binomial", trControl = control)

# Evaluación del modelo con validación cruzada
print(model_cv)
\end{verbatim}



\section{Introducci\'on}

El diagn\'ostico del modelo y el ajuste de par\'ametros son pasos esenciales para mejorar la precisi\'on y la robustez de los modelos de regresi\'on log\'istica. Este cap\'itulo se enfoca en las t\'ecnicas para diagnosticar problemas en los modelos y en m\'etodos para ajustar los par\'ametros de manera \'optima.

\section{Diagn\'ostico del Modelo}

El diagn\'ostico del modelo implica evaluar el rendimiento del modelo y detectar posibles problemas, como el sobreajuste, la multicolinealidad y la influencia de puntos de datos individuales.

\subsection{Residuos}

Los residuos son las diferencias entre los valores observados y los valores predichos por el modelo. El an\'alisis de residuos puede revelar patrones que indican problemas con el modelo.

\begin{eqnarray*}
\text{Residuo}_i = y_i - \hat{y}_i
\end{eqnarray*}

\subsubsection{Residuos Estudiantizados}

Los residuos estudiantizados se ajustan por la variabilidad del residuo y se utilizan para detectar outliers.

\begin{eqnarray*}
r_i = \frac{\text{Residuo}_i}{\hat{\sigma} \sqrt{1 - h_i}}
\end{eqnarray*}
donde $h_i$ es el leverage del punto de datos.

\subsection{Influencia}

La influencia mide el impacto de un punto de datos en los coeficientes del modelo. Los puntos con alta influencia pueden distorsionar el modelo.

\subsubsection{Distancia de Cook}

La distancia de Cook es una medida de la influencia de un punto de datos en los coeficientes del modelo.

\begin{eqnarray*}
D_i = \frac{r_i^2}{p} \cdot \frac{h_i}{1 - h_i}
\end{eqnarray*}
donde $p$ es el n\'umero de par\'ametros en el modelo.

\subsection{Multicolinealidad}

La multicolinealidad ocurre cuando dos o m\'as variables independientes est\'an altamente correlacionadas. Esto puede inflar las varianzas de los coeficientes y hacer que el modelo sea inestable.

\subsubsection{Factor de Inflaci\'on de la Varianza (VIF)}

El VIF mide cu\'anto se inflan las varianzas de los coeficientes debido a la multicolinealidad.

\begin{eqnarray*}
\text{VIF}_j = \frac{1}{1 - R_j^2}
\end{eqnarray*}
donde $R_j^2$ es el coeficiente de determinaci\'on de la regresi\'on de la variable $j$ contra todas las dem\'as variables.

\section{Ajuste de Par\'ametros}

El ajuste de par\'ametros implica seleccionar los valores \'optimos para los hiperpar\'ametros del modelo. Esto puede mejorar el rendimiento y prevenir el sobreajuste.

\subsection{Grid Search}

El grid search es un m\'etodo exhaustivo para ajustar los par\'ametros. Se define una rejilla de posibles valores de par\'ametros y se eval\'ua el rendimiento del modelo para cada combinaci\'on.

\subsection{Random Search}

El random search selecciona aleatoriamente combinaciones de valores de par\'ametros dentro de un rango especificado. Es menos exhaustivo que el grid search, pero puede ser m\'as eficiente.

\subsection{Bayesian Optimization}

La optimizaci\'on bayesiana utiliza modelos probabil\'isticos para seleccionar iterativamente los valores de par\'ametros m\'as prometedores.

\section{Implementaci\'on en R}

\subsection{Diagn\'ostico del Modelo}

\begin{verbatim}
# Cargar el paquete necesario
library(car)

# Residuos estudentizados
dataTrain$resid <- rstudent(model)
hist(dataTrain$resid, breaks = 20, main = "Residuos Estudentizados")

# Distancia de Cook
dataTrain$cook <- cooks.distance(model)
plot(dataTrain$cook, type = "h", main = "Distancia de Cook")

# Factor de Inflaci\'on de la Varianza
vif_values <- vif(model)
print(vif_values)
\end{verbatim}

\subsection{Ajuste de Par\'ametros}

\begin{verbatim}
# Grid Search con caret
control <- trainControl(method = "cv", number = 10)
tune_grid <- expand.grid(.alpha = c(0, 0.5, 1), .lambda = seq(0.01, 0.1, by = 0.01))

model_tune <- train(var1 ~ ., data = dataTrain, method = "glmnet", 
                    trControl = control, tuneGrid = tune_grid)

print(model_tune)
\end{verbatim}



\section{Introducci\'on}

Interpretar correctamente los resultados de un modelo de regresi\'on log\'istica es esencial para tomar decisiones informadas. Este cap\'itulo se centra en la interpretaci\'on de los coeficientes del modelo, las odds ratios, los intervalos de confianza y la significancia estad\'istica.

\section{Coeficientes de Regresi\'on Log\'istica}

Los coeficientes de regresi\'on log\'istica representan la relaci\'on entre las variables independientes y la variable dependiente en t\'erminos de log-odds. 

\subsection{Interpretaci\'on de los Coeficientes}

Cada coeficiente $\beta_j$ en el modelo de regresi\'on log\'istica se interpreta como el cambio en el log-odds de la variable dependiente por unidad de cambio en la variable independiente $X_j$.

\begin{eqnarray*}
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{eqnarray*}

\subsection{Signo de los Coeficientes}

\begin{itemize}
    \item \textbf{Coeficiente Positivo}: Un coeficiente positivo indica que un aumento en la variable independiente est\'a asociado con un aumento en el log-odds de la variable dependiente.
    \item \textbf{Coeficiente Negativo}: Un coeficiente negativo indica que un aumento en la variable independiente est\'a asociado con una disminuci\'on en el log-odds de la variable dependiente.
\end{itemize}

\section{Odds Ratios}

Las odds ratios proporcionan una interpretaci\'on m\'as intuitiva de los coeficientes de regresi\'on log\'istica. La odds ratio para una variable independiente $X_j$ se calcula como $e^{\beta_j}$.

\subsection{C\'alculo de las Odds Ratios}

\begin{eqnarray*}
\text{OR}_j = e^{\beta_j}
\end{eqnarray*}

\subsection{Interpretaci\'on de las Odds Ratios}

\begin{itemize}
    \item \textbf{OR > 1}: Un OR mayor que 1 indica que un aumento en la variable independiente est\'a asociado con un aumento en las odds de la variable dependiente.
    \item \textbf{OR < 1}: Un OR menor que 1 indica que un aumento en la variable independiente est\'a asociado con una disminuci\'on en las odds de la variable dependiente.
    \item \textbf{OR = 1}: Un OR igual a 1 indica que la variable independiente no tiene efecto sobre las odds de la variable dependiente.
\end{itemize}

\section{Intervalos de Confianza}

Los intervalos de confianza proporcionan una medida de la incertidumbre asociada con los estimadores de los coeficientes. Un intervalo de confianza del 95\% para un coeficiente $\beta_j$ indica que, en el 95\% de las muestras, el intervalo contendr\'a el valor verdadero de $\beta_j$.

\subsection{C\'alculo de los Intervalos de Confianza}

Para calcular un intervalo de confianza del 95\% para un coeficiente $\beta_j$, utilizamos la f\'ormula:
\begin{eqnarray*}
\beta_j \pm 1.96 \cdot \text{SE}(\beta_j)
\end{eqnarray*}
donde $\text{SE}(\beta_j)$ es el error est\'andar de $\beta_j$.

\section{Significancia Estad\'istica}

La significancia estad\'istica se utiliza para determinar si los coeficientes del modelo son significativamente diferentes de cero. Esto se eval\'ua mediante pruebas de hip\'otesis.

\subsection{Prueba de Hip\'otesis}

Para cada coeficiente $\beta_j$, la hip\'otesis nula $H_0$ es que $\beta_j = 0$. La hip\'otesis alternativa $H_a$ es que $\beta_j \neq 0$.

\subsection{P-valor}

El p-valor indica la probabilidad de obtener un coeficiente tan extremo como el observado, asumiendo que la hip\'otesis nula es verdadera. Un p-valor menor que el nivel de significancia $\alpha$ (t\'ipicamente 0.05) indica que podemos rechazar la hip\'otesis nula.

\section{Implementaci\'on en R}

\subsection{C\'alculo de Coeficientes y Odds Ratios}

\begin{verbatim}
# Cargar el paquete necesario
library(broom)

# Entrenar el modelo de regresi\'on log\'istica
model <- glm(var1 ~ ., data = dataTrain, family = "binomial")

# Coeficientes del modelo
coef(model)

# Odds ratios
exp(coef(model))
\end{verbatim}

\subsection{Intervalos de Confianza}

\begin{verbatim}
# Intervalos de confianza para los coeficientes
confint(model)

# Intervalos de confianza para las odds ratios
exp(confint(model))
\end{verbatim}

\subsection{P-valores y Significancia Estad\'istica}

\begin{verbatim}
# Resumen del modelo con p-valores
summary(model)
\end{verbatim}



\section{Introducci\'on}

La regresi\'on log\'istica multinomial y el an\'alisis de supervivencia son extensiones de la regresi\'on log\'istica binaria. Este cap\'itulo se enfoca en las t\'ecnicas y aplicaciones de estos m\'etodos avanzados.

\section{Regresi\'on Log\'istica Multinomial}

La regresi\'on log\'istica multinomial se utiliza cuando la variable dependiente tiene m\'as de dos categor\'ias.

\subsection{Modelo Multinomial}

El modelo de regresi\'on log\'istica multinomial generaliza el modelo binario para manejar m\'ultiples categor\'ias. La probabilidad de que una observaci\'on pertenezca a la categor\'ia $k$ se expresa como:

\begin{eqnarray*}
P(Y = k) = \frac{e^{\beta_{0k} + \beta_{1k} X_1 + \ldots + \beta_{nk} X_n}}{\sum_{j=1}^{K} e^{\beta_{0j} + \beta_{1j} X_1 + \ldots + \beta_{nj} X_n}}
\end{eqnarray*}

\subsection{Estimaci\'on de Par\'ametros}

Los coeficientes del modelo multinomial se estiman utilizando m\'axima verosimilitud, similar a la regresi\'on log\'istica binaria.

\section{An\'alisis de Supervivencia}

El an\'alisis de supervivencia se utiliza para modelar el tiempo hasta que ocurre un evento de inter\'es, como la muerte o la falla de un componente.

\subsection{Funci\'on de Supervivencia}

La funci\'on de supervivencia $S(t)$ describe la probabilidad de que una observaci\'on sobreviva m\'as all\'a del tiempo $t$:

\begin{eqnarray*}
S(t) = P(T > t)
\end{eqnarray*}

\subsection{Modelo de Riesgos Proporcionales de Cox}

El modelo de Cox es un modelo de regresi\'on semiparam\'etrico utilizado para analizar datos de supervivencia:

\begin{eqnarray*}
h(t|X) = h_0(t) e^{\beta_1 X_1 + \ldots + \beta_p X_p}
\end{eqnarray*}
donde $h(t|X)$ es la tasa de riesgo en el tiempo $t$ dado el vector de covariables $X$ y $h_0(t)$ es la tasa de riesgo basal.

\section{Implementaci\'on en R}

\subsection{Regresi\'on Log\'istica Multinomial}

\begin{verbatim}
# Cargar el paquete necesario
library(nnet)

# Entrenar el modelo de regresi\'on log\'istica multinomial
model_multinom <- multinom(var1 ~ ., data = dataTrain)

# Resumen del modelo
summary(model_multinom)
\end{verbatim}

\subsection{An\'alisis de Supervivencia}

\begin{verbatim}
# Cargar el paquete necesario
library(survival)

# Crear el objeto de supervivencia
surv_object <- Surv(time = data$time, event = data$status)

# Ajustar el modelo de Cox
model_cox <- coxph(surv_object ~ var1 + var2, data = data)

# Resumen del modelo
summary(model_cox)
\end{verbatim}



\chapter{Num\'erico}

\section{Conceptos Básicos de la Regresión Logística}

La regresión logística es una técnica de modelado estadístico utilizada para predecir la probabilidad de un evento binario (es decir, un evento que tiene dos posibles resultados) en función de una o más variables independientes. A diferencia de la regresión lineal, que se utiliza para predecir valores continuos, la regresión logística se usa cuando la variable dependiente es categórica.

\section{Diferencias entre Regresión Lineal y Logística}

\subsection{Regresión Lineal}
La regresión lineal busca modelar la relación entre una variable dependiente continua $Y$ y una o más variables independientes $X_1, X_2, \ldots, X_n$ mediante una ecuación de la forma:
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
\]
donde $\beta_0, \beta_1, \ldots, \beta_n$ son los coeficientes del modelo y $\epsilon$ es el término de error.

\subsection{Regresión Logística}
La regresión logística, en cambio, modela la probabilidad de que un evento ocurra (por ejemplo, éxito vs. fracaso) utilizando la función logística. La variable dependiente $Y$ es binaria, tomando valores de 0 o 1. La ecuación de la regresión logística es:
\[
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\]
donde $p$ es la probabilidad de que $Y=1$. La función logística es:
\[
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}
\]

\section{Casos de Uso de la Regresión Logística}

La regresión logística se utiliza en una variedad de campos para problemas de clasificación binaria, tales como:
\begin{itemize}
    \item \textbf{Medicina}: Predicción de la presencia o ausencia de una enfermedad.
    \item \textbf{Marketing}: Determinación de la probabilidad de que un cliente compre un producto.
    \item \textbf{Finanzas}: Evaluación del riesgo de crédito, es decir, si un cliente va a incumplir o no con un préstamo.
    \item \textbf{Seguridad}: Detección de fraudes o intrusiones.
\end{itemize}

\section{Implementación Básica en R}

Para implementar una regresión logística en R, primero es necesario instalar y cargar los paquetes necesarios. Aquí se muestra un ejemplo básico de implementación:

\subsection{Instalación y Configuración de R y RStudio}
\begin{itemize}
    \item Descargue e instale R desde \texttt{https://cran.r-project.org/}.
    \item Descargue e instale RStudio desde \texttt{https://rstudio.com/products/rstudio/download/}.
\end{itemize}

\subsection{Introducción Básica a R}
\begin{itemize}
    \item Sintaxis básica de R.
    \item Operaciones básicas: asignación, operaciones aritméticas, funciones básicas.
\end{itemize}

\subsection{Ejemplo de Regresión Logística en R}

\begin{verbatim}
# Instalación del paquete necesario
install.packages("stats")

# Carga del paquete
library(stats)

# Ejemplo de conjunto de datos
data <- data.frame(
  outcome = c(1, 0, 1, 0, 1, 1, 0, 1, 0, 0),
  predictor = c(2.3, 1.9, 3.1, 2.8, 3.6, 2.4, 2.1, 3.3, 2.2, 1.7)
)

# Ajuste del modelo de regresión logística
model <- glm(outcome ~ predictor, data = data, family = binomial)

# Resumen del modelo
summary(model)
\end{verbatim}

En este ejemplo, se utiliza el conjunto de datos `data` que contiene una variable de resultado binaria `outcome` y una variable predictora continua `predictor`. El modelo de regresión logística se ajusta utilizando la función \texttt{glm} con la familia binomial.


\section{D\'ia 1: Regresión Logística}

\subsection*{Implementación Básica en R}

Para implementar una regresión logística en R, primero es necesario instalar y cargar los paquetes necesarios.

\subsection*{Instalación y Configuración de R y RStudio}
\begin{itemize}
    \item Descargue e instale R desde \texttt{https://cran.r-project.org/}. Siga las instrucciones para su sistema operativo (Windows, MacOS, Linux).
    \item Descargue e instale RStudio desde \texttt{https://rstudio.com/products/rstudio/download/}. 
\end{itemize}

\subsection{Ejemplo de Regresión Logística en R}

A continuación, se muestra un ejemplo de cómo ajustar un modelo de regresión logística en R utilizando un conjunto de datos simulado. El ejemplo incluye la instalación del paquete necesario, la carga de datos, el ajuste del modelo, y la interpretación de los resultados.

\begin{verbatim}
# Instalación del paquete necesario
install.packages("stats")

# Carga del paquete
library(stats)

# Ejemplo de conjunto de datos
data <- data.frame(
  outcome = c(1, 0, 1, 0, 1, 1, 0, 1, 0, 0),
  predictor = c(2.3, 1.9, 3.1, 2.8, 3.6, 2.4, 2.1, 3.3, 2.2, 1.7)
)

# Ajuste del modelo de regresión logística
model <- glm(outcome ~ predictor, data = data, family = binomial)

# Resumen del modelo
summary(model)
\end{verbatim}

En este ejemplo, se utiliza el conjunto de datos \textit{data} que contiene una variable de resultado binaria \textit{outcome} y una variable predictora continua \textit{predictor}. El modelo de regresión logística se ajusta utilizando la función \texttt{glm} con la familia binomial. La función \texttt{summary(model)} proporciona un resumen del modelo ajustado, incluyendo los coeficientes estimados, sus errores estándar, valores z, y p-valores.

\begin{itemize}
    \item \textbf{Coeficientes}: Los coeficientes estimados $\beta_0$ y $\beta_1$ indican la dirección y magnitud de la relación entre las variables predictoras y la probabilidad del resultado.
    \item \textbf{Errores Estándar}: Los errores estándar proporcionan una medida de la precisión de los coeficientes estimados.
    \item \textbf{Valores z y p-valores}: Los valores z y p-valores se utilizan para evaluar la significancia estadística de los coeficientes. Un p-valor pequeño (generalmente < 0.05) indica que el coeficiente es significativamente diferente de cero.
\end{itemize}

Este es solo un ejemplo básico, en aplicaciones reales, es posible que necesites realizar más análisis y validaciones, como la evaluación de la bondad de ajuste del modelo, el diagnóstico de posibles problemas de multicolinealidad, y la validación cruzada del modelo.

\begin{verbatim}
# Archivo: regresionlogistica.R

# Instalación del paquete necesario
#install.packages("stats")

# Carga del paquete
library(stats)

# Fijar la semilla para reproducibilidad
set.seed(123)

# Número de observaciones
n <- 100

# Generar las variables independientes X1, X2, ..., X15
# Creamos una matriz de tamaño n x 15 con valores generados aleatoriamente de una
 distribución normal
X <- as.data.frame(matrix(rnorm(n * 15), nrow = n, ncol = 15))
colnames(X) <- paste0("X", 1:15)  # Nombramos las columnas como X1, X2, ..., X15

# Coeficientes verdaderos para las variables independientes
# Generamos un vector de 16 coeficientes (incluyendo el intercepto) aleatorios entre -1 y 1
beta <- runif(16, -1, 1)  # 15 coeficientes más el intercepto

# Generar el término lineal
# Calculamos el término lineal utilizando los coeficientes y las variables independientes
linear_term <- beta[1] + as.matrix(X) %*% beta[-1]

# Generar la probabilidad utilizando la función logística
# Calculamos las probabilidades utilizando la función logística
p <- 1 / (1 + exp(-linear_term))

# Generar la variable dependiente binaria Y
# Generamos valores binarios (0 o 1) utilizando las probabilidades calculadas
Y <- rbinom(n, 1, p)

# Combinar las variables independientes y la variable dependiente en un data frame
data <- cbind(Y, X)

# Dividir el conjunto de datos en entrenamiento y prueba
set.seed(123)  # Fijar la semilla para reproducibilidad
train_indices <- sample(1:n, size = 0.7 * n)  # 70% de los datos para entrenamiento
train_set <- data[train_indices, ]  # Conjunto de entrenamiento
test_set <- data[-train_indices, ]  # Conjunto de prueba

# Ajuste del modelo de regresión logística en el conjunto de entrenamiento
# Ajustamos un modelo de regresión logística utilizando las variables independientes
para predecir Y
model <- glm(Y ~ ., data = train_set, family = binomial)

# Resumen del modelo
# Mostramos un resumen del modelo ajustado
summary(model)

# Guardar el modelo y los resultados en un archivo
# Guardamos el modelo ajustado en un archivo .RData
save(model, file = "regresion_logistica_modelo.RData")

# Guardar los datos simulados en archivos CSV
# Guardamos los conjuntos de datos de entrenamiento y prueba en archivos CSV
write.csv(train_set, "datos_entrenamiento_regresion_logistica.csv", row.names = FALSE)
write.csv(test_set, "datos_prueba_regresion_logistica.csv", row.names = FALSE)

# Hacer predicciones en el conjunto de prueba
# Utilizamos el modelo ajustado para hacer predicciones en el conjunto de prueba
test_set$prob_pred <- predict(model, newdata = test_set, type = "response")
test_set$Y_pred <- ifelse(test_set$prob_pred > 0.5, 1, 0)  
# Convertimos probabilidades a clases binarias

# Calcular la precisión de las predicciones
# Calculamos la precisión de las predicciones comparando con los valores reales de Y
accuracy <- mean(test_set$Y_pred == test_set$Y)
cat("La precisión del modelo en el conjunto de prueba es:", accuracy, "\n")

# Guardar las predicciones en un archivo CSV
# Guardamos las predicciones y las probabilidades predichas en un archivo CSV
write.csv(test_set, "predicciones_regresion_logistica.csv", row.names = FALSE)

# Graficar los coeficientes estimados
# Graficamos los coeficientes estimados del modelo ajustado
plot(coef(model), main = "Coeficientes Estimados del Modelo de Regresión Logística", 
     xlab = "Variables", ylab = "Coeficientes", type = "h", col = "blue")
abline(h = 0, col = "red", lwd = 2)

# Mostrar un mensaje indicando que el proceso ha finalizado
cat("El modelo de regresión logística se ha ajustado, se han hecho predicciones y los resultados se han guardado en 'regresion_logistica_modelo.RData'.\n")
\end{verbatim}

\subsection{Aplicación a Datos de Cáncer - Parte I}

A continuación, se muestra un ejemplo de cómo ajustar un modelo de regresión logística en R utilizando el conjunto de datos del cáncer de mama de Wisconsin.

\begin{verbatim}
# Archivo: regresionlogistica_cancer.R

# Instalación del paquete necesario
install.packages("mlbench")
install.packages("dplyr")

# Carga de los paquetes
library(mlbench)
library(dplyr)

# Cargar el conjunto de datos BreastCancer
data("BreastCancer")

# Ver las primeras filas del conjunto de datos
head(BreastCancer)

# Preprocesamiento de los datos
# Eliminar la columna de identificación y filas con valores faltantes
breast_cancer_clean <- BreastCancer %>%
  select(-Id) %>%
  na.omit()

# Convertir la variable 'Class' a factor binario
breast_cancer_clean$Class <- ifelse(breast_cancer_clean$Class == "malignant", 1, 0)
breast_cancer_clean$Class <- as.factor(breast_cancer_clean$Class)

# Convertir las demás columnas a numéricas
breast_cancer_clean[, 1:9] <- lapply(breast_cancer_clean[, 1:9], as.numeric)

# Dividir el conjunto de datos en entrenamiento (70%) y prueba (30%)
set.seed(123)
train_indices <- sample(1:nrow(breast_cancer_clean), size = 0.7 * nrow(breast_cancer_clean))
train_set <- breast_cancer_clean[train_indices, ]
test_set <- breast_cancer_clean[-train_indices, ]

# Ajuste del modelo de regresión logística en el conjunto de entrenamiento
model <- glm(Class ~ ., data = train_set, family = binomial)

# Resumen del modelo
summary(model)

# Guardar el modelo y los resultados en un archivo
save(model, file = "regresion_logistica_cancer_modelo.RData")

# Guardar los datos simulados en archivos CSV
write.csv(train_set, "datos_entrenamiento_cancer.csv", row.names = FALSE)
write.csv(test_set, "datos_prueba_cancer.csv", row.names = FALSE)

# Hacer predicciones en el conjunto de prueba
test_set$prob_pred <- predict(model, newdata = test_set, type = "response")
test_set$Class_pred <- ifelse(test_set$prob_pred > 0.5, 1, 0)

# Calcular la precisión de las predicciones
accuracy <- mean(test_set$Class_pred == test_set$Class)
cat("La precisión del modelo en el conjunto de prueba es:", accuracy, "\n")

# Guardar las predicciones en un archivo CSV
write.csv(test_set, "predicciones_cancer.csv", row.names = FALSE)

# Graficar los coeficientes estimados
plot(coef(model), main = "Coeficientes Estimados del Modelo de Regresión Logística", 
     xlab = "Variables", ylab = "Coeficientes", type = "h", col = "blue")
abline(h = 0, col = "red", lwd = 2)

# Mostrar un mensaje indicando que el proceso ha finalizado
cat("El modelo de regresión logística se ha ajustado, se han hecho predicciones y los resultados se han guardado en 'regresion_logistica_cancer_modelo.RData'.\n")
\end{verbatim}

\subsubsection*{Descripción del Código}

\textbf{Instalación y Carga de Paquetes:}

Instalamos y cargamos el paquete \texttt{stats} necesario para la regresión logística.

\textbf{Generación de Datos Simulados:}

\begin{itemize}
    \item Fijamos una semilla para la reproducibilidad.
    \item Generamos un conjunto de datos con 100 observaciones y 15 variables independientes (\texttt{X1, X2, ..., X15}) usando una distribución normal.
    \item Definimos los coeficientes verdaderos para las variables independientes y calculamos el término lineal.
    \item Calculamos las probabilidades usando la función logística y generamos una variable dependiente binaria \texttt{Y} basada en esas probabilidades.
    \item Combinamos las variables independientes y la variable dependiente en un \texttt{data frame}.
\end{itemize}

\textbf{División de Datos en Conjuntos de Entrenamiento y Prueba:}

\begin{itemize}
    \item Dividimos los datos en un conjunto de entrenamiento (70\%) y un conjunto de prueba (30\%).
\end{itemize}

\textbf{Ajuste del Modelo de Regresión Logística:}

\begin{itemize}
    \item Ajustamos un modelo de regresión logística en el conjunto de entrenamiento.
    \item Mostramos un resumen del modelo ajustado.
\end{itemize}

\textbf{Guardado de Datos y Modelo:}

\begin{itemize}
    \item Guardamos el modelo ajustado en un archivo \texttt{.RData}.
    \item Guardamos los conjuntos de datos de entrenamiento y prueba en archivos CSV.
\end{itemize}

\textbf{Predicciones y Evaluación del Modelo:}

\begin{itemize}
    \item Hacemos predicciones en el conjunto de prueba utilizando el modelo ajustado.
    \item Calculamos la precisión de las predicciones comparando con los valores reales de \texttt{Y}.
    \item Guardamos las predicciones y las probabilidades predichas en un archivo CSV.
\end{itemize}

\textbf{Visualización de los Coeficientes del Modelo:}

\begin{itemize}
    \item Graficamos los coeficientes estimados del modelo ajustado.
    \item Mostramos un mensaje indicando que el proceso ha finalizado.
\end{itemize}

Para ejecutar este script, guarda el código en un archivo llamado \textit{regresionlogistica.R}, abre R o RStudio, navega hasta el directorio donde guardaste el archivo y ejecuta el script usando \textit{source("regresionlogistica.R")}.

\subsubsection{Ejemplo Titanic}

Cuando realizas una regresión logística, obtienes coeficientes para cada variable independiente en tu modelo. Estos coeficientes indican la dirección y la magnitud de la relación entre cada variable independiente y la variable dependiente (en este caso, \textit{Survived}).

\subsubsection*{Interpretación de los Coeficientes}

\begin{itemize}
    \item \textbf{Intercepto} (\textit{(Intercept)}): Este coeficiente representa el logaritmo de las probabilidades (log-odds) de que \textit{Survived} sea 1 (supervivencia) cuando todas las variables independientes son cero.
    \item \textbf{Pclass}: El coeficiente asociado con \textit{Pclass} indica cómo cambia el log-odds de supervivencia con cada incremento en la clase del pasajero. Si el coeficiente es negativo, sugiere que una clase más alta (por ejemplo, de primera clase a tercera clase) reduce las probabilidades de supervivencia.
    \item \textbf{Sex}: Este coeficiente muestra el efecto de ser hombre o mujer en las probabilidades de supervivencia. Generalmente, se espera que el coeficiente sea positivo para \textit{female} indicando que las mujeres tenían mayores probabilidades de sobrevivir.
    \item \textbf{Age}: El coeficiente de \textit{Age} indica cómo cambia el log-odds de supervivencia con cada año de incremento en la edad. Un coeficiente negativo sugiere que la probabilidad de supervivencia disminuye con la edad.
    \item \textbf{SibSp} y \textbf{Parch}: Estos coeficientes indican el efecto del número de hermanos/cónyuges a bordo y padres/hijos a bordo en las probabilidades de supervivencia.
    \item \textbf{Fare}: Este coeficiente indica el efecto del precio del billete en las probabilidades de supervivencia. Un coeficiente positivo sugiere que pagar más por el billete se asocia con mayores probabilidades de supervivencia.
\end{itemize}

\subsubsection*{Estadísticas de Ajuste del Modelo}

El resumen del modelo (\textit{summary(model)}) incluye varias estadísticas importantes:

\begin{itemize}
    \item \textbf{Estadísticos z y p-valores}: Estas estadísticas indican la significancia de cada coeficiente. Un p-valor bajo (generalmente < 0.05) sugiere que la variable es un predictor significativo de la variable dependiente.
    \item \textbf{Desviación Residual}: La desviación residual mide la calidad del ajuste del modelo. Valores más bajos indican un mejor ajuste.
    \item \textbf{AIC (Akaike Information Criterion)}: El AIC es una medida de la calidad del modelo que toma en cuenta tanto la bondad del ajuste como la complejidad del modelo. Modelos con AIC más bajo son preferidos.
\end{itemize}

\subsubsection*{Precisión del Modelo}

La precisión del modelo en el conjunto de prueba es una métrica importante para evaluar el rendimiento del modelo. La precisión se calcula como el número de predicciones correctas dividido por el número total de predicciones.

\subsubsection*{Ejemplo de Resultados}

Supongamos que la precisión del modelo es 0.78 (78\%). Esto significa que el modelo correctamente predijo el estado de supervivencia del 78\% de los pasajeros en el conjunto de prueba.

\subsubsection*{Matriz de Confusión y Otras Métricas}

Además de la precisión, otras métricas como la matriz de confusión, la sensibilidad, la especificidad, y el área bajo la curva ROC (AUC-ROC) también pueden proporcionar una visión más completa del rendimiento del modelo.

\subsubsection*{Matriz de Confusión}

\begin{itemize}
    \item \textbf{Verdaderos Positivos (TP)}: Número de pasajeros que sobrevivieron y fueron predichos como sobrevivientes.
    \item \textbf{Verdaderos Negativos (TN)}: Número de pasajeros que no sobrevivieron y fueron predichos como no sobrevivientes.
    \item \textbf{Falsos Positivos (FP)}: Número de pasajeros que no sobrevivieron pero fueron predichos como sobrevivientes.
    \item \textbf{Falsos Negativos (FN)}: Número de pasajeros que sobrevivieron pero fueron predichos como no sobrevivientes.
\end{itemize}

\subsubsection*{Ejemplo de Cálculo de Métricas}

\begin{verbatim}
# Calcular la matriz de confusión
table(test_set$Survived, test_set$Survived_pred)

# Calcular sensibilidad y especificidad
sensitivity <- sum(test_set$Survived == 1 & test_set$Survived_pred == 1) / sum(test_set$Survived == 1)
specificity <- sum(test_set$Survived == 0 & test_set$Survived_pred == 0) / sum(test_set$Survived == 0)

# Calcular AUC-ROC
library(pROC)
roc_curve <- roc(test_set$Survived, test_set$prob_pred)
auc(roc_curve)
\end{verbatim}

\subsubsection*{Visualización de Resultados}

Graficar los coeficientes del modelo, la curva ROC y otras visualizaciones ayudan a entender mejor el rendimiento y la importancia de cada variable en el modelo.

\begin{verbatim}
# Graficar la curva ROC
plot(roc_curve, main = "Curva ROC para el Modelo de Regresión Logística")
\end{verbatim}

\subsubsection*{Resumen Final}

El modelo de regresión logística aplicado al conjunto de datos del Titanic proporciona una forma de entender cómo diferentes características de los pasajeros influyen en sus probabilidades de supervivencia. La interpretación de los coeficientes del modelo, las estadísticas de ajuste, y la precisión del modelo en el conjunto de prueba son fundamentales para evaluar el rendimiento y la utilidad del modelo en hacer predicciones sobre la supervivencia de los pasajeros del Titanic.

\subsection{Simulaci\'on de Datos de Cáncer - Parte II}

Aquí se presenta un ejemplo de cómo realizar una regresión logística utilizando datos simulados de pacientes con cáncer.

\begin{verbatim}
#---- Archivo: cancerLogRegSimulado.R ----

# Instalación del paquete necesario
if (!requireNamespace("dplyr", quietly = TRUE)) {
  install.packages("dplyr")
}

# Carga del paquete
library(dplyr)

# Fijar la semilla para reproducibilidad
set.seed(123)

# Número de observaciones
n <- 150

# Generar las variables independientes X1, X2, ..., X15
# Creamos una matriz de tamaño n x 15 con valores generados aleatoriamente de una 
distribución normal
X <- as.data.frame(matrix(rnorm(n * 15), nrow = n, ncol = 15))
colnames(X) <- paste0("X", 1:15)  # Nombramos las columnas como X1, X2, ..., X15

# Coeficientes verdaderos para las variables independientes
# Generamos un vector de 16 coeficientes (incluyendo el intercepto) aleatorios entre -1 y 1
beta <- runif(16, -1, 1)  # 15 coeficientes más el intercepto

# Generar el término lineal
# Calculamos el término lineal utilizando los coeficientes y las variables independientes
linear_term <- beta[1] + as.matrix(X) %*% beta[-1]

# Generar la probabilidad utilizando la función logística
# Calculamos las probabilidades utilizando la función logística
p <- 1 / (1 + exp(-linear_term))

# Generar la variable dependiente binaria Y
# Generamos valores binarios (0 o 1) utilizando las probabilidades calculadas
Y <- rbinom(n, 1, p)

# Combinar las variables independientes y la variable dependiente en un data frame
data <- cbind(Y, X)

# Dividir el conjunto de datos en entrenamiento y prueba
set.seed(123)  # Fijar la semilla para reproducibilidad
train_indices <- sample(1:n, size = 0.7 * n)  # 70% de los datos para entrenamiento
train_set <- data[train_indices, ]  # Conjunto de entrenamiento
test_set <- data[-train_indices, ]  # Conjunto de prueba

# Ajuste del modelo de regresión logística en el conjunto de entrenamiento
# Ajustamos un modelo de regresión logística utilizando las variables independientes 
para predecir Y
model <- glm(Y ~ ., data = train_set, family = binomial)

# Resumen del modelo
# Mostramos un resumen del modelo ajustado
summary(model)

# Guardar el modelo y los resultados en un archivo
# Guardamos el modelo ajustado en un archivo .RData
save(model, file = "regresion_logistica_cancer_modelo_simulado.RData")

# Guardar los datos simulados en archivos CSV
# Guardamos los conjuntos de datos de entrenamiento y prueba en archivos CSV
write.csv(train_set, "datos_entrenamiento_cancer_simulado.csv", row.names = FALSE)
write.csv(test_set, "datos_prueba_cancer_simulado.csv", row.names = FALSE)

# Hacer predicciones en el conjunto de prueba
# Utilizamos el modelo ajustado para hacer predicciones en el conjunto de prueba
test_set$prob_pred <- predict(model, newdata = test_set, type = "response")
test_set$Y_pred <- ifelse(test_set$prob_pred > 0.5, 1, 0)  
# Convertimos probabilidades a clases binarias

# Calcular la precisión de las predicciones
# Calculamos la precisión de las predicciones comparando con los valores reales de Y
accuracy <- mean(test_set$Y_pred == test_set$Y)
cat("La precisión del modelo en el conjunto de prueba es:", accuracy, "\n")

# Guardar las predicciones en un archivo CSV
# Guardamos las predicciones y las probabilidades predichas en un archivo CSV
write.csv(test_set, "predicciones_cancer_simulado.csv", row.names = FALSE)

# Graficar los coeficientes estimados
# Graficamos los coeficientes estimados del modelo ajustado
plot(coef(model), main = "Coeficientes Estimados del Modelo de Regresión Logística", 
     xlab = "Variables", ylab = "Coeficientes", type = "h", col = "blue")
abline(h = 0, col = "red", lwd = 2)

# Mostrar un mensaje indicando que el proceso ha finalizado
cat("El modelo de regresión logística se ha ajustado, se han hecho predicciones 
y los resultados se han guardado en 'regresion_logistica_cancer_modelo_simulado.RData'.\n")
\end{verbatim}

\subsection{Simulaci\'on de Datos de Cáncer - Parte III}

En un estudio sobre cáncer, especialmente en el contexto del cáncer de mama, las principales mediciones suelen incluir una variedad de características clínicas y patológicas. Aquí hay algunas de las principales mediciones que se tienen en cuenta:

\begin{itemize}
    \item \textbf{Tamaño del Tumor}: Medición del diámetro del tumor.
    \item \textbf{Estado de los Ganglios Linfáticos}: Número de ganglios linfáticos afectados.
    \item \textbf{Grado del Tumor}: Clasificación del tumor basada en la apariencia de las células cancerosas.
    \item \textbf{Receptores Hormonales}: Estado de los receptores de estrógeno y progesterona.
    \item \textbf{Estado HER2}: Expresión del receptor 2 del factor de crecimiento epidérmico humano.
    \item \textbf{Ki-67}: Índice de proliferación celular.
    \item \textbf{Edad del Paciente}: Edad en el momento del diagnóstico.
    \item \textbf{Histopatología}: Tipo y subtipo histológico del cáncer.
    \item \textbf{Márgenes Quirúrgicos}: Estado de los márgenes después de la cirugía (si están libres de cáncer o no).
    \item \textbf{Invasión Linfovascular}: Presencia de células cancerosas en los vasos linfáticos o sanguíneos.
    \item \textbf{Tratamientos Previos}: Tipos de tratamientos recibidos antes del diagnóstico (quimioterapia, radioterapia, etc.).
    \item \textbf{Tipo de Cirugía}: Tipo de procedimiento quirúrgico realizado (mastectomía, lumpectomía, etc.).
    \item \textbf{Metástasis}: Presencia de metástasis y ubicación de las mismas.
    \item \textbf{Índice de Masa Corporal (IMC)}: Relación entre el peso y la altura del paciente.
    \item \textbf{Marcadores Genéticos}: Presencia de mutaciones genéticas específicas (BRCA1, BRCA2, etc.).
\end{itemize}

Estas mediciones proporcionan una visión integral del estado del cáncer y se utilizan para planificar el tratamiento y predecir el pronóstico.

A continuación, se muestra un ejemplo de cómo ajustar un modelo de regresión logística en R utilizando un conjunto de datos simulado con estas mediciones.

\begin{verbatim}
# Archivo: simulcorrectedCancer.R

# Instalación del paquete necesario
if (!requireNamespace("dplyr", quietly = TRUE)) {
  install.packages("dplyr")
}

# Carga del paquete
library(dplyr)

# Fijar la semilla para reproducibilidad
set.seed(123)

# Número de observaciones
n <- 1500

# Simulación de las variables independientes
# Tamaño del Tumor (en cm)
Tumor_Size <- rnorm(n, mean = 3, sd = 1.5)

# Estado de los Ganglios Linfáticos (número de ganglios afectados)
Lymph_Nodes <- rpois(n, lambda = 3)

# Grado del Tumor (1 a 3)
Tumor_Grade <- sample(1:3, n, replace = TRUE)

# Receptores Hormonales (0: negativo, 1: positivo)
Estrogen_Receptor <- rbinom(n, 1, 0.7)
Progesterone_Receptor <- rbinom(n, 1, 0.7)

# Estado HER2 (0: negativo, 1: positivo)
HER2_Status <- rbinom(n, 1, 0.3)

# Ki-67 (% de células proliferativas)
Ki_67 <- rnorm(n, mean = 20, sd = 10)

# Edad del Paciente (años)
Age <- rnorm(n, mean = 50, sd = 10)

# Histopatología (1: ductal, 2: lobular, 3: otros)
Histopathology <- sample(1:3, n, replace = TRUE)

# Márgenes Quirúrgicos (0: positivo, 1: negativo)
Surgical_Margins <- rbinom(n, 1, 0.8)

# Invasión Linfovascular (0: no, 1: sí)
Lymphovascular_Invasion <- rbinom(n, 1, 0.4)

# Tratamientos Previos (0: no, 1: sí)
Prior_Treatments <- rbinom(n, 1, 0.5)

# Tipo de Cirugía (0: mastectomía, 1: lumpectomía)
Surgery_Type <- rbinom(n, 1, 0.5)

# Metástasis (0: no, 1: sí)
Metastasis <- rbinom(n, 1, 0.2)

# Índice de Masa Corporal (IMC)
BMI <- rnorm(n, mean = 25, sd = 5)

# Marcadores Genéticos (0: negativo, 1: positivo)
Genetic_Markers <- rbinom(n, 1, 0.1)

# Generar la variable dependiente binaria Y (sobrevivencia 0: no, 1: sí)
# Utilizaremos una combinación arbitraria de las variables para generar Y
linear_term <- -1 + 0.5 * Tumor_Size - 0.3 * Lymph_Nodes + 0.2 * Tumor_Grade + 
  0.4 * Estrogen_Receptor + 0.3 * Progesterone_Receptor - 0.2 * HER2_Status + 
  0.1 * Ki_67 - 0.05 * Age + 0.3 * Surgical_Margins - 0.4 * Lymphovascular_Invasion +
  0.2 * Prior_Treatments + 0.1 * Surgery_Type - 0.5 * Metastasis + 0.01 * BMI + 
  0.2 * Genetic_Markers
p <- 1 / (1 + exp(-linear_term))
Y <- rbinom(n, 1, p)

# Combinar las variables independientes y la variable dependiente en un data frame
data <- data.frame(Y, Tumor_Size, Lymph_Nodes, Tumor_Grade, Estrogen_Receptor, 
                   Progesterone_Receptor, HER2_Status, Ki_67, Age, Histopathology,
                   Surgical_Margins, Lymphovascular_Invasion, Prior_Treatments,
                   Surgery_Type, Metastasis, BMI, Genetic_Markers)

# Dividir el conjunto de datos en entrenamiento y prueba
set.seed(123)  # Fijar la semilla para reproducibilidad
train_indices <- sample(1:n, size = 0.7 * n)  # 70% de los datos para entrenamiento
train_set <- data[train_indices, ]  # Conjunto de entrenamiento
test_set <- data[-train_indices, ]  # Conjunto de prueba

# Ajuste del modelo de regresión logística en el conjunto de entrenamiento
# Ajustamos un modelo de regresión logística utilizando las variables independientes para
 predecir Y
model <- glm(Y ~ ., data = train_set, family = binomial)

# Resumen del modelo
# Mostramos un resumen del modelo ajustado
summary(model)

# Guardar el modelo y los resultados en un archivo
# Guardamos el modelo ajustado en un archivo .RData
save(model, file = "regresion_logistica_cancer_modelo_simulado.RData")

# Guardar los datos simulados en archivos CSV
# Guardamos los conjuntos de datos de entrenamiento y prueba en archivos CSV
write.csv(train_set, "datos_entrenamiento_cancer_simulado.csv", row.names = FALSE)
write.csv(test_set, "datos_prueba_cancer_simulado.csv", row.names = FALSE)

# Hacer predicciones en el conjunto de prueba
# Utilizamos el modelo ajustado para hacer predicciones en el conjunto de prueba
test_set$prob_pred <- predict(model, newdata = test_set, type = "response")
test_set$Y_pred <- ifelse(test_set$prob_pred > 0.5, 1, 0)  
# Convertimos probabilidades a clases binarias

# Calcular la precisión de las predicciones
# Calculamos la precisión de las predicciones comparando con los valores reales de Y
accuracy <- mean(test_set$Y_pred == test_set$Y)
cat("La precisión del modelo en el conjunto de prueba es:", accuracy, "\n")

# Guardar las predicciones en un archivo CSV
# Guardamos las predicciones y las probabilidades predichas en un archivo CSV
write.csv(test_set, "predicciones_cancer_simulado.csv", row.names = FALSE)

# Graficar los coeficientes estimados
# Graficamos los coeficientes estimados del modelo ajustado
plot(coef(model), main = "Coeficientes Estimados del Modelo de Regresión Logística", 
     xlab = "Variables", ylab = "Coeficientes", type = "h", col = "blue")
abline(h = 0, col = "red", lwd = 2)

# Mostrar un mensaje indicando que el proceso ha finalizado
cat("El modelo de regresión logística se ha ajustado, se han hecho predicciones 
y los resultados se han guardado en 'regresion_logistica_cancer_modelo_simulado.RData'.\n")
\end{verbatim}






\begin{thebibliography}{99}

\bibitem{darlington1990}%1
Darlington RB. \textit{Regression and Linear Models}. Columbus, OH: McGraw-Hill Publishing Company, 1990.

\bibitem{tabachnick2007}%2
Tabachnick BG, Fidell LS. \textit{Using Multivariate Statistics}. 5th ed. Boston, MA: Pearson Education, Inc., 2007.

\bibitem{hosmer2000}%3
Hosmer DW, Lemeshow SL. \textit{Applied Logistic Regression}. 2nd ed. Hoboken, NJ: Wiley-Interscience, 2000.

\bibitem{campbell1963}%4
Campbell DT, Stanley JC. \textit{Experimental and Quasi-experimental Designs for Research}. Boston, MA: Houghton Mifflin Co., 1963.

\bibitem{stokes2000}%5
Stokes ME, Davis CS, Koch GG. \textit{Categorical Data Analysis Using the SAS System}. 2nd ed. Cary, NC: SAS Institute, Inc., 2000.

\bibitem{newgard2004}%6
Newgard CD, Hedges JR, Arthur M, Mullins RJ. Advanced statistics: the propensity score—a method for estimating treatment effect in observational research. \textit{Acad Emerg Med}. 2004; \textbf{11}:953–961.

\bibitem{newgard2007}%7
Newgard CD, Haukoos JS. Advanced statistics: missing data in clinical research—part 2: multiple imputation. \textit{Acad Emerg Med}. 2007; \textbf{14}:669–678.

\bibitem{allison1999}%8
Allison PD. \textit{Logistic Regression Using the SAS System: Theory and Application}. Cary, NC: SAS Institute, Inc., 1999.

\bibitem{peduzzi1996}%9
Peduzzi P, Concato J, Kemper E, Holford TR, Feinstein AR. A simulation study of the number of events per variable in logistic regression analysis. \textit{J Clin Epidemiol}. 1996; \textbf{49}:1373–1379.

\bibitem{agresti2007}%10
Agresti A. \textit{An Introduction to Categorical Data Analysis}. Hoboken, NJ: Wiley, 2007.

\bibitem{feinstein1996}%11
Feinstein AR. \textit{Multivariable Analysis: An Introduction}. New Haven, CT: Yale University Press, 1996.

\bibitem{altman2000}%12
Altman DG, Royston P. What Do We Mean by Validating a Prognostic Model? \textit{Stats Med}. 2000; \textbf{19}:453–473.

\bibitem{kohavi1995}%13
Kohavi R. A study of cross-validation and bootstrap for accuracy estimation and model selection. In: \textit{Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI)}. Montreal, Quebec, Canada, August 20–25, 1995. 1995:1137–1143.

\bibitem{efron1993}%14
Efron B, Tibshirani R. \textit{An Introduction to the Bootstrap}. New York: Chapman \& Hall, 1993.

\bibitem{miller1991}%15
Miller ME, Hiu SL, Tierney WM. Validation techniques for logistic regression models. \textit{Stat Med}. 1991; \textbf{10}:1213–1226.

\bibitem{hosmer1997}%16
Hosmer DW, Hosmer T, Le Cessie S, Lemeshow S. A comparison of goodness-of-fit tests for the logistic regression model. \textit{Stat Med}. 1997; \textbf{16}:965–980.

\bibitem{kuss2002}%17
Kuss O. Global goodness-of-fit tests in logistic regression with sparse data. \textit{Stat Med}. 2002; \textbf{21}:3789–3801.

\bibitem{zou2007}%18
Zou KH, O'Malley AJ, Mauri L. Receiver-operating characteristic analysis for evaluating diagnostic tests and predictive models. \textit{Circulation}. 2007; \textbf{115}:654–657.
\bibitem{Mazurenko}Mazurenko, S., Prokop, Z., and Damborsky, J. (2019). Machine learning in enzyme engineering. ACS Catalysis, 10(2), 1210-1223. 

\bibitem{15} Yang, Y.; Niroula, A.; Shen, B.; Vihinen, M. PON-Sol: Prediction of Effects of Amino Acid Substitutions on Protein Solubility. Bioinformatics 2016, 32, 2032!2034.

\bibitem{16} Folkman, L.; Stantic, B.; Sattar, A.; Zhou, Y. EASE-MM: Sequence-Based Prediction of Mutation-Induced Stability Changes with Feature-Based Multiple Models. J. Mol. Biol. 2016, 428, 1394! 1405.

\bibitem{17} Teng, S.; Srivastava, A. K.; Wang, L. Sequence Feature-Based Prediction of Protein Stability Changes upon Amino Acid Substitutions. BMC Genomics 2010, 11, S5.

\bibitem{18} Huang, L.; Gromiha, M. M.; Ho, S. iPTREE-STAB: Interpretable Decision Tree Based Method for Predicting Protein Stability Changes Upon Mutations. Bioinformatics 2007, 23, 1292! 1293.


\bibitem{19} Koskinen, P.; Toronen, P.; Nokso-Koivisto, J.; Holm, L. PANNZER: High-Throughput Functional Annotation of Uncharac- terized Proteins in an Error-Prone Environment. Bioinformatics 2015, 31, 1544!1552.

\bibitem{20} De Ferrari, L.; Mitchell, J. B. From Sequence to Enzyme Mechanism Using Multi-Label Machine Learning. BMC Bioinf. 2014, 15, 150.

\bibitem{21} Falda, M.; Toppo, S.; Pescarolo, A.; Lavezzo, E.; Di Camillo, B.; Facchinetti, A.; Cilia, E.; Velasco, R.; Fontana, P. Argot2: A Large Scale Function Prediction Tool Relying on Semantic Similarity of Weighted Gene Ontology Terms. BMC Bioinf. 2012, 13, S14.

\bibitem{22} Cozzetto, D.; Buchan, D. W.; Bryson, K.; Jones, D. T. Protein Function Prediction by Massive Integration of Evolutionary Analyses and Multiple Data Sources. BMC Bioinf. 2013, 14, S1.

\bibitem{47} Kulski, J. Next Generation Sequencing: Advances, Applications and Challenges; InTechOpen: London, 2016.

\bibitem{48} Straiton, J.; Free, T.; Sawyer, A.; Martin, J. From Sanger Sequencing to Genome Databases and Beyond. BioTechniques 2019, 66, 60-63.

\bibitem{50} Ardui, S.; Ameur, A.; Vermeesch, J. R.; Hestand, M. S. Single Molecule Real-Time (SMRT) Sequencing Comes of Age: Applications and Utilities for Medical Diagnostics. Nucleic Acids Res. 2018, 46, 2159-2168.

\bibitem{51}Kono, N., and Arakawa, K. (2019). Nanopore sequencing: Review of potential applications in functional genomics. Development, growth and differentiation, 61(5), 316-326.

\bibitem{52} Bunzel, H. A., Garrabou, X., Pott, M., and Hilvert, D. (2018). Speeding up enzyme discovery and engineering with ultrahigh-throughput methods. Current opinion in structural biology, 48, 149-156.

\bibitem{55} Wrenbeck, E. E., Faber, M. S., and Whitehead, T. A. (2017). Deep sequencing methods for protein engineering and design. Current opinion in structural biology, 45, 36-44.

\bibitem{56} Fowler, D. M., and Fields, S. (2014). Deep mutational scanning: a new style of protein science. Nature methods, 11(8), 801-807.

\bibitem{57} Gupta, K., and Varadarajan, R. (2018). Insights into protein structure, stability and function from saturation mutagenesis. Current opinion in structural biology, 50, 117-125.

\bibitem{28} UniProt Consortium. UniProt: A Worldwide Hub of Protein Knowledge. Nucleic Acids Res. 2018, 47, D506-D515.

\bibitem{60} Evans, R.; Jumper, J.; Kirkpatrick, J.; Sifre, L.; Green, T.; Qin, C.; Zidek, A.; Nelson, A.; Bridgland, A.; Penedones, H.; Petersen, S.; Simonyan, K.; Jones, D. T.; Silver, D.; Kavukcuoglu, K.; Hassabis, D.; Senior, A. W. De Novo Structure Prediction with Deeplearning Based Scoring. In Thirteenth Critical Assessment of Techniques for Protein Structure Prediction Abstracts; 2018; pp 11-12.

\bibitem{61} Kinch, L. N.; Shi, S.; Cheng, H.; Cong, Q.; Pei, J.; Mariani, V.; Schwede, T.; Grishin, N. V. CASP9 Target Classification. Proteins: Struct., Funct., Genet. 2011, 79, 21-36.

\bibitem{62} Shehu, A.; Barbará, D.; Molloy, K. A Survey of ComputationalMethods for Protein Function Prediction. In Big Data Analytics in Genomics; Wong, K. C., Ed.; Springer: Cham, 2016; pp 225-298.

\bibitem{63} Zhang, C.; Freddolino, P. L.; Zhang, Y. COFACTOR: Improved Protein Function Prediction by Combining Structure, Sequence and Protein-Protein Interaction Information. Nucleic Acids Res. 2017, 45, W291-W299.

\bibitem{64} Kumar, N.; Skolnick, J. EFICAz2. 5: Application of a High-Precision Enzyme Function Predictor to 396 Proteomes. Bioinformatics 2012, 28, 2687-2688.

\bibitem{65} Li, Y.; Wang, S.; Umarov, R.; Xie, B.; Fan, M.; Li, L.; Gao, X. DEEPre: Sequence-Based Enzyme EC Number Prediction by Deep Learning. Bioinformatics 2018, 34, 760-769.

\bibitem{66} Yang, M.; Fehl, C.; Lees, K. V.; Lim, E. K.; Offen, W. A.; Davies, G. J.; Bowles, D. J.; Davidson, M. G.; Roberts, S. J.; Davis, B. G. Functional and Informatics Analysis Enables Glycosyltransferase Activity Prediction. Nat. Chem. Biol. 2018, 14, 1109-1117.

\bibitem{67} Niwa, T.; Ying, B. W.; Saito, K.; Jin, W.; Takada, S.; Ueda, T.; Taguchi, H. Bimodal Protein Solubility Distribution Revealed by an Aggregation Analysis of the Entire Ensemble of Escherichia Coli Proteins. Proc. Natl. Acad. Sci. U. S. A. 2009, 106, 4201-4206.

\bibitem{68} Klesmith, J. R.; Bacik, J. P.; Wrenbeck, E. E.; Michalczyk, R.; Whitehead, T. A. Trade-Offs Between Enzyme Fitness and Solubility Illuminated by Deep Mutational Scanning. Proc. Natl. Acad. Sci. U. S. A. 2017, 114, 2265-2270.

\bibitem{69} Ruiz-Blanco, Y. B.; Paz, W.; Green, J.; Marrero-Ponce, Y. ProtDCal: A Program to Compute General-Purpose-Numerical Descriptors for Sequences and 3D-Structures of Proteins. BMC Bioinf. 2015, 16, 162.

\bibitem{35} Han, X.; Wang, X.; Zhou, K. Develop Machine Learning-Based Regression Predictive Models for Engineering Protein Solubility. Bioinformatics 2019, 35, 4640-4646.

\bibitem{5} Musil, M.; Konegger, H.; Hon, J.; Bednar, D.; Damborsky, J. Computational Design of Stable and Soluble Biocatalysts. ACS Catal. 2019, 9, 1033-1054.

\bibitem{70} Li, G.; Dong, Y.; Reetz, M. T. Can Machine Learning Revolutionize Directed Evolution of Selective Enzymes? Adv. Synth. Catal. 2019, 361, 2377-2386. 

\bibitem{71} Wu, Z.; Kan, S. B. J.; Lewis, R. D.; Wittmann, B. J.; Arnold, F. H. Machine Learning-Assisted Directed Protein Evolution with Combinatorial Libraries. Proc. Natl. Acad. Sci. U. S. A. 2019, 116, 8852-8858.

\bibitem{85} Wolpert, D. H.; Macready, W. G. No Free Lunch Theorems for Optimization. IEEE Trans. Evol. Comput. 1997, 1, 67-82.

\bibitem{86} Wolpert, D. H. The Lack of a Priori Distinctions between Learning Algorithms. Neural Comput. 1996, 8, 1341-1390.

\bibitem{87} Walsh, I.; Pollastri, G.; Tosatto, S. C. Correct Machine Learning on Protein Sequences: A Peer-Reviewing Perspective. Briefings Bioinf. 2016, 17, 831-840.

\bibitem{88} Rao, R.; Bhattacharya, N.; Thomas, N.; Duan, Y.; Chen, X.; Canny, J.; Abbeel, P.; Song, Y. S. Evaluating Protein Transfer Learning with TAPE. arXiv preprint arXiv:1906.08230, 2019.


\bibitem{89} Romero, P. A.; Krause, A.; Arnold, F. H. Navigating the Protein Fitness Landscape with Gaussian Processes. Proc. Natl. Acad. Sci. U. S. A. 2013, 110, E193-E201

\bibitem{14} Eraslan, G.; Avsec, Z; Gagneur, J.; Theis, F. J. Deep Learning: New Computational Modelling Techniques for Genomics. Nat. Rev. Genet. 2019, 20, 389-403.

\bibitem{90} Repecka, D.; Jauniskis, V.; Karpus, L.; Rembeza, E.; Zrimec, J.; Poviloniene, S.; Rokaitis, I.; Laurynenas, A.; Abuajwa, W.; Savolainen, O.; Meskys, R.; Engqvist, M. K. M.; Zelezniak, A. Expanding Functional Protein Sequence Space Using Generative Adversarial Networks. bioRxiv 2019, DOI: 10.1101/789719.

\bibitem{91} Riesselman, A. J.; Ingraham, J. B.; Marks, D. S. Deep Generative Models of Genetic Variation Capture the Effects of Mutations. Nat. Methods 2018, 15, 816-822.

\bibitem{92} Thornton, C.; Hutter, F.; Hoos, H. H.; Leyton-Brown, K. Auto- WEKA: Combined Selection and Hyperparameter Optimization of Classiffication Algorithms. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining; 2013; pp 847-855.

\bibitem{93} Polikar, R. Ensemble based systems in decision making. IEEE Circuits and systems magazine 2006, 6, 21-45.

\bibitem{94} Gammerman, A.; Vovk, V. Hedging Predictions in Machine Learning. Comput. J. 2007, 50, 151-163.

\bibitem{95} Samek, W.; Wiegand, T.; Müller, K. Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models. ITU Journal: ICT Discoveries 2017, 39-48. 

\bibitem{96} Shrikumar, A.; Greenside, P.; Kundaje, A. Learning Important Features through Propagating Activation differences. In Proceedings of the 34th International Conference on Machine Learning; 2017; Vol. 70, pp 3145-3153.

\bibitem{97} Simonyan, K.; Vedaldi, A.; Zisserman, A. Deep Inside Convolutional Networks: Visualising Image Classiffication Models and Saliency Maps. arXiv preprint arXiv:1312.6034 2013.

\bibitem{98} Brookes, D. H.; Park, H.; Listgarten, J. Conditioning by Adaptive Sampling for Robust Design. In Proceedings of the 36th International Conference on Machine Learning; 2019; Vol. 97, pp 773-782.

\bibitem{99} Ribeiro, M. T.; Singh, S.; Guestrin, C. “Why Should I Trust You?” Explaining the Predictions of Any Classiffier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining; 2016; pp 1135-1144.

\bibitem{100} Szegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan, D.; Goodfellow, I.; Fergus, R. Intriguing Properties of Neural Networks. arXiv preprint arXiv:1312.6199 201

\bibitem{101} Yu, M. K.; Ma, J.; Fisher, J.; Kreisberg, J. F.; Raphael, B. J.; Ideker, T. Visible Machine Learning for Biomedicine. Cell 2018, 173, 1562-1565.

% Lista de referencias del segundo articulo

\bibitem{2.2} Copley, S. D. Shining a light on enzyme promiscuity. Curr. Opin. Struct. Biol. 47, 167–175 (2017).

\bibitem{2.4} Nobeli, I., Favia, A. D. and Thornton, J. M. Protein promiscuity and its implications for biotechnology. Nat. Biotechnol. 27, 157–167 (2009)

\bibitem{2.5} Adrio, J. L. and Demain, A. L. Microbial enzymes: tools for biotechnological processes. Biomolecules 4, 117–139 (2014).

\bibitem{2.6} Wang, S. et al. Engineering a synthetic pathway for gentisate in pseudomonas chlororaphis p3. Front. Bioeng. Biotechnol. 8, 1588 (2021).

\bibitem{2.7} Wu, M.-C., Law, B., Wilkinson, B. and micklefied, J. Bioengineering natural product biosynthetic pathways for therapeutic applications. Curr. Opin. Biotechnol. 23, 931–940 (2012)


\bibitem{2.9} Rembeza, E., Boverio, A., Fraaije, M. W. and Engqvist, M. K. Discovery of two novel oxidases using a high-throughput activity screen. ChemBioChem 23, e202100510 (2022).

\bibitem{2.10} Longwell, C. K., Labanieh, L. and Cochran, J. R. High-throughput screening technologies for enzyme engineering. Curr. Opin. Biotechnol. 48, 196–202 (2017).

\bibitem{2.11} Black, G. W. et al. A high-throughput screening method for determining the substrate scope of nitrilases. Chem. Commun. 51, 2660–2662 (2015).

\bibitem{2.13} Pertusi, D. A. et al. Predicting novel substrates for enzymes with minimal experimental effort with active learning. Metab. Eng. 44,171-181 (2017).

\bibitem{2.14} Mou, Z. et al. Machine learning-based prediction of enzyme substrate scope: Application to bacterial nitrilases. Proteins Struct. Funct. Bioinf. 89, 336-347 (2021).

\bibitem{2.15} Yang, M. et al. Functional and informatics analysis enables glycosyltransferase activity prediction. Nat. Chem. Biol. 14, 1109–1117 (2018).

\bibitem{2.16} Rottig, M., Rausch, C. and Kohlbacher, O. Combining structure and sequence information allows automated prediction of substrate specificities within enzyme families. PLoS Comput. Biol. 6, e1000636 (2010).

\bibitem{2.17} Chevrette, M. G., Aicheler, F., Kohlbacher, O., Currie, C. R. and Medema, M. H. Sandpuma: ensemble predictions of nonribosomal peptide chemistry reveal biosynthetic diversity across actinobacteria. Bioinformatics 33, 3202-3210 (2017).

\bibitem{2.18} Goldman, S., Das, R., Yang, K. K. and Coley, C. W. Machine learning modeling of family wide enzyme-substrate specificity screens. PLoS Comput. Biol. 18, e1009853 (2022).

\bibitem{2.19} Visani, G. M., Hughes, M. C. and Hassoun, S. Enzyme promiscuity prediction using hierarchy-informed multi-label classiffication Bioinformatics 37, 2017-2024 (2021).

\bibitem{2.20} Ryu, J. Y., Kim, H. U. and Lee, S. Y. Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers. PNAS 116, 13996-14001 (2019).

\bibitem{2.21} Li, Y. et al. DEEPre: sequence-based enzyme EC number prediction by deep learning. Bioinformatics 34, 760-769 (2017).

\bibitem{2.22} Sanderson, T., Bileschi, M. L., Belanger, D. and Colwell, L. J. Proteinfer, deep neural networks for protein functional inference. eLife 12, e80942 (2023).

\bibitem{2.23} Bileschi, M. L. et al. Using deep learning to annotate the protein universe. Nat. Biotechnol.https://doi.org/10.1038/s41587-021-01179-w (2022).

\bibitem{2.24} Rembeza, E. and Engqvist, M. K. Experimental and computational investigation of enzyme functional annotations uncovers misannotation in the ec 1.1. 3.15 enzyme class. PLoS Comput. Biol. 17, e1009446 (2021).

\bibitem{2.25} Ozturk, H., Ozgur, A. and Ozkirimli, E. Deepdta: deep drugtarget binding affinity prediction. Bioinformatics 34, i821-i829 (2018).

\bibitem{2.26} Feng, Q., Dueva, E., Cherkasov, A. and Ester, M. Padme: A deep learning-based framework for drug-target interaction prediction. Preprint at https://doi.org/10.48550/arXiv.1807.09741 (2018).

\bibitem{2.27} Karimi, M., Wu, D., Wang, Z. and Shen, Y. Deep affinity: interpretable deep learning of compound–protein affinity through UNIFIED recurrent and convolutional neural networks. Bioinformatics 35, 3329-3338 (2019).

\bibitem{2.28} Kroll, A., Engqvist, M. K., Heckmann, D. and Lercher, M. J. Deep learning allows genome-scale prediction of michaelis constants from structural features. PLoS Biol. 19, e3001402 (2021).

\bibitem{2.29} Li, F. et al. Deep learning-based k cat prediction enables improved enzyme-constrained model reconstruction. Nat. Catal. 5, 662-672 (2022).

\bibitem{2.30} Weininger, D. SMILES, a chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci. 28, 31-36 (1988).

\bibitem{2.31} Rogers, D. and Hahn, M. Extended-connectivity fingerprints. J. Chem. Inf. Model. 50, 742-754 (2010).

\bibitem{2.32} Zhou, J. et al. Graph neural networks: A review of methods and applications. AI Open 1, 57-81 (2020).

\bibitem{2.33} Yang, K. et al. Analyzing learned molecular representations for property prediction. J. Chem. Inf. Model. 59, 3370-3388 (2019).


\bibitem{2.34} Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. PNAS 118, e2016239118 (2021).

\bibitem{2.35} Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. and Church, G. M. Unified rational protein engineering with sequence-based deep representation learning. Nat. Methods. 16, 1315-1322 (2019).

\bibitem{2.36} Xu, Y. et al. Deep dive into machine learning models for protein engineering. J. Chem. Inf. Model. 60, 2773–2790 (2020).

\bibitem{2.42} Bekker, J. and Davis, J. Learning from positive and unlabeled data: A survey. Mach. Learn. 109, 719-760 (2020)


\bibitem{2.38} Kearnes, S., McCloskey, K., Berndl, M., Pande, V. and  Riley, P. Mole- cular graph convolutions: moving beyond !ngerprints. J. Comput. -Aided Mol. Des. 30, 595–608 (2016).

\bibitem{2.39} Duvenaud, D. K. et al. Convolutional networks on graphs for learning molecular fingerprints. In Advances in Neural Information Processing Systems, 2224-2232 (2015).

\bibitem{2.40} Zhou, J. et al. Graph neural networks: A review of methods and applications. AI Open 1, 57–81 (2020).

\bibitem{2.45} Hu, W. et al. Strategies for pre-training graph neural networks. Preprint at https://doi.org/10.48550/arXiv.1905.12265 (2019). 



\bibitem{2.46} Capela, F., Nouchi, V., Van Deursen, R., Tetko, I. V. and  Godin, G. Multitask learning on graph neural networks applied to molecular property predictions. Preprint at https://doi.org/10.48550/arXiv. 1910.13124 (2019).

\bibitem{2.47}  Vaswani, A. et al. Attention is all you need. In Advances in neural information processing systems, 5998–6008 (2017).

\bibitem{2,48} Suzek, B. E. et al. Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinfor- matics 31, 926–932 (2015).
\bibitem{2.49} Elnaggar, A. et al. Prottrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing. IEEE Trans. Pattern Anal. Mach. Intell. PP https://doi. org/10.1109/TPAMI.2021.3095381 (2021).


\bibitem{Wittman} Wittmann, B. J., Johnston, K. E., Wu, Z., and  Arnold, F. H. (2021). Advances in machine learning for directed evolution. Current opinion in structural biology, 69, 11-18.

\end{thebibliography}



\end{document}
