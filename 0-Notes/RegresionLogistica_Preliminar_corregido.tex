%===========================================
\documentclass[12pt]{article}
%===========================================
\usepackage[utf8]{inputenc}
%\usepackage[margin=2.5in]{geometry}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{graphicx,graphics}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{color}
\usepackage{float} 
\usepackage{subfig}
\usepackage[figuresright]{rotating}
\usepackage{enumerate}
\usepackage{anysize} 
\usepackage{url}
\usepackage{imakeidx}
\usepackage[left=0.5in, right=0.5in, top=1in, bottom=1in]{geometry}

%===========================================
\title{Notas sobre Regresión Logística \\
\textit{Breve introducción con aplicaciones}}
\author{Carlos E Martinez-Rodriguez}
\date{}
%===========================================
\newtheorem{Criterio}{Criterio}%[section]
\newtheorem{Sup}{Supuesto}%[section]
\newtheorem{Note}{Nota}%[section]
\newtheorem{Ejem}{Ejemplo}%[section]
\newtheorem{Prop}{Proposici\'on}%[section]
\newtheorem{Def}{Definici\'on}
\newtheorem{Teo}{Teorema}
\newtheorem{Algthm}{Algoritmo}
\newtheorem{Sol}{Soluci\'on}

%===========================================
\begin{document}
%===========================================
\maketitle
\tableofcontents
%__________________________
%\chapter{Principios}
%__________________________

%---------------------------------------------------------
\section{An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------
En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}.  Supongamos que se tiene una \'unica variable dependiente $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$. La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que aproxime a $\phi\left(\cdot\right)$.
\medskip

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria. El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x,
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y  $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}

donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}. Supongamos adem\'as que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$,  estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on se realiza utilizando el \textbf{m\'etodos de m\'inimos cuadrados}. Entonces la ecuaci\'on (\ref{Modelo.Regresion}) se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on 
\begin{eqnarray*}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}.
\end{eqnarray*}

Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$,  $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$,  igualar a cero y  resolver el sistema de ecuaciones lineales resultante. Para resolver el problema de regresi\'on lineal univariado se requiere encontrar $\beta_0$ y $\beta_1$ minimizando la funci\'on de versosimilitud:

\begin{eqnarray*}
L(\beta_0^*, \beta_1) = \sum_{i=1}^{n} [y_i - (\beta_0^* + \beta_1(x_i - \bar{x}))]^2;
\end{eqnarray*}


Que se obtiene resolviendo
\begin{eqnarray*}
\frac{\partial}{\partial \beta_0} \sum \left[ y_i - (\beta_0 + \beta_1 x_i) \right]^2 = 0,\\
\frac{\partial}{\partial \beta_1} \sum \left[ y_i - (\beta_0 + \beta_1 x_i) \right]^2 = 0.
\end{eqnarray*}

Si se define

\begin{eqnarray*}
y_i = \beta_0^* + \beta_1 (x_i - \bar{x}) + \varepsilon_i,\textrm{ donde }\beta_0 = \beta_0^* + \beta_1 \bar{x}.
\end{eqnarray*}


Entonces se tiene 

\begin{eqnarray*}
\frac{\partial}{\partial \beta_0^*} \sum \left[ y_i - (\beta_0^* + \beta_1 \bar{x}) \right]^2 &=& 0,\\
\frac{\partial}{\partial \beta_1^*} \sum \left[ y_i - (\beta_0^* + \beta_1 \bar{x}) \right]^2&=& 0.
\end{eqnarray*}

Sabemos que:

\begin{eqnarray*}
\beta_0 = \beta_0^* + \beta_1 \bar{x}, \textrm{ entonces }\quad \beta_0^* = \beta_0 - \beta_1 \bar{x},\textrm{ por lo tanto}
\end{eqnarray*}

\begin{eqnarray*}
\frac{\partial}{\partial \beta_0} \sum \left[ y_i - \left( \beta_0 + \beta_1 \bar{x} \right) \right]^2 &=&\sum \frac{\partial}{\partial \beta_0} \left[ y_i - (\beta_0 + \beta_1 \bar{x}) \right]^2
= \sum \frac{\partial}{\partial \beta_0} \left[ y_i - \left( \beta_0^* - \beta_1 \bar{x} + \beta_1 x_i \right) \right]^2\\
 &=& \sum \frac{\partial}{\partial \beta_0} \left[ y_i - \left( \beta_0^* - (x_i - \bar{x}) \beta_1 \right) \right]^2 = - \sum 2 \left[ y_i - \left( \beta_0^* - (x_i - \bar{x}) \beta_1 \right) \right](-1) \\
 &=&0,
\end{eqnarray*}

entonces
\begin{eqnarray*}
2 \sum \left[ y_i - \left( \beta_0^* + (x_i - \bar{x}) \beta_1 \right) \right] (-1) = -2 \sum \left[ y_i - \beta_0^* - \beta_1 (x_i - \bar{x}) \right] \\
=\sum y_i - n \beta_0^* - \beta_1 \sum (x_i - \bar{x}) =\sum y_i -n \beta_0^* =0,
\end{eqnarray*}

entonces, $ \sum y_i =n \beta_0^*$,  ya que $\sum (x_i - \bar{x}) = 0$, por lo tanto
\begin{eqnarray}
\beta_0^* = \bar{y}.
\end{eqnarray}

Por otra parte, la derivada respecto a $\beta_1$:

\begin{eqnarray*}
\frac{\partial}{\partial \beta_1} \sum \left[ y_i - \left( \beta_0^* + \beta_1 x_i \right) \right]^2 &=&\frac{\partial}{\partial \beta_1} \sum \left[ y_i - \left( \beta_0^* - \beta_1 \bar{x} + \beta_1 x_i \right) \right]^2 =\frac{\partial}{\partial \beta_1} \sum \left[ y_i - \left( \beta_0^* + \beta_1 (x_i - \bar{x}) \right) \right]^2\\
&=& 2 \sum \left[ y_i - \left( \beta_0^* + \beta_1 (x_i - \bar{x}) \right) \right](x_i - \bar{x}) (-1) = 0,
\end{eqnarray*}

de aqu\'i que 

\begin{eqnarray*}
\sum \left[ y_i - \left( \beta_0^* + \beta_1 (x_i - \bar{x}) \right) \right](x_i - \bar{x})=\sum y_i (x_i - \bar{x}) - \beta_0^* \sum (x_i - \bar{x}) - \beta_1 \sum (x_i - \bar{x})^2 = 0.
\end{eqnarray*}

Recordar que $\beta_0^* = \bar{y}$, entonces:

\begin{eqnarray*}
\sum y_i (x_i - \bar{x}) - \bar{y} \sum (x_i - \bar{x}) - \beta_1 \sum (x_i - \bar{x})^2 = \sum (y_i - \bar{y})(x_i - \bar{x}) - \beta_1 \sum (x_i - \bar{x})^2 = 0
\end{eqnarray*}

entonces 
\begin{eqnarray*}
 \beta_1 = \frac{\sum (y_i - \bar{y})(x_i - \bar{x})}{\sum (x_i - \bar{x})^2} = \frac{S_{xy}}{S_{xx}},
\end{eqnarray*}

por lo tanto
\begin{eqnarray}
\boxed{ \beta_1 = \frac{S_{xy}}{S_{xx}}, } \quad \textrm{ y }\boxed{ \beta_0 = \bar{y} - \beta_1 \bar{x}.}
\end{eqnarray}


%---------------------------------------------------------
\subsection{Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}
%---------------------------------------------------------
Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo. Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias. A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)+E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]\\
&=&\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1},
\end{eqnarray*}
por lo tanto 

\begin{equation}\label{Esperanza.Beta.1}
E\left(\hat{\beta}_{1}\right)=\beta_{1}.
\end{equation}

Es decir, $\hat{\beta_{1}}$ es un estimador insesgado. Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}\\
&=&\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}},
\end{eqnarray*}

por lo tanto
\begin{equation}\label{Varianza.Beta.1}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}.
\end{equation}
Se tiene la siguiente proposici\'on

\begin{Prop}
\begin{eqnarray}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray}
\end{Prop}

Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k}\textrm{ (\textbf{residuo}).}
\end{eqnarray*}

La suma de los cuadrados de los errores de los residuos, \textit{suma de cuadrados del error},

\begin{eqnarray*}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{eqnarray*}

sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},
\end{eqnarray*}
por lo tanto
\begin{eqnarray*}
\hat{\sigma}^{2}=\frac{SC_{E}}{n-2}=MC_{E}
\end{eqnarray*}
que es un estimador insesgado de $\sigma^{2}$.

%---------------------------------------------------------
\subsection{Prueba de Hip\'otesis en RLS}
%---------------------------------------------------------

Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza.
\medskip 

Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$. Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:

\begin{centering}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,

\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.

\end{itemize}
donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. 
\end{centering}

\medskip 

De las ecuaciones (\ref{Ecs.Estimadores.Regresion}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1}) y (\ref{Varianza.Beta.1}).
Entonces se tiene que el estad\'istico de prueba apropiado es

\begin{eqnarray*}%\label{Estadistico.Beta.1}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}},
\end{eqnarray*}

que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 

\begin{eqnarray*}%\label{Zona.Rechazo.Beta.1}
t_{0}>t_{\alpha/2,n-2}.
\end{eqnarray*}


Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto

\begin{eqnarray*}%\label{Estadistico.Beta.0}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{eqnarray*}

con el que rechazamos la hip\'otesis nula si

\begin{eqnarray*}%\label{Zona.Rechazo.Beta.0}
t_{0}>t_{\alpha/2,n-2}.
\end{eqnarray*}

No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$. Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado. El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:

\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\\
&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\\
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
\end{eqnarray*}

\begin{eqnarray*}
&=&\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&0+0+0=0.
\end{eqnarray*}


Por lo tanto, efectivamente se tiene

\begin{equation}\label{Suma.Total.Cuadrados}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}

entonces se tiene que 

\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}.\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}.
\end{eqnarray*}

Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados}) se puede reescribir como 

\begin{eqnarray*}%\label{Suma.Total.Cuadrados.Dos}
S_{yy}=SC_{R}+SC_{E},
\end{eqnarray*}

recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$, entonces

\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right),\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R},
\end{eqnarray*}

luego, $S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.

\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx},
\end{equation}

adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.

\end{Prop}

Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,

\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}},
\end{eqnarray*}

se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$. El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza \ref{Tabla.Analisis.Varianza}:

\begin{table}[htbp]
\centering
\caption{Tabla de análisis de varianza}
\label{Tabla.Analisis.Varianza}
\begin{tabular}{lcccc}
\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
variación & Cuadrados & Libertad & Cuadrática & \\
\hline
Regresión & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\
\hline
Total & $S_{yy}$ & $n-1$ & & \\
\hline
\end{tabular}
\end{table}


\medskip

La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1}), con $\hat{\beta}_{1,0}=0$, es decir
\begin{eqnarray*}%\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}.
\end{eqnarray*}

Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}.
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.

%---------------------------------------------------------
\subsection{Estimaci\'on de Intervalos en RLS}
%---------------------------------------------------------

Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros. El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on. Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por
\begin{eqnarray*}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray*}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\small{
\begin{eqnarray*}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray*}}

%---------------------------------------------------------
\subsection{Predicci\'on}
%---------------------------------------------------------

Supongamos que se tiene un valor $x_{0}$ de inter\'es, entonces la estimaci\'on puntual de este nuevo valor
\begin{eqnarray*}
\hat{y}_{0}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}.
\end{eqnarray*}

Esta nueva observaci\'on es independiente de las utilizadas para obtener el modelo de regresi\'on, por tanto, el intervalo en torno a la recta de regresi\'on es inapropiado, puesto que se basa \'unicamente en los datos empleados para ajustar el modelo de regresi\'on. El intervalo de confianza en torno a la recta de regresi\'on se refiere a la respuesta media verdadera $x=x_{0}$, no a observaciones futuras. Sea $y_{0}$ la observaci\'on futura en $x=x_{0}$, y sea $\hat{y}_{0}$ dada en la ecuaci\'on anterior, el estimador de $y_{0}$. Si se define la variable aleatoria $$w=y_{0}-\hat{y}_{0},$$ esta se distribuye normalmente con media cero y varianza $$V\left(w\right)=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right],$$ dado que $y_{0}$ es independiente de $\hat{y}_{0}$, por lo tanto el intervalo de predicci\'on al nivel $\alpha$ para futuras observaciones $x_{0}$ es

\begin{eqnarray*}
\hat{y}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}\leq y_{0}\\
\leq \hat{y}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}.
\end{eqnarray*}

Es com\'un encontrar que el modelo ajustado no satisface totalmente el modelo necesario para los datos, en este caso es preciso saber qu\'e tan bueno es el modelo propuesto. Para esto se propone la siguiente prueba de hip\'otesis:

\begin{itemize}
\item[$H_{0}:$ ]El modelo propuesto se ajusta adecuademente a los datos.
\item[$H_{1}:$ ]El modelo NO se ajusta a los datos.
\end{itemize}
La prueba implica dividir la suma de cuadrados del eror o del residuo en las siguientes dos componentes:
\begin{eqnarray*}
SC_{E}=SC_{EP}+SC_{FDA},
\end{eqnarray*}

donde $SC_{EP}$ es la suma de cuadrados atribuibles al error puro, y $SC_{FDA}$ es la suma de cuadrados atribuible a la falta de ajuste del modelo. La cantidad

\begin{equation}
R^{2}=\frac{SC_{R}}{S_{yy}}=1-\frac{SC_{E}}{S_{yy}},
\end{equation}

se denomina coeficiente de determinaci\'on y se utiliza para saber si el modelo de regresi\'on es suficiente o no. Se puede demostrar que $0\leq R^{2}\leq1$, una manera de interpretar este valor es que si $R^{2}=k$, entonces el modelo de regresi\'on explica el $k*100\%$ de la variabilidad en los datos. En lo que corresponde a  $R^{2}$ 

\begin{itemize}
\item No mide la magnitud de la pendiente de la recta de regresi\'on.
\item Un valor grande de $R^{2}$ no implica una pendiente empinada.
\item No mide la suficiencia del modelo.
\item Valores grandes de $R^{2}$ no implican necesariamente que el modelo de regresi\'on proporcionar\'a predicciones precisas para futuras observaciones.
\end{itemize}

%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
%\chapter{Regresi\'on Log\'istica}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->


%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Regresi\'on m\'ultiple}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

La regresión logística es una técnica de modelado estadístico utilizada para predecir la probabilidad de un evento binario (es decir, un evento que tiene dos posibles resultados) en función de una o más variables independientes.  Un modelo de regresión logística describe cómo una variable dependiente binaria $Y$ (que puede tomar los valores $0$ o $1$) está relacionada con una o más variables independientes $X_1, X_2, \ldots, X_n$. A diferencia de la regresión lineal, que predice un valor continuo, la regresión logística predice una probabilidad que puede ser interpretada como la probabilidad de que $Y=1$ dado un conjunto de valores para $X_1, X_2, \ldots, X_n$, la regresión logística está diseñada para manejar situaciones donde la respuesta es categórica. 
\medskip

En su forma más común, la regresión logística binaria, el modelo predice la probabilidad de que un evento ocurra en función de una o más variables independientes. Este tipo de regresión toma la forma de un modelo no lineal, debido a la naturaleza discreta de la variable dependiente. La regresión lineal busca modelar la relación entre una variable dependiente continua $Y$ y una o más variables independientes $X_1, X_2, \ldots, X_n$ mediante una ecuación de la forma:

\begin{eqnarray*}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon,
\end{eqnarray*}
donde:
\begin{itemize}
    \item[a) ] $Y$ es la variable dependiente.
    \item[b) ] $\beta_0$ es la intersección con el eje $Y$ o término constante.
    \item[c) ] $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes que representan la relación entre las variables independientes y la variable dependiente.
    \item[d) ] $X_1, X_2, \ldots, X_n$ son las variables independientes.
    \item[e) ] $\epsilon$ es el término de error, que representa la desviación de los datos observados de los valores predichos por el modelo.
\end{itemize}

%La ecuación de la regresión logística es:
%\begin{eqnarray}
%\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n,
%\end{eqnarray}
%donde $p$ es la probabilidad de que $Y=1$. La función logística es:
%\begin{eqnarray}
%p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}.
%\end{eqnarray}

El objetivo de la regresión lineal es encontrar los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ que minimicen la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos. Este método se conoce como mínimos cuadrados ordinarios (OLS, por sus siglas en inglés). La función de costo a minimizar es:

\begin{equation}
J\left(\beta_0, \beta_1, \ldots, \beta_n\right) = \sum_{i=1}^{n}\left(y_i - \hat{y}_i\right)^2,
\end{equation}
donde:
\begin{itemize}
    \item[a) ] $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación.
    \item[b) ] $\hat{y}_i$ es el valor predicho por el modelo para la $i$-ésima observación, dado por:
    \begin{eqnarray*}
    \hat{y}_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_n x_{in}.
    \end{eqnarray*}
\end{itemize}

Para encontrar los valores óptimos de los coeficientes, se toman las derivadas parciales de la función de costo con respecto a cada coeficiente y se igualan a cero:
\begin{eqnarray*}
\frac{\partial J}{\partial \beta_j} = 0 \quad \text{para } j = 0, 1, \ldots, n.
\end{eqnarray*}

Resolviendo este sistema de ecuaciones, se obtienen los valores de los coeficientes que minimizan la función de costo. 
\medskip

La deducción de la fórmula de la regresión logística comienza con la necesidad de modelar la probabilidad de un evento binario. Queremos encontrar una función que relacione las variables independientes con la probabilidad de que la variable dependiente tome el valor $1$. La probabilidad de que el evento ocurra, $P(Y=1)$, se denota como $p$. La probabilidad de que el evento no ocurra, $P(Y=0)$, es $1-p$. Los \textbf{odds} (chances/momios) de que ocurra el evento se definen como:
\begin{eqnarray*}
\text{odds} = \frac{p}{1-p}.
\end{eqnarray*}

Los momios indican cuántas veces más probable es que ocurra el evento frente a que no ocurra. Para simplificar el modelado de los \textit{momios}, se aplica el logaritmo natural, obteniendo la función \textbf{logit}:
\begin{eqnarray*}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right).
\end{eqnarray*}

La transformación logit es útil porque convierte el rango de la probabilidad (0, 1) al rango de números reales $\left(-\infty, \infty\right)$. La idea clave de la regresión logística es modelar la transformación logit de la probabilidad como una combinación lineal de las variables independientes:

\begin{eqnarray*}%\label{Ec.logit}
\text{logit}(p) := \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n.
\end{eqnarray*}

Aquí, $\beta_0$ es el t\'ermino constante y $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes asociados con las variables independientes $X_1, X_2, \ldots, X_n$. Para expresar $p$ en función de una combinación lineal de las variables independientes, invertimos la transformación logit. Partimos de la ecuación \ref{Ec.logit}, aplicando la funci\'on exponencial en ambos lados:

\begin{eqnarray*}
\frac{p}{1-p} = e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}.
\end{eqnarray*}

Despejando $p$:

\begin{eqnarray*}
p = \frac{e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}{1 + e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}.
\end{eqnarray*}

La expresión final que obtenemos es conocida como la \textbf{función logística}:

\begin{eqnarray*}%\label{Eq.Logit1}
p = \frac{1}{1 + e^{-\left(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n\right)}}.
\end{eqnarray*}

Esta función describe cómo las variables independientes se relacionan con la probabilidad de que el evento de interés ocurra. Los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ se estiman a partir de los datos utilizando el método de máxima verosimilitud.

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection{Método de Máxima Verosimilitud}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

Para estimar los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ en la regresión logística, utilizamos el método de máxima verosimilitud. La idea es encontrar los valores de los coeficientes que maximicen la probabilidad de observar los datos dados. Esta probabilidad se expresa mediante la función de verosimilitud $L$. La función de verosimilitud $L(\beta_0, \beta_1, \ldots, \beta_n)$ para un conjunto de $n$ observaciones se define como el producto de las probabilidades de las observaciones dadas las variables independientes:

\begin{eqnarray*}%\label{Eq.Verosimilitud}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i},
\end{eqnarray*}

donde:
\begin{itemize}
    \item $p_i$ es la probabilidad predicha de que $Y_i = 1$,
    \item $y_i$ es el valor observado de la variable dependiente para la $i$-ésima observación.
\end{itemize}

Trabajar directamente con esta función de verosimilitud puede ser complicado debido al producto de muchas probabilidades, especialmente si $n$ es grande. Para simplificar los cálculos, se utiliza el logaritmo de la función de verosimilitud, conocido como la función de \textit{log-verosimilitud}. El uso del logaritmo simplifica significativamente la diferenciación y maximización de la función. La función de log-verosimilitud se define como:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right].
\end{eqnarray*}

Aquí, $\log$ representa el logaritmo natural. Esta transformación es válida porque el logaritmo es una función monótona creciente, lo que significa que maximizar la log-verosimilitud es equivalente a maximizar la verosimilitud original. En la regresión logística, la probabilidad $p_i$ está dada por la función logística:

\begin{eqnarray*}
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}}.
\end{eqnarray*}

Sustituyendo esta expresión en la función de log-verosimilitud, obtenemos:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &=& \sum_{i=1}^{n} \left[ y_i \log \left( \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}} \right) + \right. \nonumber \\
& \quad& \left. (1 - y_i) \log \left( 1 - \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}} \right) \right].
\end{eqnarray*}

Simplificando esta expresión, notamos que:

\begin{eqnarray*}
\log \left( \frac{1}{1 + e^{-z}} \right) = -\log(1 + e^{-z}),
\end{eqnarray*}

y

\begin{eqnarray*}
\log \left( 1 - \frac{1}{1 + e^{-z}} \right) = \log \left( \frac{e^{-z}}{1 + e^{-z}} \right) = -z - \log(1 + e^{-z}).
\end{eqnarray*}

Aplicando estas identidades, la función de log-verosimilitud se convierte en:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &=& \sum_{i=1}^{n} \left[ y_i (-\log(1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}))\right.\\
 &+&\left. (1 - y_i) \left( -(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})\right)\right.\\
 &-& \left.(1 - y_i)\log(1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}) \right].
\end{eqnarray*}

Simplificando aún más, obtenemos:

\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) &=& \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})-\log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right].
\end{eqnarray*}

Para simplificar aún más la notación, podemos utilizar notación matricial. Definimos la matriz $\mathbf{X}$ de tamaño $n \times (k+1)$ y el vector de coeficientes $\boldsymbol{\beta}$ de tamaño $(k+1) \times 1$ como sigue:

\begin{eqnarray*}%\label{Eq.Matricial1}
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1k} \\
1 & x_{21} & x_{22} & \ldots & x_{2k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nk}
\end{bmatrix}, \quad
\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix}.
\end{eqnarray*}

Entonces, la expresión para la función de log-verosimilitud es:

\begin{eqnarray*}%\label{Eq.LogLikelihood1}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i (\mathbf{X}_i \boldsymbol{\beta}) - \log(1 + e^{\mathbf{X}_i \boldsymbol{\beta}}) \right],
\end{eqnarray*}

donde $\mathbf{X}_i$ es la $i$-ésima fila de la matriz $\mathbf{X}$.  Esta notación matricial simplifica la implementación y la derivación de los estimadores de los coeficientes en la regresión logística. Utilizando métodos numéricos, como el algoritmo de \textit{Newton-Raphson}, se pueden encontrar los coeficientes que maximizan la función de log-verosimilitud. Para maximizar la función de log-verosimilitud, derivamos esta función con respecto a cada uno de los coeficientes $\beta_j$ y encontramos los puntos críticos. La derivada parcial de la función de log-verosimilitud con respecto a $\beta_j$ es:

\begin{eqnarray}\label{Eq.1.14}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} \left[ y_i X_{ij} - \frac{X_{ij} e^{\mathbf{X}_i \boldsymbol{\beta}}}{1 + e^{\mathbf{X}_i \boldsymbol{\beta}}} \right].
\end{eqnarray}

Simplificando, esta derivada se puede expresar como:

\begin{eqnarray*}%\label{Eq.PrimeraDerivada}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} X_{ij} (y_i - p_i),\textrm{ donde }p_i = \frac{1}{1 + e^{-\mathbf{X}_i \boldsymbol{\beta}}}.
\end{eqnarray*}

Para encontrar los coeficientes que maximizan la log-verosimilitud, resolvemos el sistema de ecuaciones 
\begin{eqnarray*}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = 0 \textrm{ para todos los }j = 0, 1, \ldots, k. 
\end{eqnarray*}
Este sistema de ecuaciones no tiene una solución analítica cerrada, por lo que se resuelve numéricamente utilizando métodos iterativos como el algoritmo de Newton-Raphson.

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\subsection{Método de Newton-Raphson}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

El método de Newton-Raphson es un algoritmo iterativo que se utiliza para encontrar las raíces de una función. En el contexto de la regresión logística, se utiliza para maximizar la función de log-verosimilitud encontrando los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$. Este m\'etodo se basa en una aproximación de segundo orden de la función objetivo. Dado un valor inicial de los coeficientes $\boldsymbol{\beta}^{(0)}$, se actualiza iterativamente el valor de los coeficientes utilizando la fórmula:

\begin{equation*}%\label{Eq.Criterio0}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \mathbf{H}(\boldsymbol{\beta}^{(t)}) \right]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)}),
\end{equation*}

donde:
\begin{itemize}
\item $\boldsymbol{\beta}^{(t)}$ es el vector de coeficientes en la $t$-ésima iteración.
\item $\nabla \log L(\boldsymbol{\beta}^{(t)})$ es el gradiente de la función de log-verosimilitud con respecto a los coeficientes $\boldsymbol{\beta}$:

\begin{eqnarray}%\label{Eq.Gradiente1}
\nabla \log L(\boldsymbol{\beta}) = \mathbf{X}^T (\mathbf{y} - \mathbf{p}),
\end{eqnarray}

donde $\mathbf{y}$ es el vector de valores observados y $\mathbf{p}$ es el vector de probabilidades.

\item $\mathbf{H}(\boldsymbol{\beta}^{(t)})$ es la matriz Hessiana (matriz de segundas derivadas) evaluada en $\boldsymbol{\beta}^{(t)}$:
\begin{eqnarray}%\label{Eq.Hessiana1}
\mathbf{H}(\boldsymbol{\beta}) = -\mathbf{X}^T \mathbf{W} \mathbf{X},
\end{eqnarray}

donde $\mathbf{W}$ es una matriz diagonal de pesos con elementos $w_i = p_i (1 - p_i)$.
\end{itemize}

\begin{Algthm}\label{Algoritmo1}
El algoritmo Newton-Raphson para la regresión logística se puede resumir en los siguientes pasos:
\begin{enumerate}
\item Inicializar el vector de coeficientes $\boldsymbol{\beta}^{(0)}$ (por ejemplo, con ceros o valores pequeños aleatorios).
\item Calcular el gradiente $\nabla \log L(\boldsymbol{\beta}^{(t)})$ y la matriz Hessiana $\mathbf{H}(\boldsymbol{\beta}^{(t)})$ en la iteración $t$.
\item Actualizar los coeficientes utilizando la fórmula:

\begin{eqnarray}%\label{Eq.Criterio1}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \mathbf{H}(\boldsymbol{\beta}^{(t)}) \right]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
\end{eqnarray}
\item Repetir los pasos 2 y 3 hasta que la diferencia entre $\boldsymbol{\beta}^{(t+1)}$ y $\boldsymbol{\beta}^{(t)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}
\end{Algthm}

El método de Newton-Raphson permite encontrar los coeficientes que maximizan la función de log-verosimilitud de manera eficiente, y se basa en una aproximación de segundo orden de la función objetivo. Dado un valor inicial de los coeficientes $\beta^{(0)}$, se iterativamente actualiza el valor de los coeficientes utilizando la fórmula:

\begin{equation}\label{Eq.Criterio1.5}
\beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)}),
\end{equation}

donde:

\begin{itemize}
\item $\beta^{(k)}$ es el vector de coeficientes en la $k$-ésima iteración.
\item $\mathbf{g}(\beta^{(k)})$ es el gradiente (vector de primeras derivadas) evaluado en $\beta^{(k)}$:

\begin{equation}\label{Eq.Gradiente2}
\mathbf{g}(\beta) = \frac{\partial \log L}{\partial \beta} = \sum_{i=1}^{n} \mathbf{X}_i (y_i - p_i),
\end{equation}

donde $\mathbf{X}_i$ es el vector de valores de las variables independientes para la $i$-ésima observación (comparar con ecuaci\'on \ref{Eq.Gradiente1}).

\item $\mathbf{H}(\beta^{(k)})$ es la matriz Hessiana (matriz de segundas derivadas) evaluada en $\beta^{(k)}$:

\begin{equation}\label{Eq.Hessiana2}
\mathbf{H}(\beta) = \frac{\partial^2 \log L}{\partial \beta \partial \beta^T} = -\sum_{i=1}^{n} p_i (1 - p_i) \mathbf{X}_i \mathbf{X}_i^T,
\end{equation}

comparar con ecuaci\'on \ref{Eq.Hessiana1}.
\end{itemize}

\begin{Algthm} \label{Algoritmo2}
Los pasos del algoritmo Newton-Raphson para la regresión logística son:
\begin{enumerate}
\item Inicializar el vector de coeficientes $\beta^{(0)}$ (por ejemplo, con ceros o valores pequeños aleatorios).
\item Calcular el gradiente $\mathbf{g}(\beta^{(k)})$ y la matriz Hessiana $\mathbf{H}(\beta^{(k)})$ en la iteración $k$.
\item Actualizar los coeficientes utilizando la fórmula:
\begin{equation}\label{Eq.Criterio2}
\beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)}).
\end{equation}
\item Repetir los pasos 2 y 3 hasta que la diferencia entre $\beta^{(k+1)}$ y $\beta^{(k)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}
\end{Algthm}

Como se puede observar, la diferencia entre el Algoritmo \ref{Algoritmo1} y el Algoritmo \ref{Algoritmo2} son m\'inimas.


%>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<%
%\chapter{Revisando la Regresión Logística}
%>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<%

%____________________________________________________________
%\section{Desarrollo}
%____________________________________________________________

\begin{Ejem}\textbf{Caso Bernoulli}

Sea $X \in \mathbb{R}^{n \times d}$, donde $n$ es el  número de instancias; $d$ el  número de características; $y$ un vector binario de resultados. Para todo $x_i \in \mathbb{R}^d$, la salida es $y_i \in \{0,1\}$. El objetivo es clasificar la instancia $x_i$ como positiva o negativa. Una instancia se puede pensar como un intento Bernoulli con esperanza $\mathbb{E}[y_i | x_i]$ o probabilidad $\rho_i$. Se propone el modelo
\begin{eqnarray*}
y = X \beta + \varepsilon, \quad \text{donde }\varepsilon \text{es el vector error}.
\end{eqnarray*}

\begin{eqnarray*}
y = 
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix},
\quad
X = 
\begin{bmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1d} \\
1 & x_{21} & x_{22} & \cdots & x_{2d} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{nd}
\end{bmatrix},
\quad
\varepsilon = 
\begin{pmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{pmatrix},
\end{eqnarray*}

y

\begin{eqnarray*}
\beta = 
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_d
\end{pmatrix}
\quad \text{es el vector de parámetros.}
\end{eqnarray*}

Supongamos que $X_i = [1, x_i^\top]$ y $\beta = [\beta_0, \boldsymbol{\beta}^\top]$.  Como $y$ es una variable aleatoria Bernoulli con probabilidad $\rho_i$, se tiene:

\begin{eqnarray*}
P(y_i) = 
\begin{cases}
\rho_i, & \text{si } y_i = 1, \\
1 - \rho_i, & \text{si } y_i = 0.
\end{cases}
\end{eqnarray*}
Entonces
\begin{eqnarray*}
\mathbb{E}[y_i] &=& 1 \cdot \rho_i + 0 \cdot (1 - \rho_i) = \rho_i = X_i^\top \beta,\\
\mathbb{V}[y_i] &=& \rho_i (1 - \rho_i).
\end{eqnarray*}

Por lo tanto se tiene
\begin{eqnarray*}
y_i = X_i^\top \beta + \varepsilon_i, \quad \text{donde} \quad
\varepsilon_i =
\begin{cases}
1 - \rho_i, & \text{si } y_i = 1, \\
 \rho_i, & \text{si } y_i = 0.
\end{cases}
\end{eqnarray*}


donde $\varepsilon_i \sim \text{Binomial}$, con esperanza:

\begin{eqnarray*}
\mathbb{E}[\varepsilon_i] = (1 - \rho_i)(\rho_i + (1 - \rho_i))(1 - \rho_i) = 0;
\end{eqnarray*}

y varianza:

\begin{eqnarray*}
\mathbb{V}[\varepsilon_i] = \mathbb{E}[\varepsilon_i^2] - (\mathbb{E}[\varepsilon_i])^2
= (1 - \rho_i)^2 \rho_i + (-\rho_i)^2 (1 - \rho_i) \neq 0
= \rho_i (1 - \rho_i).
\end{eqnarray*}

Ahora, se sabe que 
\begin{eqnarray*}
\mathbb{E}[Y_i = 1 \mid x_i, \beta] = \rho_i = \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}} = \frac{1}{1 + e^{-x_i^\top \beta}};
\end{eqnarray*}

si se define
\begin{eqnarray*}
\eta_i = g(\rho_i) &=& \ln \left( \frac{\rho_i}{1 - \rho_i} \right) = x_i^\top \beta \textrm{ entonces }\eta = X. 
\end{eqnarray*}

Ahora definamos la \textbf{Función de verosimilitud:}

\begin{eqnarray*}
\mathcal{L}(\beta) =\prod_{i=1}^{n} \rho_i^{y_i} (1 - \rho_i)^{1 - y_i}
= \prod_{i=1}^{n} \left( \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}} \right)^{y_i}
\left( \frac{1}{1 + e^{x_i^\top \beta}} \right)^{1 - y_i},
\end{eqnarray*}
aplicando el logaritmo natural
\begin{eqnarray*}
\ln \mathcal{L}(\beta) =\sum_{i=1}^{n} \left[y_i \ln \left( \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}} \right) + (1 - y_i) \ln \left( \frac{1}{1 + e^{x_i^\top \beta}} \right)\right].
\end{eqnarray*}

Calculando  el gradiente y la matriz Hessiana:

\begin{eqnarray*}
\frac{\partial}{\partial \beta_j} \ln \mathcal{L}(\beta)
= \sum \left[ y_i \left( \frac{x_{ij}}{1 + e^{x_i^\top \beta}} \right)
+ (1 - y_i) \left( \frac{-x_{ij} e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}} \right) \right].
\end{eqnarray*}


Recordemos que:

\begin{eqnarray*}
\frac{\partial}{\partial \beta_j} \ln \left( \frac{1}{1 + e^{x_i^\top \beta}} \right)
&=& - \frac{1}{1 + e^{x_i^\top \beta}} \cdot e^{x_i^\top \beta} \cdot x_{ij}
= - \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}} x_{ij},
\end{eqnarray*}

entonces
\begin{eqnarray*}
\frac{\partial}{\partial \beta_j} \ln \mathcal{L}(\beta)= y_i \cdot \frac{1}{1 + e^{x_i^\top \beta}} \cdot x_i
= y_i \cdot \frac{x_i}{1 + e^{x_i^\top \beta}}.
\end{eqnarray*}


Por lo tanto
\begin{eqnarray*}
\frac{\partial}{\partial \beta_j} \ln \mathcal{L}(\beta)
&=& \sum_i \frac{\partial}{\partial \beta_j} \left[
y_i \ln \left( \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}} \right)
+ (1 - y_i) \ln \left( \frac{1}{1 + e^{x_i^\top \beta}} \right)
\right]\\
&=& \sum_i \left[
y_i \frac{\partial}{\partial \beta_j} \ln \left( \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}} \right)
+ (1 - y_i) \frac{\partial}{\partial \beta_j} \ln \left( \frac{1}{1 + e^{x_i^\top \beta}} \right)
\right].
\end{eqnarray*}

Dado que

\begin{eqnarray*}
\frac{\partial}{\partial \beta_j} \ln \left( \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}} \right)
&=& \frac{\partial}{\partial \beta_j} \left[ x_{ij} - \ln \left( 1 + e^{x_i^\top \beta} \right) \right]
= x_{ij} - \frac{1}{1 + e^{-x_i^\top \beta}} \cdot x_{ij}= x_{ij} \left[ 1 - \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}} \right]\\
&=& x_{ij} \cdot \frac{1}{1 + e^{x_i^\top \beta}},
\end{eqnarray*}

por otra parte

\begin{eqnarray*}
\frac{\partial}{\partial \beta_j} \ln \left( \frac{1}{1 + e^{x_i^\top \beta}} \right) 
&=& \frac{\partial}{\partial \beta_j} \ln \left(1 + e^{x_i^\top \beta} \right)
= - \frac{1}{1 + e^{x_i^\top \beta}} \cdot e^{x_i^\top \beta} \cdot x_{ij}= -x_{ij} \cdot \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}},
\end{eqnarray*}

por lo tanto
\begin{eqnarray*}
\frac{\partial}{\partial \beta_j} \ln \mathcal{L}(\beta) &=&
\sum y_i x_{ij} \cdot \frac{1}{1 + e^{x_i^\top \beta}} 
- (1 - y_i) x_{ij} \cdot \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}}\\
&=& \sum y_i x_{ij} (1 - \rho_i) - (1 - y_i) x_{ij} \rho_i
= \sum x_{ij} (y_i - \rho_i) = 0.
\end{eqnarray*}

En forma matricial se puede reescribir como

\begin{eqnarray*}
g(\beta) = \nabla_\beta \ln \mathcal{L}(\beta) = X^\top (y - \rho) = 0.
\end{eqnarray*}

Ahora calculemos la segunda derivada de $\beta$:

\begin{eqnarray*}
\frac{\partial^2}{\partial \beta_j \partial \beta_k} \ln \mathcal{L}(\beta)= \frac{\partial^2}{\partial \beta_j \partial \beta_k}\sum \left( \frac{-x_{ij} x_{ik} e^{x_i^\top \beta}}{(1 + e^{x_i^\top \beta})^2} \right)= \frac{\partial^2}{\partial \beta_j \partial \beta_k}\sum x_{ij} x_{ik} \, \rho_i \, (1 - \rho_i),
\end{eqnarray*}

entonces

\begin{eqnarray*}
\frac{\partial^2}{\partial \beta_j \partial \beta_k} \ln \mathcal{L}(\beta)= \frac{\partial}{\partial \beta_k} \sum (x_{ij}) (y_i - \rho_i)= -\sum x_{ij} \frac{\partial}{\partial \beta_k} \rho_i,
\end{eqnarray*}
donde

\begin{eqnarray*}
\frac{\partial}{\partial \beta_k} \rho_i = \rho_i (1 - \rho_i) x_{ik},
\end{eqnarray*}
por lo tanto

\begin{eqnarray*}
\frac{\partial^2}{\partial \beta_j \partial \beta_k} \ln \mathcal{L}(\beta)= -\sum x_{ij} \rho_i (1 - \rho_i) x_{ik}= -\sum x_{ij} x_{ik} \rho_i (1 - \rho_i).
\end{eqnarray*}


Si $v_i := \rho_i (1 - \rho_i)$ y $\mathbb{V} = \mathrm{diag}(v_1, v_2, \dots, v_n)$, entonces

\begin{eqnarray*}
\mathcal{H}(\beta) = \nabla^2_\beta \ln \mathcal{L}(\beta) = -X^\top \mathbb{V} X
\end{eqnarray*}
que es negativa definida, es decir, es cóncava con un máximo global. La matriz de información \textit{LR} está dada por:

\begin{eqnarray*}
\mathcal{I}(\beta) = -\mathbb{E}[\mathcal{H}(\beta)] = X^\top \mathbb{V} X,
\end{eqnarray*}

con Varianza:

\begin{eqnarray*}
\mathbb{V}(\hat{\beta}) = \mathcal{I}^{-1}(\beta) = (X^\top \mathbb{V} X)^{-1}.
\end{eqnarray*}
\end{Ejem}


\begin{Note}
La \textit{log-verosimilitud regularizada} se define por

\begin{eqnarray*}
\ln \mathcal{L}(\beta) &=& \sum_i y_i \ln \left( \frac{e^{x_i^\top \beta}}{1 + e^{x_i^\top \beta}} \right) + (1 - y_i) \ln \left( \frac{1}{1 + e^{x_i^\top \beta}} \right)
- \frac{\lambda}{2} \| \beta \|^2\\
&=& \sum_i \ln \left( \frac{e^{y_i x_i^\top \beta}}{1 + e^{x_i^\top \beta}} \right) - \frac{\lambda}{2} \| \beta \|^2,
\end{eqnarray*}
donde $\lambda$  es el \textit{parámetro de regularización}. Retomando,

\begin{eqnarray*}
\nabla_\beta \ln \mathcal{L}(\beta) = X^\top (y - p) = 0,\\
\nabla_\beta^2 \ln \mathcal{L}(\beta) = -X^\top \mathbb{V} X - \Sigma^{-1}.
\end{eqnarray*}
\end{Note}



\begin{Ejem}
Supongamos que se tienen $\mathcal{D}=\left\{u^{(1)},u^{(2)},\ldots,u^{(N)}\right\}$ observaciones, supongamos adem\'as que se tienen datos generados con distrubuci\'on $U\sim\left(U;\theta\right)$. Calculemos la funci\'on de verosimlitud.

\begin{eqnarray*}
\mathcal{L}\left(\theta\right)=\prod_{i=1}^{N}p\left(u^{(i)};\theta\right),
\end{eqnarray*}
con
\begin{eqnarray*}
\theta_{ML}=\operatorname*{arg\,max}_{\theta}\mathcal{L}\left(\theta\right)=\operatorname*{arg\,max}_{\theta}\sum_{i=1}^{N}\log p\left(u^{(i)};\theta\right),
\end{eqnarray*}
donde tanto $log(f(x))$ y $\operatorname*{arg\,max}_{\theta}$ son funciones mon\'otonas crecientes. supongamos que se tiene un ruido gaussiano com media $0$ y varianza $\sigma^{2}$, entonces

\begin{eqnarray*}
y^{(i)}=h_{\theta}\left(x^{(i)}\right)+\epsilon^{(i)}=\theta^{\top}\mathbf{X}^{(i)}+\epsilon^{(i)},
\end{eqnarray*}
por lo tanto
\begin{eqnarray*}
y^{(i)}\sim N\left(\theta^{\top}\mathbf{X}^{(i)},\sigma^{2}\right), 
\end{eqnarray*}
entonces
\begin{eqnarray*}
p\left(y|\mathbf{X},\theta,\sigma^{2}\right)&=&\prod_{i=1}^{N}p\left(y|\mathbf{x}^{(i)},\theta,\sigma^{2}\right)=\prod_{i=1}^{N}\left(2\pi\sigma^{2}\right)e^{-\frac{1}{2\sigma^{2}}\left(y^{(i)}-\theta^{\top}\mathbf{x}^{(i)}\right)^{2}}\\
&=&\left(2\pi\sigma^{2}\right)^{-\frac{N}{2}}e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{2}\left(y^{(i)}-\theta^{\top}\mathbf{x}^{(i)}\right)^{2}}=\left(2\pi\sigma^{2}\right)^{-\frac{N}{2}}e^{-\frac{1}{2\sigma^{2}}\left(y-\mathbf{X}\theta\right)^{\top}\left(y-\mathbf{X}\theta\right)},
\end{eqnarray*}
por tanto la verosimilitud es

\begin{eqnarray*}
p\left(y|\mathbf{X},\theta,\sigma^{2}\right)=\left(2\pi\sigma^{2}\right)^{-\frac{N}{2}}e^{-\frac{1}{2\sigma^{2}}\left(y-\mathbf{X}\theta\right)^{\top}\left(y-\mathbf{X}\theta\right)},
\end{eqnarray*}

y la log-verosimilitud est\'a dada por 
\begin{eqnarray*}
\mathcal{L}\left(\theta,\sigma^{2}\right)=-\frac{N}{2}\log\left(2\pi\sigma^{2}\right)\left[-\frac{1}{2\sigma^{2}}\left(y-\mathbf{X}\theta\right)^{\top}\left(y-\mathbf{X}\theta\right)\right].
\end{eqnarray*}
Maximizar la log-verosimilitud con respecto a $\theta$ es equivalente a maximizar $-\left(y-\mathbf{X}\theta\right)^{\top}\left(y-\mathbf{X}\theta\right)$ que a su vez es equivalente a minimizar $\left(y-\mathbf{X}\theta\right)^{\top}\left(y-\mathbf{X}\theta\right)$.
\end{Ejem}

\begin{Ejem}
Se define la función sigmoide
\begin{eqnarray*}
\sigma(u) = \frac{1}{1 + e^{-u}}, \text{(clasificador de regresión logística)}
\end{eqnarray*}
donde la regla de decisión para $y$:
\begin{eqnarray*}
y = \sigma(h_{\boldsymbol{\theta}}(x)) = \sigma(\boldsymbol{\theta}^{\top} x),
\end{eqnarray*}

Matemáticamente, la probabilidad de que un ejemplo pertenezca a la clase 1 es:
\begin{eqnarray*}
p\left(y^{(i)} = 1 \mid x^{(i)}; \boldsymbol{\theta} \right) &=& \sigma(\boldsymbol{\theta}^{\top} x^{(i)}),\\
p\left(y^{(i)} = 0 \mid x^{(i)}; \boldsymbol{\theta} \right) &=& 1 - \sigma(\boldsymbol{\theta}^{\top} x^{(i)}),
\end{eqnarray*}

la probabilidad conjunta en función de $y^{(i)}$
\begin{eqnarray*}
p\left(y^{(i)} \mid x^{(i)}; \boldsymbol{\theta} \right) = 
\sigma(\boldsymbol{\theta}^{\top} x^{(i)})^{y^{(i)}} \cdot \left[1 - \sigma(\boldsymbol{\theta}^{\top} x^{(i)}) \right]^{1 - y^{(i)}};
\end{eqnarray*}
mientras que la probabilidad conjunta de todas las etiquetas
\begin{eqnarray*}
\prod_{i=1}^{N} \sigma(\boldsymbol{\theta}^{\top} x^{(i)})^{y^{(i)}} \left(1 - \sigma(\boldsymbol{\theta}^{\top} x^{(i)}) \right)^{(1 - y^{(i)})}.
\end{eqnarray*}
La log-verosimilitud para regresión logística está dada por:
\begin{eqnarray*}
\ell(\boldsymbol{\theta}) = \sum_{i=1}^{N} y^{(i)} \log \left( \sigma(\boldsymbol{\theta}^{\top} x^{(i)}) \right) + 
(1 - y^{(i)}) \log \left( 1 - \sigma(\boldsymbol{\theta}^{\top} x^{(i)}) \right).
\end{eqnarray*}
Antes de calcular la derivada, recordemos:
\begin{eqnarray*}
\sigma(z) = \frac{1}{1 + e^{-z}},
\end{eqnarray*}
con derivada
\begin{eqnarray*}
\frac{d}{dz} \sigma(z) = \frac{d}{dz} \left(1 + e^{-z} \right)^{-1} = 
-(1 + e^{-z})^{-2} \cdot (-e^{-z}) =
\frac{e^{-z}}{(1 + e^{-z})^2},
\end{eqnarray*}
además:
\begin{eqnarray*}
\sigma(z) &=& \frac{1}{1 + e^{-z}} \Rightarrow 1 - \sigma(z) = \frac{e^{-z}}{1 + e^{-z}}\\
&\Rightarrow& \sigma(z)(1 - \sigma(z)) = \frac{e^{-z}}{(1 + e^{-z})^2}\\
&\therefore& \frac{d}{dz} \sigma(z) = \sigma(z)(1 - \sigma(z)).
\end{eqnarray*}
Derivando la log-verosimilitud respecto a $theta_{j}$ la función $\ell(\boldsymbol{\theta})$:
\begin{eqnarray*}
\frac{\partial \ell(\boldsymbol{\theta})}{\partial \theta_j} &=& 
\sum_{i=1}^{N} \left[ y^{(i)} \log \sigma(\boldsymbol{\theta}^{\top} x^{(i)}) +
(1 - y^{(i)}) \log (1 - \sigma(\boldsymbol{\theta}^{\top} x^{(i)})) \right]\\
&=& \sum_{i=1}^{N} \left[ \frac{y^{(i)}}{\sigma(\boldsymbol{\theta}^{\top} x^{(i)})} 
- \frac{1 - y^{(i)}}{1 - \sigma(\boldsymbol{\theta}^{\top} x^{(i)})} \right]
\cdot \frac{d}{d\theta_j} \sigma(\boldsymbol{\theta}^{\top} x^{(i)})\\
&=& \sum_{i=1}^{N} \left[ \frac{y^{(i)}}{\sigma(\boldsymbol{\theta}^{\top} x^{(i)})} 
- \frac{1 - y^{(i)}}{1 - \sigma(\boldsymbol{\theta}^{\top} x^{(i)})} \right]
\cdot  \sigma(\boldsymbol{\theta}^{\top} x^{(i)})\sigma(1-\boldsymbol{\theta}^{\top} x^{(i)})x_{j}^{(i)}\\
&=& \sum_{i=1}^{N} \left[ \frac{y^{(i)}- \sigma(\boldsymbol{\theta}^{\top}x^{(i)})}
{\sigma(\boldsymbol{\theta}^{\top} x^{(i)})(1 - \sigma(\boldsymbol{\theta}^{\top} x^{(i)}))} \right]
\cdot  \sigma(\boldsymbol{\theta}^{\top} x^{(i)})\sigma(1-\boldsymbol{\theta}^{\top} x^{(i)})x_{j}^{(i)}\\
&=&\sum_{i=1}^{N}\left[y^{(i)}-\sigma(\boldsymbol{\theta}^{\top} x^{(i)})\right]x_{j}^{(i)}, 
\end{eqnarray*}
la cual es la funci\'on recursiva para calcular el gradiente.
\end{Ejem}

\begin{Ejem}
El modelo lineal univariado se expresa como:

\begin{eqnarray*}
h_\theta(x) = \theta_0 + \theta_1 x,
\end{eqnarray*}

donde la función de \textit{Costo (SSE - Error Cuadrático medio)} está definida por:

\begin{eqnarray*}
J(\theta_0, \theta_1) = \frac{1}{2N} \sum_{i=1}^{N} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2.
\end{eqnarray*}

Nuestro objetivo es encontrar los valores de $\theta_0$ y $\theta_1$ que minimicen la función de costo

\begin{eqnarray*}
\min_{\theta_0, \theta_1} J(\theta_0, \theta_1).
\end{eqnarray*}

El gradiente de la función de costo respecto a $\theta_0$

\begin{eqnarray*}
\frac{\partial J(\theta_0, \theta_1)}{\partial \theta_0} = \frac{1}{N} \sum_{i=1}^{N} \left( h_\theta(x^{(i)}) - y^{(i)} \right),
\end{eqnarray*}

y el gradiente de la función de costo respecto a $\theta_1$

\begin{eqnarray*}
\frac{\partial J(\theta_0, \theta_1)}{\partial \theta_1} = \frac{1}{N} \sum_{i=1}^{N} x^{(i)} \left( h_\theta(x^{(i)}) - y^{(i)} \right).
\end{eqnarray*}

Para minimizar $J(\theta_0, \theta_1)$, se pueden utilizar métodos iterativos, como el de gradiente descentiente
\begin{eqnarray*}
\theta_j := \theta_j - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta_j}, \quad j = 0, 1;
\end{eqnarray*}

donde $\alpha$ es la corrección de la direcci\'on de descenso.


\begin{eqnarray*}
\theta_{0}&=& \frac{1}{N} \left\{\sum_{i=1}^{N} y^{(i)}-\theta_{1} \sum_{i=1}^{N}  x^{(i)}\right\},\\
\theta_{1}&=& \frac{N\sum_{i=1}^{N} y^{(i)}x^{(i)} -\sum_{i=1}^{N} y^{(i)}\sum_{i=1}^{N} x^{(i)}}{N\sum_{i=1}^{N} (x^{(i)})^{2}-(\sum_{i=1}^{N} x^{(i)})^{2}}.
\end{eqnarray*}

Para el caso multivariado ser\'ia

\begin{eqnarray*}
h_{\theta}\left(x\right)=\sum_{i=1}^{d}\theta_{i}x_{i}+\theta_{0}=\sum_{i=0}^{d}\theta_{i}x_{i}\textrm{, con }x_{0}=1,
\end{eqnarray*}
es decir, en forma matricial se puede ver como

\begin{eqnarray*}
h_{\theta}(x) &= \theta^{T} X\textrm{, } X = \begin{pmatrix} x_0 \\ x_1 \\ \vdots \\ x_d \end{pmatrix},
\end{eqnarray*}
con 
\begin{eqnarray*}
J(\theta) &= J(\theta_0, \theta_1, \ldots, \theta_d) = \frac{1}{2N} \sum_{i=1}^{N} \left( \theta^{T} x^{(i)} - y^{(i)} \right)^2.
\end{eqnarray*}
y

\begin{eqnarray*}
h_{\mathbf{\theta}}(\mathbf{X}) = \mathbf{\theta}^{T} \mathbf{X} = \mathbf{X}^{T} \mathbf{\theta}.
\end{eqnarray*}
Por lo tanto
\begin{eqnarray*}
\hat{\textbf{y}} &= \mathbf{X} \mathbf{\theta} \quad \Leftrightarrow \quad 
\begin{bmatrix}
\hat{y}^{(1)} \\
\hat{y}^{(2)} \\
\vdots \\
\hat{y}^{(N)}
\end{bmatrix} =
\begin{bmatrix}
h_{\theta}\mathbf{x}^{(1)} \\
h_{\theta}\mathbf{x}^{(2)} \\
\vdots \\
h_{\theta}\mathbf{x}^{(N)}
\end{bmatrix}= 
\begin{bmatrix}
x_0^{(1)} & x_1^{(1)} & \cdots & x_d^{(1)} \\
x_0^{(2)} & x_1^{(2)} & \cdots & x_d^{(2)} \\
\vdots & \vdots & \ddots & \vdots \\
x_0^{(N)} & x_1^{(N)} & \cdots & x_d^{(N)} \\
\end{bmatrix}
\begin{bmatrix}
\theta_0 \\
\theta_1 \\
\vdots \\
\theta_d
\end{bmatrix},
\end{eqnarray*}

donde $\mathbf{X} \in \mathbb{R}^{N \times (d+1)}$, $\hat{\mathbf{y}} \in \mathbb{R}^{N \times 1}$ y $\mathbf{\theta} \in \mathbb{R}^{(d+1) \times 1}$. Entonces

\begin{eqnarray*}
J(\mathbf{\theta}) &=& \frac{1}{2N} \sum_{i=1}^{N} \left( \mathbf{\theta}^{T} \mathbf{x}^{(i)} - y^{(i)} \right)^2 = \frac{1}{2N} \sum_{i=1}^{N} \left( \hat{y}^{(i)} - y^{(i)} \right)^2\\
&=& \frac{1}{2N} | \hat{\mathbf{y}} - \mathbf{y} |_2^2 = \frac{1}{2N} (\hat{\mathbf{y}} - \mathbf{y})^{T} (\hat{\mathbf{y}} - \mathbf{y})=\frac{1}{2N}\left(\mathbf{X}\mathbf{\theta}-y\right)^{\top}\left(\mathbf{X}\mathbf{\theta}-y\right)\\
&=&\frac{1}{2N}\left\{\mathbf{\theta}^{\top}\left(\mathbf{X}^{\top}\mathbf{X}\right)\mathbf{\theta}-\mathbf{\theta}^{\top}\mathbf{X}^{\top}y-y^{\top}\mathbf{X}\mathbf{\theta}+y^{\top}y\right\}\\
&=&\frac{1}{2N}\left\{\mathbf{\theta}^{\top}\left(\mathbf{X}^{\top}\mathbf{X}\right)\mathbf{\theta}-\left(\mathbf{X}^{\top}y\right)^{\top}\mathbf{\theta}-\left(\mathbf{X}^{\top}y\right)^{\top}\mathbf{\theta}+y^{\top}y\right\}\\
&=&\frac{1}{2N}\left\{\mathbf{\theta}^{\top}\left(\mathbf{X}^{\top}\mathbf{X}\right)\mathbf{\theta}-2\left(\mathbf{X}^{\top}y\right)^{\top}\mathbf{\theta}+y^{\top}y\right\},
\end{eqnarray*}

por lo tanto
\begin{eqnarray*}
J(\mathbf{\theta}) = \frac{1}{2N} (\mathbf{X} \mathbf{\theta} - \mathbf{y})^{\top} (\mathbf{X} \mathbf{\theta} - \mathbf{y}).
\end{eqnarray*}

Recordemos que $\mathbf{\theta}^{\top} \mathbf{X}^{\top} \mathbf{y} = (\mathbf{X}^{\top} \mathbf{y})^{\top} \mathbf{\theta}$, $(\mathbf{X}^{\top} \mathbf{y})^{\top} = \mathbf{y}^{\top} \mathbf{X}$ y $(\mathbf{a}^{\top} \mathbf{b}) = (\mathbf{b}^{\top} \mathbf{a})$, por lo tanto podemos reescribir:

\begin{eqnarray*}
J(\mathbf{\theta}) = \frac{1}{2N}\left( \mathbf{\theta}^{\top}\left(\mathbf{X}^{\top}\mathbf{X}\right)\mathbf{\theta}- 2\left(\mathbf{X}^{\top}y\right)^{\top}\mathbf{\theta}+ \mathbf{y}^{\top}\mathbf{y}\right).
\end{eqnarray*}

Calculando el gradiente e igualando a cero:

\begin{eqnarray*}
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta})&=& -\frac{1}{2N} \left\{ \boldsymbol{\theta}^{\top} (X^{\top}X)\boldsymbol{\theta} - 2(X^{\top} \mathbf{y})^{\top} \boldsymbol{\theta} + \mathbf{y}^{\top} \mathbf{y} \right\}= \frac{1}{2N} \left\{ 2 X^{\top} X \boldsymbol{\theta} - 2 X^{\top} \mathbf{y} \right\}
\nabla_{\boldsymbol{\theta}}\\
 J(\boldsymbol{\theta})&=& 0 \Leftrightarrow X^{\top} X \boldsymbol{\theta} = X^{\top} \mathbf{y} \Leftrightarrow \boldsymbol{\theta}= (X^{\top} X)^{-1} X^{\top} \mathbf{y}.
\end{eqnarray*}
\end{Ejem}

\begin{Note}
$(X^{\top} X)^{\top}=X^{\top}(X^{\top})^{\top}=X^{\top} X$.
\end{Note}


%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Notas finales}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

\begin{Note} \textbf{Selecci\'on de Variables:} La selecci\'on de variables es el proceso de elegir las variables m\'as relevantes para el modelo. Existen varias t\'ecnicas para la selecci\'on de variables:
\begin{itemize}
\item \textbf{M\'etodos de Filtrado:}  Los m\'etodos de filtrado seleccionan variables basadas en criterios estad\'isticos, como la correlaci\'on o la chi-cuadrado. Algunas t\'ecnicas comunes incluyen:
\begin{itemize}
    \item \textbf{An\'alisis de Correlaci\'on}: Se seleccionan variables con alta correlaci\'on con la variable dependiente y baja correlaci\'on entre ellas.
    \item \textbf{Pruebas de Chi-cuadrado}: Se utilizan para variables categ\'oricas para determinar la asociaci\'on entre la variable independiente y la variable dependiente.
\end{itemize}

\item \textbf{M\'etodos de Wrapper:} Los m\'etodos de wrapper eval\'uan m\'ultiples combinaciones de variables y seleccionan la combinaci\'on que optimiza el rendimiento del modelo. Ejemplos incluyen:
\begin{itemize}
    \item \textbf{Selecci\'on hacia Adelante}: Comienza con un modelo vac\'io y agrega variables una por una, seleccionando la variable que mejora m\'as el modelo en cada paso.
    \item \textbf{Selecci\'on hacia Atr\'as}: Comienza con todas las variables y elimina una por una, removiendo la variable que tiene el menor impacto en el modelo en cada paso.
    \item \textbf{Selecci\'on Paso a Paso}: Combina la selecci\'on hacia adelante y hacia atr\'as, agregando y eliminando variables seg\'un sea necesario.
\end{itemize}

\item \textbf{M\'etodos Basados en Modelos:} Los m\'etodos basados en modelos utilizan t\'ecnicas de regularizaci\'on como Lasso y Ridge para seleccionar variables. Estas t\'ecnicas a\~naden un t\'ermino de penalizaci\'on a la funci\'on de costo para evitar el sobreajuste.

\begin{itemize}

\item \textbf{Regresi\'on Lasso:} La regresi\'on Lasso (Least Absolute Shrinkage and Selection Operator) a\~nade una penalizaci\'on $L_1$ a la funci\'on de costo:

\begin{eqnarray*}
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j|,
\end{eqnarray*}
donde $\lambda$ es el par\'ametro de regularizaci\'on que controla la cantidad de penalizaci\'on.

\item \textbf{Regresi\'on Ridge:} La regresi\'on Ridge a\~nade una penalizaci\'on $L_2$ a la funci\'on de costo:
\begin{eqnarray*}
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2,
\end{eqnarray*}
donde $\lambda$ es el par\'ametro de regularizaci\'on.
\end{itemize}
\end{itemize}
\end{Note}

\begin{Note} Evaluar la calidad y el rendimiento de un modelo de regresión logística es crucial para asegurar que las predicciones sean precisas y útiles. Las métricas de evaluación permiten cuantificar la precisión y el rendimiento de un modelo. Algunas de las métricas más comunes incluyen \textbf{Curva ROC y AUC}. 
\medskip

\begin{itemize}
\item \textbf{Curva ROC (Receiver Operating Characteristic)} es una representación gráfica de la sensibilidad (verdaderos positivos) frente a especificidad (falsos positivos). El área bajo la curva (AUC) mide la capacidad del modelo para distinguir entre las clases.

\begin{eqnarray}
\text{Sensibilidad} &=& \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\text{Especificidad} &=& \frac{\text{TN}}{\text{TN} + \text{FP}}
\end{eqnarray}


\item \textbf{Matriz de Confusión:} La matriz de confusión es una tabla que muestra el rendimiento del modelo comparando las predicciones con los valores reales. Los términos incluyen:
\begin{itemize}
    \item \textbf{Verdaderos Positivos (TP)}: Predicciones correctas de la clase positiva.
    \item \textbf{Falsos Positivos (FP)}: Predicciones incorrectas de la clase positiva.
    \item \textbf{Verdaderos Negativos (TN)}: Predicciones correctas de la clase negativa.
    \item \textbf{Falsos Negativos (FN)}: Predicciones incorrectas de la clase negativa.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Predicción Positiva} & \textbf{Predicción Negativa} \\
\hline
\textbf{Real Positiva} & TP & FN \\
\hline
\textbf{Real Negativa} & FP & TN \\
\hline
\end{tabular}
\caption{Matriz de Confusión}
\label{tab:confusion_matrix}
\end{table}

\item \textbf{Precisión, Recall y F1-Score:} se define como 

\begin{eqnarray}
\text{Precisión} &=& \frac{\text{TP}}{\text{TP} + \text{FP}} \\
\text{Recall} &=& \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\text{F1-Score} &=& 2 \cdot \frac{\text{Precisión} \cdot \text{Recall}}{\text{Precisión} + \text{Recall}}.
\end{eqnarray}

\item \textbf{Log-Loss:} La pérdida logarítmica (Log-Loss) mide la precisión de las probabilidades predichas. La fórmula es:
\begin{eqnarray*}
\text{Log-Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right],
\end{eqnarray*}
donde $y_i$ son los valores reales y $p_i$ son las probabilidades predichas.

\item \textbf{Validación Cruzada:} La validación cruzada es una técnica para evaluar la capacidad de generalización de un modelo. Existen varios tipos de validación cruzada:

\item \textbf{K-Fold Cross-Validation:}  En K-Fold Cross-Validation, los datos se dividen en K subconjuntos. El modelo se entrena K veces, cada vez utilizando K-1 subconjuntos para el entrenamiento y el subconjunto restante para la validación.

\begin{eqnarray*}
\text{Error Medio} = \frac{1}{K} \sum_{k=1}^{K} \text{Error}_k.
\end{eqnarray*}

\item \textbf{Leave-One-Out Cross-Validation (LOOCV):} En LOOCV, cada observación se usa una vez como conjunto de validación y las restantes como conjunto de entrenamiento. Este método es computacionalmente costoso pero útil para conjuntos de datos pequeños.

\item \textbf{Ajuste y Sobreajuste del Modelo:} El ajuste adecuado del modelo es crucial para evitar el sobreajuste (overfitting) y el subajuste (underfitting).
\begin{itemize}
\item \textbf{Sobreajuste:} El sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento, capturando ruido y patrones irrelevantes. Los síntomas incluyen una alta precisión en el entrenamiento y baja precisión en la validación.

\item \textbf{Subajuste:} El subajuste ocurre cuando un modelo no captura los patrones subyacentes de los datos. Los síntomas incluyen baja precisión tanto en el entrenamiento como en la validación.
\end{itemize}

\item \textbf{Regularización:} La regularización es una técnica para prevenir el sobreajuste añadiendo un término de penalización a la función de costo. Las técnicas comunes incluyen:
\begin{itemize}
    \item \textbf{Regresión Lasso (L1)}
    \item \textbf{Regresión Ridge (L2)}
\end{itemize}
\end{itemize}
\end{Note}

\begin{Note}

\textbf{Interpretación de los Resultados:} Interpretar correctamente los resultados de un modelo de regresi\'on log\'istica es esencial para tomar decisiones informadas. 
\begin{itemize}

\item \textbf{Coeficientes de Regresi\'on Log\'istica:} Los coeficientes de regresi\'on log\'istica representan la relaci\'on entre las variables independientes y la variable dependiente en t\'erminos de log-odds.  Cada coeficiente $\beta_j$ en el modelo de regresi\'on log\'istica se interpreta como el cambio en el log-odds de la variable dependiente por unidad de cambio en la variable independiente $X_j$.

\begin{eqnarray*}
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{eqnarray*}

\item \textbf{Signo de los Coeficientes:}\medskip

\begin{itemize}
    \item \textbf{Coeficiente Positivo}: Un coeficiente positivo indica que un aumento en la variable independiente est\'a asociado con un aumento en el log-odds de la variable dependiente.
    \item \textbf{Coeficiente Negativo}: Un coeficiente negativo indica que un aumento en la variable independiente est\'a asociado con una disminuci\'on en el log-odds de la variable dependiente.
\end{itemize}

\item \textbf{Odds Ratios:} Las odds ratios proporcionan una interpretaci\'on m\'as intuitiva de los coeficientes de regresi\'on log\'istica. La odds ratio para una variable independiente $X_j$ se calcula como $e^{\beta_j}$. Recordemos que 

\begin{eqnarray*}
\text{OR}_j = e^{\beta_j}
\end{eqnarray*}

\begin{itemize}
    \item $OR > 1$: Un OR mayor que 1 indica que un aumento en la variable independiente est\'a asociado con un aumento en las odds de la variable dependiente.
    \item $OR < 1$: Un OR menor que 1 indica que un aumento en la variable independiente est\'a asociado con una disminuci\'on en las odds de la variable dependiente.
    \item $OR = 1$: Un OR igual a 1 indica que la variable independiente no tiene efecto sobre las odds de la variable dependiente.
\end{itemize}

\item \textbf{Intervalos de Confianza: } Los intervalos de confianza proporcionan una medida de la incertidumbre asociada con los estimadores de los coeficientes. Un intervalo de confianza del 95\% para un coeficiente $\beta_j$ indica que, en el 95\% de las muestras, el intervalo contendr\'a el valor verdadero de $\beta_j$.

Para calcular un intervalo de confianza del 95\% para un coeficiente $\beta_j$, 
\begin{eqnarray*}
\beta_j \pm 1.96 \cdot \text{SE}(\beta_j),
\end{eqnarray*}
donde $\text{SE}(\beta_j)$ es el error est\'andar de $\beta_j$.

\item \textbf{Significancia Estad\'istica:} La significancia estad\'istica se utiliza para determinar si los coeficientes del modelo son significativamente diferentes de cero. Esto se eval\'ua mediante pruebas de hip\'otesis. Para cada coeficiente $\beta_j$, la hip\'otesis nula $H_0$ es que $\beta_j = 0$. La hip\'otesis alternativa $H_a$ es que $\beta_j \neq 0$.

\item \textbf{P-valor: } El p-valor indica la probabilidad de obtener un coeficiente tan extremo como el observado, asumiendo que la hip\'otesis nula es verdadera. Un p-valor menor que el nivel de significancia $\alpha$ (t\'ipicamente 0.05) indica que podemos rechazar la hip\'otesis nula.
\end{itemize}
\end{Note}


\begin{Note}
Para asegurar que la regresi\'on log\'istica produzca un modelo preciso, se deben considerar factores cr\'iticos como la selecci\'on de variables independientes y la elecci\'on de la estrategia de construcci\'on del modelo.

\begin{Criterio}

\textbf{Criterio de selecci\'on} Es muy importante seleccionar correctamente las variables independientes. Aunque la regresi\'on log\'istica es bastante flexible y permite distintos tipos de variables (continuas, ordinales y categ\'oricas), alternativamente, uno podr\'ia optar por incluir todas las variables independientes relevantes independientemente de sus resultados univariados, ya que puede haber variables cl\'inicamente importantes que merezcan inclusi\'on a pesar de su desempe\~no estad\'istico; sin embargo,  incluir demasiadas variables independientes en el modelo puede conducir a un modelo matem\'aticamente inestable, con menor capacidad de generalizaci\'on m\'as all\'a de la muestra actual del estudio \cite{tabachnick2007,hosmer2000}.\medskip

Una parte clave del proceso de selecci\'on de variables es reconocer y considerar el papel de los posibles factores de confusi\'on. Como se describi\'o previamente, las variables de confusi\'on son aquellas cuya relaci\'on tanto con el resultado como con otra variable independiente oculta la verdadera asociaci\'on entre esa variable independiente y el resultado. Independientemente del m\'etodo para seleccionar las variables independientes, deben cumplirse ciertos supuestos b\'asicos: 

\begin{Sup}
\textbf{Independencia de los errores} Todos los resultados del grupo de muestra deben ser independientes entre s\'i; si los datos incluyen mediciones repetidas u otros resultados correlacionados, los errores tambi\'en estar\'an correlacionados. 
\end{Sup}

\begin{Sup} \textbf{Linealidad en el logit} para las variables continuas independientes, debe existir una relaci\'on lineal entre estas variables y sus respectivos resultados transformados en logit. Esto se puede realizar a trav\'es de la creaci\'on de un t\'ermino de interacci\'on entre cada variable continua independiente y su logaritmo natural. Si alguno de estos t\'erminos es estad\'isticamente significativo, se considera que el supuesto no se cumple.
\end{Sup}

\begin{Sup} \textbf{Ausencia de multicolinealidad}, o redundancia entre variables independientes,  un modelo de regresi\'on log\'istica con variables independientes altamente correlacionadas usualmente genera errores est\'andar grandes para los coeficientes beta estimados. La soluci\'on com\'un es eliminar una o m\'as variables redundantes.
\end{Sup}

\begin{Sup} \textbf{Ausencia de valores at\'ipicos altamente influyentes}, es decir, casos en los que el resultado predicho para un miembro de la muestra difiere considerablemente de su valor real, si hay demasiados valores at\'ipicos, la precisi\'on general del modelo puede verse comprometida. La detecci\'on de valores at\'ipicos se realiza examinando los residuales (diferencia entre los valores predichos y los resultados reales) junto con estad\'isticas diagn\'osticas y gr\'aficas; luego, se puede comparar el ajuste general del modelo y los coeficientes beta estimados con y sin los casos at\'ipicos, dependiendo de la magnitud del cambio, uno podr\'ia conservar los valores at\'ipicos cuyo efecto no sea alto o eliminar aquellos con una influencia particularmente fuerte sobre el modelo.
\end{Sup}
\end{Criterio}

\begin{Criterio} \textbf{N\'umero de variables a incluir} Como parte del proceso de selecci\'on de qu\'e variables independientes incluir, tambi\'en se debe decidir cu\'antas. El reto es seleccionar el menor n\'umero posible de variables independientes que expliquen mejor el resultado sin descuidar las limitaciones del tama\~no de muestra. En t\'erminos generales, un modelo sobreajustado tiene coeficientes beta estimados para las variables independientes mucho mayores de lo que deber\'ian ser, adem\'as de errores est\'andar m\'as altos de lo esperado. Este tipo de situaci\'on genera inestabilidad en el modelo porque la regresi\'on log\'istica requiere m\'as resultados que variables independientes para poder iterar soluciones diferentes en busca del mejor ajuste a trav\'es del m\'etodo de m\'axima verosimilitud. Aunque no existe un est\'andar universalmente aceptado, hay algunas \textit{reglas generales} derivadas en parte de estudios de simulaci\'on. Una de estas reglas sugiere que por cada variable independiente, debe haber al menos 10 resultados por cada categor\'ia binaria, siendo el resultado menos frecuente el que determina el n\'umero m\'aximo de variables independientes \cite{peduzzi1996, agresti2007}. Algunos estad\'isticos recomiendan una \textit{regla general} a\'un m\'as estricta de 20 resultados por variable independiente, dado que una relaci\'on m\'as alta tiende a mejorar la validez del modelo\cite{feinstein1996}. 
\end{Criterio}
\end{Note}

\begin{Note}
Adem\'as de la cuidadosa selecci\'on de las variables independientes, se debe elegir el tipo adecuado de modelo de regresi\'on log\'istica para el estudio. De hecho, seleccionar una estrategia de construcci\'on del modelo est\'a estrechamente relacionado con la elecci\'on de variables independientes, por lo que estos dos componentes deben considerarse simult\'aneamente al planear un an\'alisis de regresi\'on log\'istica.

Existen tres enfoques generales para la construcci\'on del modelo que se aplican a las t\'ecnicas de regresi\'on en general, cada uno con un \'enfasis y prop\'osito diferente: 
\begin{itemize}
\item[a) ] \textbf{Directo} (completo, est\'andar o simult\'aneo): Este enfoque es una especie de valor por defecto, ya que introduce todas las variables independientes en el modelo al mismo tiempo y no hace suposiciones sobre el orden o la importancia relativa de dichas variables. El enfoque directo es m\'as adecuado si no existen hip\'otesis previas sobre cu\'ales variables tienen mayor relevancia que otras. 

\item[ b) ] \textbf{Secuencial} (jer\'arquico):  las variables se a\~naden secuencialmente para evaluar si mejoran el modelo de acuerdo a un orden predeterminado de prioridad. Aunque este enfoque es \'util para clarificar patrones causales entre variables independientes y resultados, puede volverse complejo conforme aumentan los patrones causales, dificultando as\'i la obtenci\'on de conclusiones definitivas sobre los datos en algunos casos.

\item[c) ] \textbf{Paso a paso} (estad\'istico): En contraste con los dos m\'etodos anteriores, la regresi\'on paso a paso identifica variables independientes que deben mantenerse o eliminarse del modelo. Existen distintos tipos de t\'ecnicas paso a paso, incluyendo selecci\'on hacia adelante y eliminaci\'on hacia atr\'as con una contribuci\'on no significativa al resultado son eliminadas una por una hasta que s\'olo queden las variables estad\'isticamente significativas. Otra estrategia de construcci\'on del modelo que es conceptualmente similar a la regresi\'on por pasos se llama \textit{selecci\'on del mejor subconjunto'}, en la que se comparan modelos separados con diferentes n\'umeros de variables independientes para determinar el mejor ajuste.
\end{itemize}

Estas estrategias de construcci\'on no son necesariamente intercambiables, ya que pueden producir diferentes medidas de ajuste del modelo y diferentes estimaciones puntuales para las variables independientes a partir de los mismos datos. Por lo tanto, identificar el modelo apropiado para los objetivos del estudio es extremadamente importante.


La regresi\'on por pasos se basa en una selecci\'on automatizada de variables que tiende a aprovechar factores aleatorios en una muestra dada.  Adem\'as, puede producir modelos que no parecen completamente razonables desde una perspectiva biol\'ogica, algunos argumentan que la regresi\'on por pasos se reserva mejor para el tamizaje preliminar o \'unicamente para pruebas de hip\'otesis, como en casos de resultados novedosos y una comprensi\'on limitada de las contribuciones de las variables independientes. Sin embargo, otros se\~nalan que los m\'etodos por pasos no son en s\'i el problema (y de hecho pueden ser bastante efectivos en ciertos contextos); en cambio, el verdadero problema es una interpretaci\'on descuidada de los resultados sin valorar completamente los pros y contras de este enfoque. Por tanto, si uno elige crear un modelo por pasos, es importante validar posteriormente los resultados antes de sacar conclusiones.

Al validar modelos de regresi\'on log\'istica, existen numerosos m\'etodos entre los cuales elegir, cada uno m\'as o menos apropiado seg\'un los par\'ametros del estudio como el tama\~no de muestra. Para establecer la validez interna, los m\'etodos comunes incluyen: 

\begin{itemize}
\item[a) ] \textbf{M\'etodo de retenci\'on, o divisi\'on de la muestra en dos subgrupos} antes de la construcci\'on del modelo, con el grupo de \textit{entrenamiento} usado para crear el modelo de regresi\'on log\'istica y el grupo de \textit{prueba} usado para validarlo; \cite{altman2000, kohavi1995} 

\item[b) ] \textbf{Validaci\'on cruzada \textit{k-fold} o divisi\'on de la muestra en $k$ subgrupos de igual tama\~no} para prop\'ositos de entrenamiento y validaci\'on;\cite{kohavi1995} 

\item[c) ] \textbf{Validaci\'on cruzada \textit{uno fuera} (leave-one-out)}, una variante del m\'etodo \textit{k-fold} donde el n\'umero de particiones es igual al n\'umero de sujetos en la muestra;\cite{kohavi1995} y 

\item[d) ] \textbf{Bootstrapping} es decir, obtener submuestras repetidas con reemplazo de toda la muestra \cite{kohavi1995,efron1993}.
\end{itemize}

Adem\'as de validar internamente el modelo, uno deber\'ia intentar validarlo externamente en un nuevo entorno de estudio como una prueba adicional de su viabilidad estad\'istica y utilidad cl\'inica \cite{altman2000,miller1991}.

Una vez que se ha creado el modelo de regresi\'on log\'istica, se determina qu\'e tan bien se ajusta a los datos de la muestra en su totalidad. Dos de los m\'etodos m\'as comunes para evaluar el ajuste del modelo son la prueba de chi-cuadrado de Pearson y la desviaci\'on residual. Ambas miden la diferencia entre los resultados observados y los resultados predichos por el modelo, donde un mal ajuste del modelo se indica mediante valores de prueba elevados, lo que se\~nala una diferencia mayor \cite{hosmer2000, hosmer1997, kuss2002}. \medskip

Otra medida com\'unmente utilizada del ajuste del modelo es la prueba de bondad de ajuste de \textit{Hosmer-Lemeshow}, que divide a los sujetos en grupos iguales (a menudo de 10) seg\'un su probabilidad estimada del resultado. El decil m\'as bajo est\'a compuesto por aquellos que tienen menor probabilidad de experimentar el resultado. Si el modelo tiene buen ajuste, los sujetos que experimentaron el resultado principal caer\'an en su mayor\'ia en los deciles de mayor riesgo. Un modelo con mal ajuste resultar\'a en sujetos distribuidos de manera m\'as uniforme a lo largo de los deciles de riesgo para ambos resultados binarios \cite{tabachnick2007, hosmer2000}.\medskip

Las ventajas de las pruebas de Hosmer-Lemeshow incluyen su aplicaci\'on sencilla y facilidad de interpretaci\'on, las limitaciones incluyen la dependencia de las pruebas sobre c\'omo se definen los puntos de corte de los grupos y los algoritmos computacionales utilizados, as\'i como una menor capacidad para identificar modelos con mal ajuste en ciertas circunstancias.  \medskip

Otras alternativas menos comunes para evaluar el ajuste del modelo son descritas por Hosmer et al \cite{hosmer1997} y Kuss \cite{kuss2002}.

\end{Note}


%>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<%
%>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<>==<%
\newpage

\begin{thebibliography}{99}

\bibitem{darlington1990}%1
Darlington RB. \textit{Regression and Linear Models}. Columbus, OH: McGraw-Hill Publishing Company, 1990.

\bibitem{tabachnick2007}%2
Tabachnick BG, Fidell LS. \textit{Using Multivariate Statistics}. 5th ed. Boston, MA: Pearson Education, Inc., 2007.

\bibitem{hosmer2000}%3
Hosmer DW, Lemeshow SL. \textit{Applied Logistic Regression}. 2nd ed. Hoboken, NJ: Wiley-Interscience, 2000.

\bibitem{campbell1963}%4
Campbell DT, Stanley JC. \textit{Experimental and Quasi-experimental Designs for Research}. Boston, MA: Houghton Mifflin Co., 1963.

\bibitem{stokes2000}%5
Stokes ME, Davis CS, Koch GG. \textit{Categorical Data Analysis Using the SAS System}. 2nd ed. Cary, NC: SAS Institute, Inc., 2000.

\bibitem{newgard2004}%6
Newgard CD, Hedges JR, Arthur M, Mullins RJ. Advanced statistics: the propensity score—a method for estimating treatment effect in observational research. \textit{Acad Emerg Med}. 2004; \textbf{11}:953–961.

\bibitem{newgard2007}%7
Newgard CD, Haukoos JS. Advanced statistics: missing data in clinical research—part 2: multiple imputation. \textit{Acad Emerg Med}. 2007; \textbf{14}:669–678.

\bibitem{allison1999}%8
Allison PD. \textit{Logistic Regression Using the SAS System: Theory and Application}. Cary, NC: SAS Institute, Inc., 1999.

\bibitem{peduzzi1996}%9
Peduzzi P, Concato J, Kemper E, Holford TR, Feinstein AR. A simulation study of the number of events per variable in logistic regression analysis. \textit{J Clin Epidemiol}. 1996; \textbf{49}:1373–1379.

\bibitem{agresti2007}%10
Agresti A. \textit{An Introduction to Categorical Data Analysis}. Hoboken, NJ: Wiley, 2007.

\bibitem{feinstein1996}%11
Feinstein AR. \textit{Multivariable Analysis: An Introduction}. New Haven, CT: Yale University Press, 1996.

\bibitem{altman2000}%12
Altman DG, Royston P. What Do We Mean by Validating a Prognostic Model? \textit{Stats Med}. 2000; \textbf{19}:453–473.

\bibitem{kohavi1995}%13
Kohavi R. A study of cross-validation and bootstrap for accuracy estimation and model selection. In: \textit{Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI)}. Montreal, Quebec, Canada, August 20–25, 1995. 1995:1137–1143.

\bibitem{efron1993}%14
Efron B, Tibshirani R. \textit{An Introduction to the Bootstrap}. New York: Chapman \& Hall, 1993.

\bibitem{miller1991}%15
Miller ME, Hiu SL, Tierney WM. Validation techniques for logistic regression models. \textit{Stat Med}. 1991; \textbf{10}:1213–1226.

\bibitem{hosmer1997}%16
Hosmer DW, Hosmer T, Le Cessie S, Lemeshow S. A comparison of goodness-of-fit tests for the logistic regression model. \textit{Stat Med}. 1997; \textbf{16}:965–980.

\bibitem{kuss2002}%17
Kuss O. Global goodness-of-fit tests in logistic regression with sparse data. \textit{Stat Med}. 2002; \textbf{21}:3789–3801.

\bibitem{zou2007}%18
Zou KH, O'Malley AJ, Mauri L. Receiver-operating characteristic analysis for evaluating diagnostic tests and predictive models. \textit{Circulation}. 2007; \textbf{115}:654–657.
\bibitem{Mazurenko}Mazurenko, S., Prokop, Z., and Damborsky, J. (2019). Machine learning in enzyme engineering. ACS Catalysis, 10(2), 1210-1223. 

\bibitem{15} Yang, Y.; Niroula, A.; Shen, B.; Vihinen, M. PON-Sol: Prediction of Effects of Amino Acid Substitutions on Protein Solubility. Bioinformatics 2016, 32, 2032!2034.

\bibitem{16} Folkman, L.; Stantic, B.; Sattar, A.; Zhou, Y. EASE-MM: Sequence-Based Prediction of Mutation-Induced Stability Changes with Feature-Based Multiple Models. J. Mol. Biol. 2016, 428, 1394! 1405.

\bibitem{17} Teng, S.; Srivastava, A. K.; Wang, L. Sequence Feature-Based Prediction of Protein Stability Changes upon Amino Acid Substitutions. BMC Genomics 2010, 11, S5.

\bibitem{18} Huang, L.; Gromiha, M. M.; Ho, S. iPTREE-STAB: Interpretable Decision Tree Based Method for Predicting Protein Stability Changes Upon Mutations. Bioinformatics 2007, 23, 1292! 1293.


\bibitem{19} Koskinen, P.; Toronen, P.; Nokso-Koivisto, J.; Holm, L. PANNZER: High-Throughput Functional Annotation of Uncharac- terized Proteins in an Error-Prone Environment. Bioinformatics 2015, 31, 1544!1552.

\bibitem{20} De Ferrari, L.; Mitchell, J. B. From Sequence to Enzyme Mechanism Using Multi-Label Machine Learning. BMC Bioinf. 2014, 15, 150.

\bibitem{21} Falda, M.; Toppo, S.; Pescarolo, A.; Lavezzo, E.; Di Camillo, B.; Facchinetti, A.; Cilia, E.; Velasco, R.; Fontana, P. Argot2: A Large Scale Function Prediction Tool Relying on Semantic Similarity of Weighted Gene Ontology Terms. BMC Bioinf. 2012, 13, S14.

\bibitem{22} Cozzetto, D.; Buchan, D. W.; Bryson, K.; Jones, D. T. Protein Function Prediction by Massive Integration of Evolutionary Analyses and Multiple Data Sources. BMC Bioinf. 2013, 14, S1.

\bibitem{47} Kulski, J. Next Generation Sequencing: Advances, Applications and Challenges; InTechOpen: London, 2016.

\bibitem{48} Straiton, J.; Free, T.; Sawyer, A.; Martin, J. From Sanger Sequencing to Genome Databases and Beyond. BioTechniques 2019, 66, 60-63.

\bibitem{50} Ardui, S.; Ameur, A.; Vermeesch, J. R.; Hestand, M. S. Single Molecule Real-Time (SMRT) Sequencing Comes of Age: Applications and Utilities for Medical Diagnostics. Nucleic Acids Res. 2018, 46, 2159-2168.

\bibitem{51}Kono, N., and Arakawa, K. (2019). Nanopore sequencing: Review of potential applications in functional genomics. Development, growth and differentiation, 61(5), 316-326.

\bibitem{52} Bunzel, H. A., Garrabou, X., Pott, M., and Hilvert, D. (2018). Speeding up enzyme discovery and engineering with ultrahigh-throughput methods. Current opinion in structural biology, 48, 149-156.

\bibitem{55} Wrenbeck, E. E., Faber, M. S., and Whitehead, T. A. (2017). Deep sequencing methods for protein engineering and design. Current opinion in structural biology, 45, 36-44.

\bibitem{56} Fowler, D. M., and Fields, S. (2014). Deep mutational scanning: a new style of protein science. Nature methods, 11(8), 801-807.

\bibitem{57} Gupta, K., and Varadarajan, R. (2018). Insights into protein structure, stability and function from saturation mutagenesis. Current opinion in structural biology, 50, 117-125.

\bibitem{28} UniProt Consortium. UniProt: A Worldwide Hub of Protein Knowledge. Nucleic Acids Res. 2018, 47, D506-D515.

\bibitem{60} Evans, R.; Jumper, J.; Kirkpatrick, J.; Sifre, L.; Green, T.; Qin, C.; Zidek, A.; Nelson, A.; Bridgland, A.; Penedones, H.; Petersen, S.; Simonyan, K.; Jones, D. T.; Silver, D.; Kavukcuoglu, K.; Hassabis, D.; Senior, A. W. De Novo Structure Prediction with Deeplearning Based Scoring. In Thirteenth Critical Assessment of Techniques for Protein Structure Prediction Abstracts; 2018; pp 11-12.

\bibitem{61} Kinch, L. N.; Shi, S.; Cheng, H.; Cong, Q.; Pei, J.; Mariani, V.; Schwede, T.; Grishin, N. V. CASP9 Target Classification. Proteins: Struct., Funct., Genet. 2011, 79, 21-36.

\bibitem{62} Shehu, A.; Barbará, D.; Molloy, K. A Survey of ComputationalMethods for Protein Function Prediction. In Big Data Analytics in Genomics; Wong, K. C., Ed.; Springer: Cham, 2016; pp 225-298.

\bibitem{63} Zhang, C.; Freddolino, P. L.; Zhang, Y. COFACTOR: Improved Protein Function Prediction by Combining Structure, Sequence and Protein-Protein Interaction Information. Nucleic Acids Res. 2017, 45, W291-W299.

\bibitem{64} Kumar, N.; Skolnick, J. EFICAz2. 5: Application of a High-Precision Enzyme Function Predictor to 396 Proteomes. Bioinformatics 2012, 28, 2687-2688.

\bibitem{65} Li, Y.; Wang, S.; Umarov, R.; Xie, B.; Fan, M.; Li, L.; Gao, X. DEEPre: Sequence-Based Enzyme EC Number Prediction by Deep Learning. Bioinformatics 2018, 34, 760-769.

\bibitem{66} Yang, M.; Fehl, C.; Lees, K. V.; Lim, E. K.; Offen, W. A.; Davies, G. J.; Bowles, D. J.; Davidson, M. G.; Roberts, S. J.; Davis, B. G. Functional and Informatics Analysis Enables Glycosyltransferase Activity Prediction. Nat. Chem. Biol. 2018, 14, 1109-1117.

\bibitem{67} Niwa, T.; Ying, B. W.; Saito, K.; Jin, W.; Takada, S.; Ueda, T.; Taguchi, H. Bimodal Protein Solubility Distribution Revealed by an Aggregation Analysis of the Entire Ensemble of Escherichia Coli Proteins. Proc. Natl. Acad. Sci. U. S. A. 2009, 106, 4201-4206.

\bibitem{68} Klesmith, J. R.; Bacik, J. P.; Wrenbeck, E. E.; Michalczyk, R.; Whitehead, T. A. Trade-Offs Between Enzyme Fitness and Solubility Illuminated by Deep Mutational Scanning. Proc. Natl. Acad. Sci. U. S. A. 2017, 114, 2265-2270.

\bibitem{69} Ruiz-Blanco, Y. B.; Paz, W.; Green, J.; Marrero-Ponce, Y. ProtDCal: A Program to Compute General-Purpose-Numerical Descriptors for Sequences and 3D-Structures of Proteins. BMC Bioinf. 2015, 16, 162.

\bibitem{35} Han, X.; Wang, X.; Zhou, K. Develop Machine Learning-Based Regression Predictive Models for Engineering Protein Solubility. Bioinformatics 2019, 35, 4640-4646.

\bibitem{5} Musil, M.; Konegger, H.; Hon, J.; Bednar, D.; Damborsky, J. Computational Design of Stable and Soluble Biocatalysts. ACS Catal. 2019, 9, 1033-1054.

\bibitem{70} Li, G.; Dong, Y.; Reetz, M. T. Can Machine Learning Revolutionize Directed Evolution of Selective Enzymes? Adv. Synth. Catal. 2019, 361, 2377-2386. 

\bibitem{71} Wu, Z.; Kan, S. B. J.; Lewis, R. D.; Wittmann, B. J.; Arnold, F. H. Machine Learning-Assisted Directed Protein Evolution with Combinatorial Libraries. Proc. Natl. Acad. Sci. U. S. A. 2019, 116, 8852-8858.

\bibitem{85} Wolpert, D. H.; Macready, W. G. No Free Lunch Theorems for Optimization. IEEE Trans. Evol. Comput. 1997, 1, 67-82.

\bibitem{86} Wolpert, D. H. The Lack of a Priori Distinctions between Learning Algorithms. Neural Comput. 1996, 8, 1341-1390.

\bibitem{87} Walsh, I.; Pollastri, G.; Tosatto, S. C. Correct Machine Learning on Protein Sequences: A Peer-Reviewing Perspective. Briefings Bioinf. 2016, 17, 831-840.

\bibitem{88} Rao, R.; Bhattacharya, N.; Thomas, N.; Duan, Y.; Chen, X.; Canny, J.; Abbeel, P.; Song, Y. S. Evaluating Protein Transfer Learning with TAPE. arXiv preprint arXiv:1906.08230, 2019.


\bibitem{89} Romero, P. A.; Krause, A.; Arnold, F. H. Navigating the Protein Fitness Landscape with Gaussian Processes. Proc. Natl. Acad. Sci. U. S. A. 2013, 110, E193-E201

\bibitem{14} Eraslan, G.; Avsec, Z; Gagneur, J.; Theis, F. J. Deep Learning: New Computational Modelling Techniques for Genomics. Nat. Rev. Genet. 2019, 20, 389-403.

\bibitem{90} Repecka, D.; Jauniskis, V.; Karpus, L.; Rembeza, E.; Zrimec, J.; Poviloniene, S.; Rokaitis, I.; Laurynenas, A.; Abuajwa, W.; Savolainen, O.; Meskys, R.; Engqvist, M. K. M.; Zelezniak, A. Expanding Functional Protein Sequence Space Using Generative Adversarial Networks. bioRxiv 2019, DOI: 10.1101/789719.

\bibitem{91} Riesselman, A. J.; Ingraham, J. B.; Marks, D. S. Deep Generative Models of Genetic Variation Capture the Effects of Mutations. Nat. Methods 2018, 15, 816-822.

\bibitem{92} Thornton, C.; Hutter, F.; Hoos, H. H.; Leyton-Brown, K. Auto- WEKA: Combined Selection and Hyperparameter Optimization of Classiffication Algorithms. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining; 2013; pp 847-855.

\bibitem{93} Polikar, R. Ensemble based systems in decision making. IEEE Circuits and systems magazine 2006, 6, 21-45.

\bibitem{94} Gammerman, A.; Vovk, V. Hedging Predictions in Machine Learning. Comput. J. 2007, 50, 151-163.

\bibitem{95} Samek, W.; Wiegand, T.; Müller, K. Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models. ITU Journal: ICT Discoveries 2017, 39-48. 

\bibitem{96} Shrikumar, A.; Greenside, P.; Kundaje, A. Learning Important Features through Propagating Activation differences. In Proceedings of the 34th International Conference on Machine Learning; 2017; Vol. 70, pp 3145-3153.

\bibitem{97} Simonyan, K.; Vedaldi, A.; Zisserman, A. Deep Inside Convolutional Networks: Visualising Image Classiffication Models and Saliency Maps. arXiv preprint arXiv:1312.6034 2013.

\bibitem{98} Brookes, D. H.; Park, H.; Listgarten, J. Conditioning by Adaptive Sampling for Robust Design. In Proceedings of the 36th International Conference on Machine Learning; 2019; Vol. 97, pp 773-782.

\bibitem{99} Ribeiro, M. T.; Singh, S.; Guestrin, C. “Why Should I Trust You?” Explaining the Predictions of Any Classiffier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining; 2016; pp 1135-1144.

\bibitem{100} Szegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan, D.; Goodfellow, I.; Fergus, R. Intriguing Properties of Neural Networks. arXiv preprint arXiv:1312.6199 201

\bibitem{101} Yu, M. K.; Ma, J.; Fisher, J.; Kreisberg, J. F.; Raphael, B. J.; Ideker, T. Visible Machine Learning for Biomedicine. Cell 2018, 173, 1562-1565.

% Lista de referencias del segundo articulo

\bibitem{2.2} Copley, S. D. Shining a light on enzyme promiscuity. Curr. Opin. Struct. Biol. 47, 167–175 (2017).

\bibitem{2.4} Nobeli, I., Favia, A. D. and Thornton, J. M. Protein promiscuity and its implications for biotechnology. Nat. Biotechnol. 27, 157–167 (2009)

\bibitem{2.5} Adrio, J. L. and Demain, A. L. Microbial enzymes: tools for biotechnological processes. Biomolecules 4, 117–139 (2014).

\bibitem{2.6} Wang, S. et al. Engineering a synthetic pathway for gentisate in pseudomonas chlororaphis p3. Front. Bioeng. Biotechnol. 8, 1588 (2021).

\bibitem{2.7} Wu, M.-C., Law, B., Wilkinson, B. and micklefied, J. Bioengineering natural product biosynthetic pathways for therapeutic applications. Curr. Opin. Biotechnol. 23, 931–940 (2012)


\bibitem{2.9} Rembeza, E., Boverio, A., Fraaije, M. W. and Engqvist, M. K. Discovery of two novel oxidases using a high-throughput activity screen. ChemBioChem 23, e202100510 (2022).

\bibitem{2.10} Longwell, C. K., Labanieh, L. and Cochran, J. R. High-throughput screening technologies for enzyme engineering. Curr. Opin. Biotechnol. 48, 196–202 (2017).

\bibitem{2.11} Black, G. W. et al. A high-throughput screening method for determining the substrate scope of nitrilases. Chem. Commun. 51, 2660–2662 (2015).

\bibitem{2.13} Pertusi, D. A. et al. Predicting novel substrates for enzymes with minimal experimental effort with active learning. Metab. Eng. 44,171-181 (2017).

\bibitem{2.14} Mou, Z. et al. Machine learning-based prediction of enzyme substrate scope: Application to bacterial nitrilases. Proteins Struct. Funct. Bioinf. 89, 336-347 (2021).

\bibitem{2.15} Yang, M. et al. Functional and informatics analysis enables glycosyltransferase activity prediction. Nat. Chem. Biol. 14, 1109–1117 (2018).

\bibitem{2.16} Rottig, M., Rausch, C. and Kohlbacher, O. Combining structure and sequence information allows automated prediction of substrate specificities within enzyme families. PLoS Comput. Biol. 6, e1000636 (2010).

\bibitem{2.17} Chevrette, M. G., Aicheler, F., Kohlbacher, O., Currie, C. R. and Medema, M. H. Sandpuma: ensemble predictions of nonribosomal peptide chemistry reveal biosynthetic diversity across actinobacteria. Bioinformatics 33, 3202-3210 (2017).

\bibitem{2.18} Goldman, S., Das, R., Yang, K. K. and Coley, C. W. Machine learning modeling of family wide enzyme-substrate specificity screens. PLoS Comput. Biol. 18, e1009853 (2022).

\bibitem{2.19} Visani, G. M., Hughes, M. C. and Hassoun, S. Enzyme promiscuity prediction using hierarchy-informed multi-label classiffication Bioinformatics 37, 2017-2024 (2021).

\bibitem{2.20} Ryu, J. Y., Kim, H. U. and Lee, S. Y. Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers. PNAS 116, 13996-14001 (2019).

\bibitem{2.21} Li, Y. et al. DEEPre: sequence-based enzyme EC number prediction by deep learning. Bioinformatics 34, 760-769 (2017).

\bibitem{2.22} Sanderson, T., Bileschi, M. L., Belanger, D. and Colwell, L. J. Proteinfer, deep neural networks for protein functional inference. eLife 12, e80942 (2023).

\bibitem{2.23} Bileschi, M. L. et al. Using deep learning to annotate the protein universe. Nat. Biotechnol.https://doi.org/10.1038/s41587-021-01179-w (2022).

\bibitem{2.24} Rembeza, E. and Engqvist, M. K. Experimental and computational investigation of enzyme functional annotations uncovers misannotation in the ec 1.1. 3.15 enzyme class. PLoS Comput. Biol. 17, e1009446 (2021).

\bibitem{2.25} Ozturk, H., Ozgur, A. and Ozkirimli, E. Deepdta: deep drugtarget binding affinity prediction. Bioinformatics 34, i821-i829 (2018).

\bibitem{2.26} Feng, Q., Dueva, E., Cherkasov, A. and Ester, M. Padme: A deep learning-based framework for drug-target interaction prediction. Preprint at https://doi.org/10.48550/arXiv.1807.09741 (2018).

\bibitem{2.27} Karimi, M., Wu, D., Wang, Z. and Shen, Y. Deep affinity: interpretable deep learning of compound–protein affinity through UNIFIED recurrent and convolutional neural networks. Bioinformatics 35, 3329-3338 (2019).

\bibitem{2.28} Kroll, A., Engqvist, M. K., Heckmann, D. and Lercher, M. J. Deep learning allows genome-scale prediction of michaelis constants from structural features. PLoS Biol. 19, e3001402 (2021).

\bibitem{2.29} Li, F. et al. Deep learning-based k cat prediction enables improved enzyme-constrained model reconstruction. Nat. Catal. 5, 662-672 (2022).

\bibitem{2.30} Weininger, D. SMILES, a chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci. 28, 31-36 (1988).

\bibitem{2.31} Rogers, D. and Hahn, M. Extended-connectivity fingerprints. J. Chem. Inf. Model. 50, 742-754 (2010).

\bibitem{2.32} Zhou, J. et al. Graph neural networks: A review of methods and applications. AI Open 1, 57-81 (2020).

\bibitem{2.33} Yang, K. et al. Analyzing learned molecular representations for property prediction. J. Chem. Inf. Model. 59, 3370-3388 (2019).


\bibitem{2.34} Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. PNAS 118, e2016239118 (2021).

\bibitem{2.35} Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. and Church, G. M. Unified rational protein engineering with sequence-based deep representation learning. Nat. Methods. 16, 1315-1322 (2019).

\bibitem{2.36} Xu, Y. et al. Deep dive into machine learning models for protein engineering. J. Chem. Inf. Model. 60, 2773–2790 (2020).

\bibitem{2.42} Bekker, J. and Davis, J. Learning from positive and unlabeled data: A survey. Mach. Learn. 109, 719-760 (2020)


\bibitem{2.38} Kearnes, S., McCloskey, K., Berndl, M., Pande, V. and  Riley, P. Mole- cular graph convolutions: moving beyond !ngerprints. J. Comput. -Aided Mol. Des. 30, 595–608 (2016).

\bibitem{2.39} Duvenaud, D. K. et al. Convolutional networks on graphs for learning molecular fingerprints. In Advances in Neural Information Processing Systems, 2224-2232 (2015).

\bibitem{2.40} Zhou, J. et al. Graph neural networks: A review of methods and applications. AI Open 1, 57–81 (2020).

\bibitem{2.45} Hu, W. et al. Strategies for pre-training graph neural networks. Preprint at https://doi.org/10.48550/arXiv.1905.12265 (2019). 



\bibitem{2.46} Capela, F., Nouchi, V., Van Deursen, R., Tetko, I. V. and  Godin, G. Multitask learning on graph neural networks applied to molecular property predictions. Preprint at https://doi.org/10.48550/arXiv. 1910.13124 (2019).

\bibitem{2.47}  Vaswani, A. et al. Attention is all you need. In Advances in neural information processing systems, 5998–6008 (2017).

\bibitem{2,48} Suzek, B. E. et al. Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinfor- matics 31, 926–932 (2015).
\bibitem{2.49} Elnaggar, A. et al. Prottrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing. IEEE Trans. Pattern Anal. Mach. Intell. PP https://doi. org/10.1109/TPAMI.2021.3095381 (2021).


\bibitem{Wittman} Wittmann, B. J., Johnston, K. E., Wu, Z., and  Arnold, F. H. (2021). Advances in machine learning for directed evolution. Current opinion in structural biology, 69, 11-18.

\end{thebibliography}



\end{document}
